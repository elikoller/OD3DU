{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "/local/home/ekoller/BT\n",
      "8eabc426-5af7-2f32-87bb-a16609b099e3\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import os.path as osp\n",
    "import numpy as np \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "from collections import Counter\n",
    "import open3d as o3d\n",
    "import open3d.core as o3c\n",
    "import json\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "import pickle\n",
    "from  plyfile import PlyData\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "#the other imports from the local stuff\n",
    "import sys\n",
    "\n",
    "ws_dir = '/local/home/ekoller/BT'\n",
    "print(ws_dir)\n",
    "sys.path.append(ws_dir)\n",
    "from utils import scan3r,visualisation\n",
    "\n",
    "\n",
    "#reading in the necessary data\n",
    "data_dir ='/local/home/ekoller/R3Scan'\n",
    "scenes_dir = '/local/home/ekoller/R3Scan/scenes'\n",
    "#scan_id= \"38770c95-86d7-27b8-8717-3485b411ddc7\" #is reference scan  since it is a reference scan everything shouls be correctly hit\n",
    "frame_number = \"000085\"\n",
    "frame_number_2 =  \"000016\"\n",
    "img_width = 960\n",
    "img_height = 540\n",
    "\n",
    "#\"38770c9d-86d7-27b8-869e-4f713b04f290\" #is ref 1d2f8510-d757-207c-8c48-3684433860e1\n",
    "new_scan_id =  \"8eabc422-5af7-2f32-860b-db7caac59e10\" #\"b8837e1a-57ec-29c6-8a01-dec1dcb87460\"#\"0f2f271d-b736-2f71-8d77-dfc428e30c1e\"#\"05c6ede7-2e69-23b1-8b27-c1cb868f1938\" #\"0cac761d-8d6f-2d13-8f35-2364ee20f2a9\"#\"77361fd2-d054-2a22-88bd-8b14f5969890\" #is rescan 9c27de56-6184-2cda-8196-591957b6387d\n",
    "curr_scan_id = scan3r.get_reference_id(data_dir, new_scan_id)\n",
    "print(curr_scan_id)\n",
    "#the original meshes are given in the file  'labels.instances.annotated.v2.ply'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ekoller/T7/Segmentation/DinoV2/color/8eabc422-5af7-2f32-860b-db7caac59e10/frame-000097.jpg\n",
      "(540, 960, 3)\n"
     ]
    }
   ],
   "source": [
    "#2d visualizations :))))\n",
    "#this codesegment takes an image on a pixel wise level and quantizes it such that every patch has only the id of the most often occuring id\n",
    "def quantize_to_patch_level(pixelwise_img):\n",
    "    #get the shape of the pixelwise img\n",
    "    input_h, input_w ,_= pixelwise_img.shape\n",
    "    patch_width =30\n",
    "    patch_height=30\n",
    "\n",
    "    patchwise_w = 32 #number of patches\n",
    "    patchwise_h = 18\n",
    "\n",
    "    patchwise_id = np.zeros((patchwise_h,patchwise_w,3))\n",
    "\n",
    "    # Initialize the output quantized image\n",
    "    quantized_img = np.zeros((input_h, input_w, 3), dtype=np.uint8)\n",
    "\n",
    "        # Loop through patches\n",
    "    for i in range(patchwise_h):\n",
    "            for j in range(patchwise_w):\n",
    "                # Define the coordinates of the current patch\n",
    "                h_start = i * patch_height\n",
    "                w_start = j * patch_width\n",
    "                h_end = min(h_start + patch_height, input_h)  # Ensure we don't go out of bounds\n",
    "                w_end = min(w_start + patch_width, input_w)\n",
    "\n",
    "                # Get the current patch from the input matrix\n",
    "                patch = pixelwise_img[h_start:h_end, w_start:w_end]\n",
    "\n",
    "                # Flatten the patch and count occurrences of each color (directly in BGR format)\n",
    "                unique_colors, counts = np.unique(patch.reshape(-1, 3), axis=0, return_counts=True)\n",
    "\n",
    "                # Find the most frequent color\n",
    "                most_common_color = unique_colors[np.argmax(counts)]\n",
    "\n",
    "                # Assign the most common color to all pixels in the corresponding patch\n",
    "                quantized_img[h_start:h_end, w_start:w_end] = most_common_color\n",
    "\n",
    "    return quantized_img\n",
    "\n",
    "\n",
    "def color_connected_components(patch_annos):\n",
    "    # Initialize the output image\n",
    "    output_img = np.zeros_like(patch_annos)\n",
    "\n",
    "    # Look for unique colors in the image (ignoring the background if needed)\n",
    "    unique_colors = np.unique(patch_annos.reshape(-1, 3), axis=0)\n",
    "\n",
    "    # Store used colors to avoid duplication\n",
    "    used_colors = set()\n",
    "\n",
    "    # Loop through each unique color\n",
    "    for color in unique_colors:\n",
    "        # Create a mask for each color\n",
    "        mask = cv2.inRange(patch_annos, color, color)\n",
    "\n",
    "        # Get the individual connected components for that color mask (8-connectivity)\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
    "\n",
    "        # Assign colors to each connected component\n",
    "        for label in range(1, num_labels):  # Start from 1 because 0 is the background\n",
    "            # Convert color to tuple for easier comparison\n",
    "            color_tuple = tuple(color)\n",
    "\n",
    "            # If the color has not been used, use it; otherwise, generate a random color\n",
    "            if color_tuple not in used_colors:\n",
    "                output_img[labels == label] = color\n",
    "                used_colors.add(color_tuple)  # Mark this color as used\n",
    "            else:\n",
    "                # Generate a random color until we find one that hasn't been used\n",
    "                while True:\n",
    "                    random_color = np.random.randint(0, 256, size=3)  # Range is 0-255\n",
    "                    random_color_tuple = tuple(random_color)\n",
    "\n",
    "                    # Check if the random color is in the used colors\n",
    "                    if random_color_tuple not in used_colors:\n",
    "                        used_colors.add(random_color_tuple)\n",
    "                        output_img[labels == label] = random_color\n",
    "                        break  # Break the loop once a unique color is found\n",
    "\n",
    "    return output_img\n",
    "\n",
    "\n",
    "chosen_frame = \"000063\"\n",
    "#get the image\n",
    "segmentation_path = osp.join(\"/media/ekoller/T7/Segmentation/DinoV2/color\", new_scan_id , \"frame-000097.jpg\".format(chosen_frame))\n",
    "print(segmentation_path)\n",
    "img = cv2.imread(segmentation_path)\n",
    "#print(img)\n",
    "#cv2.imshow('Image', img)\n",
    "quantized_img = quantize_to_patch_level(img)\n",
    "print(quantized_img.shape)\n",
    "#print(quantized_img)\n",
    "quantized_img= cv2.rotate(quantized_img, cv2.ROTATE_90_CLOCKWISE)\n",
    "# cv2.imshow(\"quantized mask\", quantized_img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "colored_regions_img = color_connected_components(quantized_img)\n",
    "cv2.imshow('Quantized Image with Colored Independent Regions', colored_regions_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_to_image( patchwise_id):\n",
    "        patch_width = 30\n",
    "        patch_height = 30\n",
    "        \n",
    "        # Initialize an empty image with zeros (assuming same type as original)\n",
    "        reconstructed_img = np.zeros((540, 960), dtype=np.int32)\n",
    "        \n",
    "        # Loop over patches and place the patchwise_id values into the reconstructed image\n",
    "        for i in range(18):\n",
    "            for j in range(32):\n",
    "                # Define the coordinates of the current patch\n",
    "                h_start = i * patch_height\n",
    "                w_start = j * patch_width\n",
    "                h_end = h_start + patch_height\n",
    "                w_end = w_start + patch_width\n",
    "                \n",
    "                # Assign the patchwise_id value to the corresponding patch area\n",
    "                reconstructed_img[h_start:h_end, w_start:w_end] = patchwise_id[i, j]\n",
    "        \n",
    "        return reconstructed_img\n",
    "\n",
    "# reference_info_path = osp.join(\"/local/home/ekoller/R3Scan/files/patch_anno\", \"patch_anno_{}_{}\".format(32,18),\"{}.pkl\".format(curr_scan_id))\n",
    "# gt_patches = scan3r.load_pkl_data(reference_info_path)\n",
    "\n",
    "# gt_patches =gt_patches[frame_number]\n",
    "# print(\"gt_patches \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# img = reconstruct_to_image(gt_patches)\n",
    "# print(img.shape)\n",
    "\n",
    "# img_display = cv2.convertScaleAbs(img, alpha=(255.0/np.max(img)))\n",
    "# cv2.imshow(\"Reconstructed Image\", img_display)\n",
    "# cv2.waitKey(0)  # Wait indefinitely until a key is pressed\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the object centers based on gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: array([-1.0764055 , -0.22986837, -1.6204726 ], dtype=float32), 2.0: array([-2.6503274, -0.4816483, -1.2477363], dtype=float32), 3.0: array([-1.4643067 ,  1.4471424 , -0.66615933], dtype=float32), 4.0: array([ 0.17609023,  1.1365478 , -0.5190943 ], dtype=float32), 5.0: array([ 0.9540581 ,  0.41990674, -0.50217104], dtype=float32), 6.0: array([ 1.515443  , -0.01749394, -0.6760977 ], dtype=float32), 7.0: array([ 1.6255511 , -0.46711496, -0.5958228 ], dtype=float32), 8.0: array([ 1.2549869 , -0.63459927, -1.2562006 ], dtype=float32), 9.0: array([1.0996282 , 0.0488719 , 0.42158273], dtype=float32), 10.0: array([-1.6009654 ,  0.19980127, -0.38268152], dtype=float32), 11.0: array([-0.35882103,  1.2908388 , -1.1122053 ], dtype=float32), 12.0: array([-1.1833532,  1.248967 , -0.966029 ], dtype=float32), 13.0: array([-1.6853609,  1.0745401, -1.530568 ], dtype=float32), 14.0: array([-1.8348129,  1.0389003, -1.4420271], dtype=float32), 15.0: array([-1.8872588,  1.1842829, -1.2606487], dtype=float32), 16.0: array([-2.4593165 ,  0.73292726, -1.429038  ], dtype=float32), 17.0: array([-0.40966704,  1.517376  , -0.69662887], dtype=float32), 18.0: array([-0.13985042,  1.4926856 , -0.71982133], dtype=float32), 19.0: array([-0.7186972 ,  1.3579338 , -0.78038126], dtype=float32), 22.0: array([-0.19745983,  0.65358263, -1.5661287 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "#access the data of the object centers\n",
    "curr_scene_path = osp.join(data_dir, \"files\", \"orig\", \"data\", curr_scan_id + \".pkl\")\n",
    "with open(curr_scene_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "obj_ids_pkl = data[\"objects_id\"]\n",
    "\n",
    "\n",
    "#access the things for the mesh\n",
    "pathToMesh = osp.join(data_dir,\"scenes\", curr_scan_id, \"labels.instances.align.annotated.v2.ply\")\n",
    "ply_data = PlyData.read(pathToMesh)\n",
    "vertices = ply_data['vertex'].data\n",
    "vertex_array = np.array([list(vertex) for vertex in vertices])\n",
    "\n",
    "# Extract x, y, z coordinates and objectId\n",
    "x = vertex_array[:, 0]\n",
    "y = vertex_array[:, 1]\n",
    "z = vertex_array[:, 2]\n",
    "object_ids_mesh = vertex_array[:, 6]  # Assuming 'objectId' is the 7th property\n",
    "\n",
    "unique_object_ids = np.unique(object_ids_mesh)\n",
    "\n",
    "bounding_boxes_tmp = {}\n",
    "centroids = {}\n",
    "#go over every id and compute the box\n",
    "for obj_id in unique_object_ids:\n",
    "                #we want the same ids for the boxes\n",
    "                if obj_id in obj_ids_pkl:\n",
    "                        # Filter vertices by object ID\n",
    "                        obj_mask = object_ids_mesh == obj_id\n",
    "                        obj_coords = np.vstack((x[obj_mask], y[obj_mask], z[obj_mask])).T\n",
    "                        \n",
    "                        #also compute the centroid\n",
    "                        centroid = np.mean(obj_coords, axis=0)\n",
    "                        centroids[obj_id] = centroid\n",
    "\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this segment gets for an object id the colour the object is assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a given scene get the colours of the differnt object_ids as a dictionary\n",
    "def get_id_colours(data_dir,scan_id):\n",
    "    #access the mesh file to get the colour of the ids\n",
    "    mesh_file = osp.join(data_dir,\"scenes\", scan_id, \"labels.instances.annotated.v2.ply\")\n",
    "    ply_data = PlyData.read(mesh_file)\n",
    "    # Extract vertex data\n",
    "    vertices = ply_data['vertex']\n",
    "    vertex_count = len(vertices)\n",
    "    \n",
    "    # Initialize dictionary to store object_id -> color mappings\n",
    "    object_colors = {}\n",
    "    \n",
    "   # Iterate through vertices\n",
    "    for i in range(vertex_count):\n",
    "        vertex = vertices[i]\n",
    "        object_id = vertex['objectId']\n",
    "        color = (vertex['red'], vertex['green'], vertex['blue'])\n",
    "        \n",
    "        # Check if object_id already in dictionary, otherwise initialize a Counter\n",
    "        if object_id in object_colors:\n",
    "            object_colors[object_id][color] += 1\n",
    "        else:\n",
    "            object_colors[object_id] = Counter({color: 1})\n",
    "    \n",
    "    # Convert Counter to dictionary with most frequent color\n",
    "    for object_id, color_counter in object_colors.items():\n",
    "        most_common_color = color_counter.most_common(1)[0][0]\n",
    "        object_colors[object_id] = np.array(most_common_color)\n",
    "    \n",
    "    return object_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_in_reference(data_dir, scan_id , pose_rescan):\n",
    "    #same coordinate system\n",
    "    # ref_id = scan3r.get_reference_id(data_dir,scan_id)\n",
    "    # #if we want the coords in the reference coordinate system return the boxes (based on pkl file)\n",
    "    # if scan_id==ref_id:\n",
    "    #     print(\"we returned the same pose\")\n",
    "    #     return pose_rescan\n",
    "    \n",
    "\n",
    "    #transform the centers of rescan to ref coord\n",
    "    path = osp.join(data_dir,\"files\", \"3RScan.json\")\n",
    "    map_id_to_trans = scan3r.read_transform_mat(path)\n",
    "    transform = map_id_to_trans[scan_id]\n",
    "    transform= transform.reshape(4,4)\n",
    "\n",
    "    #transform the pose\n",
    "\n",
    "    return    transform.transpose() * pose_rescan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the depth map file\n",
    "depth_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.depth.pgm\".format(frame_number))\n",
    "\n",
    "# Step 1: Load the depth map using PIL.Image\n",
    "depth_image = Image.open(depth_path)\n",
    "\n",
    "# Step 2: Resize the image to 960x540\n",
    "desired_size = (960, 540)  # Width, Height\n",
    "depth_image_resized = depth_image.resize(desired_size, Image.NEAREST)  # Use NEAREST for depth maps\n",
    "\n",
    "# Step 3: Convert the resized PIL image to a NumPy array for further processing (optional)\n",
    "depth_map = np.array(depth_image_resized)\n",
    "\n",
    "# Step 4: Normalize the depth map for visualization (optional)\n",
    "depth_map_normalized = (depth_map - np.min(depth_map)) / (np.max(depth_map) - np.min(depth_map))\n",
    "\n",
    "# Step 5: Convert normalized data back to an image\n",
    "# Scale normalized data to 0-255 for display and convert to uint8\n",
    "depth_map_display = (depth_map_normalized * 255).astype(np.uint8)\n",
    "\n",
    "# Create a PIL Image from the scaled array\n",
    "depth_image_final = Image.fromarray(depth_map_display)\n",
    "\n",
    "# Step 6: Display the depth map using PIL\n",
    "depth_image_final.show(title='Depth Map Visualization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "access the depthmap and based on the pose compute the 3d points of this depthmap we do that for the new scan id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose in ref shape [[ 0.33013129 -0.85075646 -0.40893419  0.09321362]\n",
      " [-0.43478957 -0.52158175  0.73410511 -0.09337129]\n",
      " [-0.8378371  -0.06455041 -0.54209011 -0.19744018]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/ekoller/T7/Depth_Anything/422885ab-192d-25fc-8448-e7f34c7b5eea/frame-000004.pgm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m depth_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/ekoller/T7/Depth_Anything\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_scan_id,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pgm\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(frame_number))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#access the file\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m pgm_file \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#convert to np array\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#depth_mat_og = np.array(pgm_file)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#print(\"og map\", pgm_file)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#since its distances so discrete things take the nearest value not a different interpolation\u001b[39;00m\n\u001b[1;32m     42\u001b[0m obj_mat_size \u001b[38;5;241m=\u001b[39m (obj_mat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], obj_mat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \n",
      "File \u001b[0;32m~/anaconda3/envs/BT/lib/python3.8/site-packages/PIL/Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3227\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/ekoller/T7/Depth_Anything/422885ab-192d-25fc-8448-e7f34c7b5eea/frame-000004.pgm'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#frame_number = \"000003\"\n",
    "\"\"\" access the needed files and stuff like that\n",
    "\"\"\"\n",
    "#to do: look at the way the pose in reference is done!!!!!\n",
    "\n",
    "#access the gt projection object ids\n",
    "gt_obj_ids_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.color.jpg\".format(frame_number))\n",
    "#access the file\n",
    "obj_ids = Image.open(gt_obj_ids_path)\n",
    "#convert to np array\n",
    "obj_mat = np.array(obj_ids)\n",
    "\n",
    "#access the extrinsic/pose of the camera\n",
    "pose_rescan = scan3r.load_pose(osp.join(data_dir, \"scenes\"), new_scan_id, frame_number)\n",
    "pose_in_ref = pose_in_reference(data_dir, new_scan_id, pose_rescan)\n",
    "print(\"pose in ref shape\", pose_in_ref)\n",
    "\n",
    "#get the intrinsic of the camera\n",
    "# get img info and camera intrinsics \n",
    "\n",
    "#the intrinsics are saved the following way\n",
    "# intrinsic_mat = np.array([[intrinsic_fx, 0, intrinsic_cx],\n",
    "#                                     [0, intrinsic_fy, intrinsic_cy],\n",
    "#                                     [0, 0, 1]])\n",
    "camera_info = scan3r.load_intrinsics(scenes_dir, new_scan_id)\n",
    "intrinsics = camera_info['intrinsic_mat']\n",
    "img_width = int(camera_info['width'])\n",
    "img_height = int(camera_info['height'])\n",
    "\n",
    "\n",
    "#access the depht image\n",
    "depth_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.depth.pgm\".format(frame_number))\n",
    "depth_path = osp.join(\"/media/ekoller/T7/Depth_Anything\", new_scan_id,\"frame-{}.pgm\".format(frame_number))\n",
    "\n",
    "#access the file\n",
    "pgm_file = Image.open(depth_path)\n",
    "#convert to np array\n",
    "#depth_mat_og = np.array(pgm_file)\n",
    "#print(\"og map\", pgm_file)\n",
    "#since its distances so discrete things take the nearest value not a different interpolation\n",
    "obj_mat_size = (obj_mat.shape[1], obj_mat.shape[0]) \n",
    "print(\"obj mat size\", obj_mat_size)\n",
    "depth_mat_resized = pgm_file.resize(obj_mat_size, Image.NEAREST) \n",
    "#print(\"resized map\", depth_mat_resized)\n",
    "#depth is given in mm so put it into m\n",
    "depth_mat = np.array(depth_mat_resized)\n",
    "#depth_mat = np.array(pgm_file)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print(\"og depthmap\", depth_mat)\n",
    "depth_mat = depth_mat *0.001\n",
    "#print(\"final map\", depth_mat)\n",
    "#print(\"objects shape\", obj_mat.shape)\n",
    "\n",
    "\"\"\"\n",
    "do the computations based on following formula \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#from 2d to camera coordinates xc = (u-cx)*z / fx,   yc = (v-cy)*z/ fy    , zc= z \n",
    "\n",
    "\n",
    "#create a mesh grid since apparently that is how it is done lol\n",
    "u, v = np.meshgrid(np.arange(img_width), np.arange(img_height))\n",
    "\n",
    "#also access the intrinsic values\n",
    "# intrinsic_mat = np.array([[intrinsic_fx, 0, intrinsic_cx],\n",
    "#                                     [0, intrinsic_fy, intrinsic_cy],\n",
    "#                                     [0, 0, 1]])\n",
    "\n",
    "#for the gt\n",
    "fx = intrinsics[0, 0]  # Focal length in x direction\n",
    "fy = intrinsics[1, 1]  # Focal length in y direction\n",
    "cx = intrinsics[0, 2]  # Principal point x\n",
    "cy = intrinsics[1, 2]  # Principal point y\n",
    "\n",
    "print(fx,fy,cx,cy)\n",
    "#for the predicted one \n",
    "# fx = 470.4\n",
    "# fy = 470.4\n",
    "# cx = img_width /2\n",
    "# cy = img_height / 2\n",
    "\n",
    "#flatten everything for computations\n",
    "u_flat = u.flatten()\n",
    "v_flat = v.flatten()\n",
    "depth_flat = depth_mat.flatten()\n",
    "\n",
    "#apply the formula from above\n",
    "x_c = (u_flat - cx) * depth_flat / fx\n",
    "y_c = (v_flat - cy) * depth_flat / fy\n",
    "z_c = depth_flat\n",
    "\n",
    "#turn the camera coordinates into homogeneous coordinates\n",
    "camera_coords_homog  = np.vstack((x_c, y_c, z_c, np.ones_like(x_c)))  \n",
    "\n",
    "#apply the extrinsic matrix\n",
    "world_coords_homog =  pose_in_ref @ camera_coords_homog\n",
    "#normalize\n",
    "world_coords_homog /= world_coords_homog[3, :]  \n",
    "\n",
    "world_coords = world_coords_homog[:3,:]\n",
    "world_coords_T = world_coords.T\n",
    "print(\"computed world coords\" , world_coords.shape)\n",
    "print(\"computed world coords T\" , world_coords_T.shape)\n",
    "#normalize the colour of the gt\n",
    "rgb_array = np.array(obj_mat) / 255.0\n",
    "#access the colours \n",
    "rgb_colors = rgb_array[v_flat, u_flat]\n",
    "print(\"computed colours\" , rgb_colors.shape)\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "point_cloud.points = o3d.utility.Vector3dVector(np.array(world_coords_T))\n",
    "point_cloud.colors = o3d.utility.Vector3dVector(np.array(rgb_colors))\n",
    "#o3d.visualization.draw_geometries([point_cloud])\n",
    "\n",
    "# Set the voxel size (you can adjust this depending on the desired resolution)\n",
    "voxel_size = 0.08  # Adjust this value based on your needs\n",
    "\n",
    "# Apply voxel downsampling\n",
    "downsampled_point_cloud = point_cloud.voxel_down_sample(voxel_size=voxel_size)\n",
    "\n",
    "# Print the number of points before and after downsampling\n",
    "#print(\"Original point cloud size:\", len(point_cloud.points))\n",
    "#print(\"Downsampled point cloud size:\", len(downsampled_point_cloud.points))\n",
    "\n",
    "\n",
    "\n",
    "# Load the mesh\n",
    "pathToMesh = osp.join(data_dir, \"scenes\", new_scan_id, \"labels.instances.align.annotated.v2.ply\")\n",
    "new_mesh = o3d.io.read_triangle_mesh(pathToMesh)\n",
    "\n",
    "# Check if the mesh has colors\n",
    "if not new_mesh.has_vertex_colors():\n",
    "    print(\"Mesh does not have vertex colors\")\n",
    "    exit()\n",
    "\n",
    "# Normalize the mesh colors if necessary\n",
    "colors = np.asarray(new_mesh.vertex_colors)\n",
    "if np.max(colors) > 1.0:  # Assuming colors are in the range [0, 255]\n",
    "    colors /= 255.0\n",
    "\n",
    "# Swap color channels if necessary\n",
    "colors = colors[:, [2, 1, 0]]  # Swap red and blue channels\n",
    "new_mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "\"\"\" pointcloud pkl\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#also access the ground truth pointclouds\n",
    "pklfile = osp.join('/local/home/ekoller/R3Scan', 'files', 'orig', 'data', '{}.pkl'.format(new_scan_id))\n",
    "\n",
    "with open(pklfile, \"rb\") as f:\n",
    "    # Load the data from the pickle file\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "    # Extract object points and IDs from the pickle data\n",
    "    pointclouds = data['obj_points'][512]\n",
    "    object_ids = data['objects_id']\n",
    "    \n",
    "    # Create a new point cloud object for the .pkl data\n",
    "    pkl_point_cloud = o3d.geometry.PointCloud()\n",
    "    obj_colors = get_id_colours(data_dir, new_scan_id)\n",
    "    # Iterate over all the points and their object IDs\n",
    "    for obj_id, points in zip(object_ids, pointclouds):\n",
    "        # Assign a default color (e.g., black) to these objects\n",
    "        #print(\"object id\", obj_id)\n",
    "        color = obj_colors[obj_id]/225.0\n",
    "        #print(\"clour\", obj_colors[obj_id]/225.0)\n",
    "        # Create a point cloud for this object's points\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(points)\n",
    "        pcd.colors = o3d.utility.Vector3dVector(np.tile(color, (points.shape[0], 1)))  # Assign the color\n",
    "\n",
    "        # Add this point cloud to the combined pkl_point_cloud\n",
    "        pkl_point_cloud += pcd\n",
    "# Display both the mesh and point cloud together\n",
    "o3d.visualization.draw_geometries([point_cloud, new_mesh], window_name=\"Mesh and Point Cloud Display\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this part now tries to get the depthmaps of the same object in 2 frames and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dino generated mask\n",
    "#returns featuer in the form of features: frame: list of {objext_id, bbox, mask} objects\n",
    "def read_segmentation_data(segmentation_path):\n",
    "    features = {}\n",
    "    with h5py.File(segmentation_path, 'r') as hdf_file:\n",
    "            for frame_idx in hdf_file.keys():\n",
    "                #init boxlist for curr frame\n",
    "                bounding_boxes = []\n",
    "                \n",
    "                # get info \n",
    "                frame_group = hdf_file[frame_idx]\n",
    "                \n",
    "                #iterate over each boundingbox\n",
    "                for bbox_key in frame_group.keys():\n",
    "                    bbox_group = frame_group[bbox_key]\n",
    "                    \n",
    "                    #get te obj id\n",
    "                    object_id = bbox_group.attrs['object_id']\n",
    "                    \n",
    "                    #get the boundingbox\n",
    "                    bbox = bbox_group['bbox'][:]\n",
    "                    \n",
    "                    # get the maskt\n",
    "                    mask = bbox_group['mask'][:]\n",
    "                    \n",
    "                    # append to list\n",
    "                    bounding_boxes.append({\n",
    "                        'object_id': object_id,\n",
    "                        'bbox': bbox,\n",
    "                        'mask': mask\n",
    "                    })\n",
    "                \n",
    "                # stor it to the corresponding frame\n",
    "                \n",
    "                features[frame_idx] = bounding_boxes\n",
    "    return features\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform to reference coordinate system\n",
    "def transform_to_3d(data_dir, scenes_dir, scan_id, depth_map, colour_map, frame_number):\n",
    "    \"\"\" access the needed files and stuff like that\n",
    "    \"\"\"\n",
    "   \n",
    "    #access the extrinsic/pose of the camera\n",
    "    pose_rescan = scan3r.load_pose(osp.join(data_dir, \"scenes\"), scan_id, frame_number)\n",
    "    pose_in_ref = pose_in_reference(data_dir, scan_id, pose_rescan)\n",
    "    \n",
    "    camera_info = scan3r.load_intrinsics(scenes_dir, scan_id)\n",
    "    intrinsics = camera_info['intrinsic_mat']\n",
    "    img_width = int(camera_info['width'])\n",
    "    img_height = int(camera_info['height'])\n",
    "\n",
    "    \"\"\"\n",
    "    do the computations based on following formula \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #from 2d to camera coordinates xc = (u-cx)*z / fx,   yc = (v-cy)*z/ fy    , zc= z \n",
    "\n",
    "\n",
    "    #create a mesh grid since apparently that is how it is done lol\n",
    "    u, v = np.meshgrid(np.arange(img_width), np.arange(img_height))\n",
    "\n",
    "    #also access the intrinsic values\n",
    "    # intrinsic_mat = np.array([[intrinsic_fx, 0, intrinsic_cx],\n",
    "    #                                     [0, intrinsic_fy, intrinsic_cy],\n",
    "    #                                     [0, 0, 1]])\n",
    "\n",
    "    fx = intrinsics[0, 0]  # Focal length in x direction\n",
    "    fy = intrinsics[1, 1]  # Focal length in y direction\n",
    "    cx = intrinsics[0, 2]  # Principal point x\n",
    "    cy = intrinsics[1, 2]  # Principal point y\n",
    "    #flatten everything for computations\n",
    "    u_flat = u.flatten()\n",
    "    v_flat = v.flatten()\n",
    "    depth_flat = depth_map.flatten()\n",
    "\n",
    "    #apply the formula from above\n",
    "    x_c = (u_flat - cx) * depth_flat / fx\n",
    "    y_c = (v_flat - cy) * depth_flat / fy\n",
    "    z_c = depth_flat\n",
    "\n",
    "    #turn the camera coordinates into homogeneous coordinates\n",
    "    camera_coords_homog  = np.vstack((x_c, y_c, z_c, np.ones_like(x_c)))  \n",
    "\n",
    "    #apply the extrinsic matrix\n",
    "    world_coords_homog = pose_in_ref @ camera_coords_homog\n",
    "    #normalize\n",
    "    world_coords_homog /= world_coords_homog[3, :]  \n",
    "\n",
    "    world_coords = world_coords_homog[:3,:]\n",
    "    world_coords_T = world_coords.T\n",
    "    # print(\"computed world coords\" , world_coords.shape)\n",
    "    # print(\"computed world coords T\" , world_coords_T.shape)\n",
    "    #normalize the colour of the gt\n",
    "    rgb_array = np.array(colour_map) / 255.0\n",
    "    #access the colours \n",
    "    rgb_colors = rgb_array[v_flat, u_flat]\n",
    "   \n",
    "\n",
    "    return world_coords_T, rgb_colors\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxel_grid_to_coordinates(voxel_grid):\n",
    "    \"\"\"Extract voxel coordinates from a VoxelGrid object.\"\"\"\n",
    "    voxels = voxel_grid.get_voxels()\n",
    "    voxel_coords = np.array([voxel.grid_index for voxel in voxels])\n",
    "    return voxel_coords\n",
    "\n",
    "\n",
    "def compare_voxel_grids(voxel_grid1, voxel_grid2):\n",
    "    \"\"\"Compare two voxel grids to see how much they overlap.\"\"\"\n",
    "    coords1 = voxel_grid_to_coordinates(voxel_grid1)\n",
    "    coords2 = voxel_grid_to_coordinates(voxel_grid2)\n",
    "    \n",
    "    # Convert to sets of tuples for intersection\n",
    "    voxels1_set = set(map(tuple, coords1))\n",
    "    voxels2_set = set(map(tuple, coords2))\n",
    "    \n",
    "    # Compute intersection\n",
    "    intersection = voxels1_set.intersection(voxels2_set)\n",
    "    union = voxels1_set.union(voxels2_set)\n",
    "    \n",
    "    similarity = len(intersection) / len(union) if len(union) > 0 else 0\n",
    "    return similarity\n",
    "\n",
    "# def create_bounding_box_lines(bbox):\n",
    "#     # Create a LineSet for the bounding box\n",
    "#     lines = [\n",
    "#         [0, 1], [1, 2], [2, 3], [3, 0], # bottom face\n",
    "#         [4, 5], [5, 6], [6, 7], [7, 4], # top face\n",
    "#         [0, 4], [1, 5], [2, 6], [3, 7]  # vertical lines\n",
    "#     ]\n",
    "#     # Convert the bounding box min and max bounds to corner points\n",
    "#     min_bound = bbox.get_min_bound()\n",
    "#     max_bound = bbox.get_max_bound()\n",
    "\n",
    "#     corners = [\n",
    "#         [min_bound[0], min_bound[1], min_bound[2]],\n",
    "#         [max_bound[0], min_bound[1], min_bound[2]],\n",
    "#         [max_bound[0], max_bound[1], min_bound[2]],\n",
    "#         [min_bound[0], max_bound[1], min_bound[2]],\n",
    "#         [min_bound[0], min_bound[1], max_bound[2]],\n",
    "#         [max_bound[0], min_bound[1], max_bound[2]],\n",
    "#         [max_bound[0], max_bound[1], max_bound[2]],\n",
    "#         [min_bound[0], max_bound[1], max_bound[2]]\n",
    "#     ]\n",
    "\n",
    "#     lines_set = o3d.geometry.LineSet()\n",
    "#     lines_set.points = o3d.utility.Vector3dVector(corners)\n",
    "#     lines_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "#     return \n",
    "\n",
    "\n",
    "# def check_mesh_intersection(mesh1, mesh2):\n",
    "#     # Check if two meshes intersect\n",
    "#     mesh1.compute_triangle_normals()\n",
    "#     mesh2.compute_triangle_normals()\n",
    "#     mesh1_tree = o3d.geometry.KDTreeFlann(mesh1)\n",
    "#     mesh2_tree = o3d.geometry.KDTreeFlann(mesh2)\n",
    "    \n",
    "#     # Check for intersection by sampling points from the first mesh and searching for them in the second mesh\n",
    "#     for point in mesh1.vertices:\n",
    "#         [_, idx, _] = mesh2_tree.search_knn_vector_3d(point, 1)\n",
    "#         if len(idx) > 0:\n",
    "#             return True\n",
    "#     return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the matched data\n",
    "\n",
    "\n",
    "voxel_size_overlap = 0.2\n",
    "def read_matching_data(scan_id):\n",
    "        # get the file and iterate through everything to create an object\n",
    "        matchfile = osp.join(\"/media/ekoller/T7/Predicted_Matches\", scan_id + \".h5\")\n",
    "        with h5py.File(matchfile, 'r') as hdf_file:\n",
    "            loaded_matches = {}\n",
    "            \n",
    "            # Iterate through frame indices\n",
    "            for frame_idx in hdf_file.keys():\n",
    "                matches = {}\n",
    "                \n",
    "                # Access the group for each frame index\n",
    "                frame_group = hdf_file[frame_idx]\n",
    "                \n",
    "                # Load the frame_id -> obj mappings\n",
    "                for frame_id in frame_group.keys():\n",
    "                    obj = frame_group[frame_id][()]\n",
    "                    matches[frame_id] = int(obj)  # Convert back to int\n",
    "                \n",
    "                loaded_matches[frame_idx] = matches \n",
    "\n",
    "        return loaded_matches\n",
    "\n",
    "\n",
    "def isolate_object_coordinates(world_coordinates, mask):\n",
    "        #make sure it is an array\n",
    "        mask = np.array(mask)\n",
    "        #print(\"points shape\", world_coordinates.shape)\n",
    "        #flatten & turn into boolean mask\n",
    "        mask = mask.flatten()\n",
    "        #print(\"mask shape\", mask.shape)\n",
    "        mask = mask.astype(bool)\n",
    "        #get the part belonging to the object\n",
    "        obj_coordinates = world_coordinates[mask]\n",
    "\n",
    "        return obj_coordinates\n",
    "\n",
    "\n",
    "\n",
    "def do_pcl_overlap(obj_pcl, cluster):\n",
    "        #create a voxel grid\n",
    "        #turn into pointclouds\n",
    "        obj_point_cloud = o3d.geometry.PointCloud()\n",
    "        obj_point_cloud.points = o3d.utility.Vector3dVector(obj_pcl)\n",
    "        voxel_grid1 = o3d.geometry.VoxelGrid.create_from_point_cloud(obj_point_cloud, voxel_size_overlap)\n",
    "\n",
    "        cluster_point_cloud = o3d.geometry.PointCloud()\n",
    "        cluster_point_cloud.points = o3d.utility.Vector3dVector(cluster)\n",
    "        voxel_grid2 = o3d.geometry.VoxelGrid.create_from_point_cloud(cluster_point_cloud, voxel_size_overlap)\n",
    "    \n",
    "        \n",
    "        \"\"\"Compare two voxel grids to see how much they overlap.\"\"\"\n",
    "        return compare_voxel_grids(voxel_grid1, voxel_grid2)\n",
    "\n",
    "\n",
    "def compare_voxel_grids(voxel_grid1, voxel_grid2):\n",
    "        \"\"\"Compare two voxel grids to see how much they overlap.\"\"\"\n",
    "        coords1 = voxel_grid_to_coordinates(voxel_grid1)\n",
    "        coords2 = voxel_grid_to_coordinates(voxel_grid2)\n",
    "        \n",
    "        # Convert to sets of tuples for intersection\n",
    "        voxels1_set = set(map(tuple, coords1))\n",
    "        voxels2_set = set(map(tuple, coords2))\n",
    "        \n",
    "        # Compute intersection\n",
    "        intersection = voxels1_set.intersection(voxels2_set)\n",
    "        union = voxels1_set.union(voxels2_set)\n",
    "        \n",
    "        similarity = len(intersection) / len(union) if len(union) > 0 else 0\n",
    "        return similarity\n",
    "\n",
    "def visualize_cluster_open3d(cluster_points, colors):\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(cluster_points)\n",
    "\n",
    "    # Set the colors for each point\n",
    "    point_cloud.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    # Visualize using Open3D\n",
    "    o3d.visualization.draw_geometries([point_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color dict dict_keys([1, 10, 5, 4, 8, 9, 2, 0, 12, 11, 6, 7, 3, 18, 17, 19, 16, 22, 21, 15, 14, 13])\n",
      "matches {'000000': {'1': -1, '2': 10, '3': 3, '4': -2}, '000001': {'1': -1, '2': 11, '3': -2, '4': 10, '5': 3, '6': 11, '7': -3, '8': -4}, '000002': {'1': -1, '2': 11, '3': 18, '4': 1, '5': 3, '6': 11, '7': -2}, '000003': {'1': 11, '2': 12, '3': 18, '4': 1, '5': 3, '6': 4, '7': -1, '8': 12}, '000004': {'1': 11, '2': 12, '3': -1, '4': 1, '5': 17, '6': 3, '7': -2, '8': -3, '9': -4}, '000005': {'1': 15, '2': 11, '3': 12, '4': 1, '5': 10, '6': 3, '7': 14}, '000006': {'1': 14, '2': 12, '3': 1, '4': 10, '5': 3}, '000007': {'1': 14, '2': -1, '3': 10, '4': 10, '5': 3, '6': -2, '7': 12}, '000008': {'1': -1, '2': 10, '3': 10, '4': 3, '5': -2, '6': 5, '7': -3}, '000009': {'1': 9, '2': 10, '3': 3, '4': -1, '5': -2}, '000010': {'1': 10, '2': 10, '3': 2, '4': -1}, '000011': {'1': 10, '2': 9, '3': 10, '4': -1, '5': 3}, '000012': {'1': 10}, '000013': {'1': 10, '2': -1, '3': 10}, '000014': {'1': 10, '2': -1, '3': 10, '4': 10}, '000015': {'1': 10}, '000016': {'1': -1, '2': 10}, '000017': {'1': 9, '2': 10, '3': 10, '4': 10}, '000018': {'1': 10, '2': 3, '3': 3}, '000019': {'1': -1, '2': 3}, '000020': {'1': 10}, '000021': {'1': 10, '2': 3}, '000022': {'1': 5, '2': 5}, '000023': {'1': -1, '2': 5}, '000024': {'1': 11, '2': 18, '3': 1, '4': 4, '5': -1}, '000025': {'1': 11, '2': 18, '3': 1, '4': 1, '5': -1, '6': 3, '7': -2}, '000026': {'1': 11, '2': 18, '3': 1, '4': 3, '5': 4, '6': 12, '7': -1}, '000027': {'1': 15, '2': 11, '3': 1, '4': 3, '5': 12, '6': -1, '7': 14}, '000028': {'1': 15, '2': 1, '3': 16, '4': 10, '5': 3, '6': -1, '7': 12}, '000029': {'1': 15, '2': 1, '3': 16, '4': 10, '5': 3, '6': 12, '7': -1}, '000030': {'1': 16, '2': 1, '3': 10, '4': 3, '5': 12, '6': -1, '7': -2}, '000031': {'1': -1, '2': 1, '3': 10, '4': 3, '5': 12, '6': -2, '7': 14, '8': 16}, '000032': {'1': 11, '2': 1, '3': 17, '4': 4, '5': -1, '6': -2, '7': 12, '8': -3}, '000033': {'1': 11, '2': -1, '3': 1, '4': 4, '5': -2}, '000034': {'1': 1, '2': 5, '3': 11}, '000035': {'1': 1, '2': 5}, '000036': {'1': 6, '2': 1, '3': 4}, '000037': {'1': 6, '2': 6}, '000038': {'1': 5, '2': 6, '3': 4, '4': -1}, '000039': {'1': 6, '2': 6}, '000040': {'1': 6, '2': -1, '3': 6, '4': -2}, '000041': {'1': 6, '2': 10, '3': 6}, '000042': {'1': 6, '2': 10, '3': 6}, '000043': {'1': 6}, '000044': {'1': 6, '2': 10, '3': 10, '4': 10, '5': -1, '6': 5}, '000045': {'1': 10, '2': 10, '3': 3}, '000046': {'1': -1, '2': 10}, '000047': {'1': 10, '2': 10, '3': 10}, '000048': {'1': 10, '2': 10, '3': 10}, '000049': {'1': 10}, '000050': {'1': 10, '2': 10}, '000051': {'1': 10, '2': 10}, '000052': {'1': 10, '2': 10, '3': 10}, '000053': {'1': 10, '2': 1, '3': 10, '4': 5, '5': -1, '6': -2}, '000054': {'1': -1, '2': 1, '3': 10, '4': 5, '5': -2}, '000055': {'1': -1, '2': 1, '3': 10, '4': 5, '5': -2, '6': -3}, '000056': {'1': -1, '2': 1, '3': 10, '4': 5, '5': -2}, '000057': {'1': -1, '2': -2, '3': -3, '4': 1, '5': 10, '6': 2}, '000058': {'1': -1, '2': 1, '3': 10, '4': 5, '5': 16, '6': -2}, '000059': {'1': 13, '2': 11, '3': 1, '4': -1, '5': -2, '6': 12}, '000060': {'1': 11, '2': 1, '3': 4, '4': 12, '5': -1}, '000061': {'1': 1, '2': 4, '3': -1, '4': 11}, '000062': {'1': 1, '2': 5, '3': -1}, '000063': {'1': 1, '2': 5}, '000064': {'1': 1, '2': 5}, '000065': {'1': 6, '2': 1, '3': 5, '4': 5}, '000066': {'1': 6, '2': 1, '3': -1, '4': 5}, '000067': {'1': 6, '2': 1, '3': 8, '4': 5}}\n"
     ]
    }
   ],
   "source": [
    "#new_scan_id = \"0cac761d-8d6f-2d13-8f35-2364ee20f2a9\"\n",
    "color_dict = get_id_colours(data_dir, curr_scan_id)\n",
    "print(\"color dict\", color_dict.keys())\n",
    "matches = read_matching_data(new_scan_id)\n",
    "print(\"matches\", matches)\n",
    "all_clusters ={}\n",
    "overlap_threshold = 0.5\n",
    "#new_scan_id = \"0cac761d-8d6f-2d13-8f35-2364ee20f2a9\" #\"fcf66d8a-622d-291c-8429-0e1109c6bb26\" #\"fcf66d9e-622d-291c-84c2-bb23dfe31327\"\n",
    "\n",
    "#access the dino segmentation\n",
    "segmentation_info_path = osp.join(\"/media/ekoller/T7/Segmentation/DinoV2/objects\", new_scan_id + \".h5\")\n",
    "segmentation_data = read_segmentation_data(segmentation_info_path)\n",
    "\n",
    "for frame_idx in matches.keys():\n",
    "    #print(\"frame idx\", frame_idx)\n",
    "\n",
    "    #access the matches for this frame\n",
    "    frame_matches = matches[frame_idx]\n",
    "    #access the depht image\n",
    "    depth_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.depth.pgm\".format(frame_idx))\n",
    "    #access the file\n",
    "    pgm_file = Image.open(depth_path)\n",
    "\n",
    "    #since its distances so discrete things take the nearest value not a different interpolation\n",
    "    depth_mat_resized = pgm_file.resize((img_width, img_height), Image.NEAREST) \n",
    "\n",
    "    #depth is given in mm so put it into m\n",
    "    depth_mat = np.array(depth_mat_resized)\n",
    "    depth_mat = depth_mat * 0.001\n",
    "    #print(\"depth map size\", depth_mat.shape)\n",
    "\n",
    "    #access the colour map\n",
    "    rgb_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.color.jpg\".format(frame_idx))\n",
    "    #access the file\n",
    "    rgb_file = Image.open(rgb_path)\n",
    "\n",
    "    rgb_mat = np.array(rgb_file)\n",
    "\n",
    "    #transform to world coordinates in the reference frame\n",
    "    world_coordinates_frame, colour_coordinates = transform_to_3d(data_dir,scenes_dir,new_scan_id, depth_mat, rgb_mat,frame_idx)\n",
    "    \n",
    "\n",
    "    # \"viszalize\"\n",
    "    # point_cloud = o3d.geometry.PointCloud()\n",
    "    # point_cloud.points = o3d.utility.Vector3dVector(np.array(world_coordinates_frame))\n",
    "    # point_cloud.colors = o3d.utility.Vector3dVector(np.array(colour_coordinates))\n",
    "    # #o3d.visualization.draw_geometries([point_cloud])\n",
    "\n",
    "    # # Set the voxel size (you can adjust this depending on the desired resolution)\n",
    "    # voxel_size = 0.08  # Adjust this value based on your needs\n",
    "\n",
    "    # # Apply voxel downsampling\n",
    "    # downsampled_point_cloud = point_cloud.voxel_down_sample(voxel_size=voxel_size)\n",
    "\n",
    "    # # Print the number of points before and after downsampling\n",
    "    # #print(\"Original point cloud size:\", len(point_cloud.points))\n",
    "    # #print(\"Downsampled point cloud size:\", len(downsampled_point_cloud.points))\n",
    "\n",
    "\n",
    "\n",
    "    # # Load the mesh\n",
    "    # pathToMesh = osp.join(data_dir, \"scenes\", new_scan_id, \"labels.instances.align.annotated.v2.ply\")\n",
    "    # new_mesh = o3d.io.read_triangle_mesh(pathToMesh)\n",
    "\n",
    "    # # Check if the mesh has colors\n",
    "    # if not new_mesh.has_vertex_colors():\n",
    "    #     print(\"Mesh does not have vertex colors\")\n",
    "    #     exit()\n",
    "\n",
    "    # # Normalize the mesh colors if necessary\n",
    "    # colors = np.asarray(new_mesh.vertex_colors)\n",
    "    # if np.max(colors) > 1.0:  # Assuming colors are in the range [0, 255]\n",
    "    #     colors /= 255.0\n",
    "\n",
    "    # # Swap color channels if necessary\n",
    "    # colors = colors[:, [2, 1, 0]]  # Swap red and blue channels\n",
    "    # new_mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "    # o3d.visualization.draw_geometries([point_cloud, new_mesh], window_name=\"Mesh and Point Cloud Display\")\n",
    "\n",
    "    \"end viszalize\"\n",
    "    #iterate through the masks of the objec\n",
    "    segmented_img_path = osp.join(\"/media/ekoller/T7/Segmentation/DinoV2/color\", new_scan_id, \"frame-{}.jpg\".format(frame_idx))\n",
    "    segmented_img = cv2.imread(segmented_img_path)\n",
    "               \n",
    "    for boundingboxes in segmentation_data[frame_idx]:\n",
    "\n",
    "        mask = boundingboxes['mask']\n",
    "        mask = mask.astype(bool)\n",
    "        # print(\"mask box\", mask)\n",
    "        #print(\"Mask shape:\", mask.shape)\n",
    "        # print(\"Mask dtype:\", mask.dtype)    \n",
    "        #access the patches withing the segmented image\n",
    "        masked_region = segmented_img[mask]\n",
    "        #determin the most occuring colour\n",
    "        colors_in_region = list(map(tuple, masked_region.reshape(-1, segmented_img.shape[-1])))\n",
    "        most_frequent_color = Counter(colors_in_region).most_common(1)[0][0]\n",
    "        #create a mask of the colour in the whole image\n",
    "        color_mask = np.all(segmented_img == most_frequent_color, axis=-1)\n",
    "        # print(\"color mask \", color_mask)\n",
    "        #we only want the mask for the region of the first region\n",
    "        result_mask = color_mask & mask\n",
    "        #access the mask for the object\n",
    "        mask = boundingboxes['mask']\n",
    "\n",
    "        #get the dino object_id \n",
    "        dino_id = boundingboxes[\"object_id\"]\n",
    "        #print(\"frame \", frame_idx, \" dino_id \", dino_id)\n",
    "        #get the matched id\n",
    "        object_id = frame_matches[str(dino_id)]\n",
    "        #print(\"matched id \", object_id)\n",
    "        \n",
    "        \n",
    "\n",
    "        #isolate only the object pointcloud\n",
    "        obj_pcl = isolate_object_coordinates(world_coordinates_frame, result_mask)\n",
    "        obj_colour = isolate_object_coordinates(colour_coordinates, result_mask)\n",
    "        #not a new object so regular precedure\n",
    "        if object_id > 0:\n",
    "            #now we need to find out if we add it to the pointcloud of the object it mapped to or not\n",
    "            if object_id not in all_clusters:\n",
    "                #print(\"create new cluster for objct id \", object_id)\n",
    "                #print(\"create first cluter obj_id \", object_id)\n",
    "                #there are no clusters & votes stored for this object jet\n",
    "                all_clusters[object_id] = [{'cluster': obj_pcl, 'votes': 1, \"colour\": obj_colour}]\n",
    "                \n",
    "                #visualize_cluster_open3d(all_clusters[object_id][0][\"cluster\"],all_clusters[object_id][0][\"colour\"] )\n",
    "                \n",
    "            #object already has pointclouds we need to see if we merge or add a new cluster\n",
    "            else:\n",
    "                #each new cluster starts unmerged\n",
    "                merged = False\n",
    "                max_overlap = 0\n",
    "                best_cluster_index = None\n",
    "                for i, cluster_data in enumerate(all_clusters[object_id]):\n",
    "                    cluster = cluster_data['cluster']\n",
    "\n",
    "                    #add to the cluster with the most overlap\n",
    "                    overlap = do_pcl_overlap(obj_pcl, cluster)\n",
    "\n",
    "                    # keep track of the most overlap cluste\n",
    "                    if overlap > overlap_threshold and overlap > max_overlap:\n",
    "                        max_overlap = overlap\n",
    "                        best_cluster_index = i\n",
    "\n",
    "                if best_cluster_index is not None:\n",
    "                    # Merge the point clouds with the best cluster\n",
    "                    #print(\"merge cluster for object id\", object_id)\n",
    "                    best_cluster = all_clusters[object_id][best_cluster_index]['cluster']\n",
    "                    merged_points = np.vstack((obj_pcl, best_cluster))\n",
    "                    merged_rgb = np.vstack((obj_colour,all_clusters[object_id][best_cluster_index]['colour']))\n",
    "                    \n",
    "                    # Update the best cluster with the merged points\n",
    "                    all_clusters[object_id][best_cluster_index]['cluster'] = merged_points\n",
    "                    all_clusters[object_id][best_cluster_index]['colour'] = merged_rgb\n",
    "     \n",
    "                    #visualize_cluster_open3d(merged_points,merged_rgb)\n",
    "                    # Increment the vote count for the best cluster\n",
    "                    all_clusters[object_id][best_cluster_index]['votes'] += 1\n",
    "\n",
    "                    # Mark as merged\n",
    "                    merged = True\n",
    "                if not merged:\n",
    "                    #print(\"not merged, create new cluster for obj id\", object_id)\n",
    "                    all_clusters[object_id].append({'cluster': obj_pcl, 'votes': 1, \"colour\": obj_colour})\n",
    "        #new object\n",
    "        else:\n",
    "            #print(\"working with new ids\")\n",
    "            #get the negative keys\n",
    "            negative_keys = [object_id for object_id in all_clusters.keys() if object_id < 0]\n",
    "            new_obj_idx = 0\n",
    "            #no negative keys yet\n",
    "            if len(negative_keys) == 0:\n",
    "                new_obj_idx = new_obj_idx - 1\n",
    "                all_clusters[new_obj_idx] = [{'cluster': obj_pcl, 'votes': 1}]\n",
    "            #since we don't know the correspondance of the points we just add id to the new cluster with the most points\n",
    "            else:\n",
    "                max_overlap = 0\n",
    "                best_cluster_index = None\n",
    "                best_object_id = None\n",
    "\n",
    "                # iterate over every cluster to get the one with the most overlap\n",
    "                for neg_key in negative_keys:\n",
    "                    for i, cluster_data in enumerate(all_clusters[neg_key]):\n",
    "                        cluster = cluster_data['cluster']\n",
    "                        overlap = do_pcl_overlap(obj_pcl, cluster)\n",
    "\n",
    "                        # Track the cluster with the highest overlap\n",
    "                        if overlap > overlap_threshold and overlap > max_overlap:\n",
    "                            max_overlap = overlap\n",
    "                            best_cluster_index = i\n",
    "                            best_object_id = neg_key\n",
    "\n",
    "                # we found a best cluster so merge it\n",
    "                if best_object_id is not None and best_cluster_index is not None:\n",
    "                    best_cluster = all_clusters[best_object_id][best_cluster_index]['cluster']\n",
    "                    merged_points = np.vstack((obj_pcl, best_cluster))\n",
    "\n",
    "                    # Update the best cluster with the merged points\n",
    "                    all_clusters[best_object_id][best_cluster_index]['cluster'] = merged_points\n",
    "\n",
    "                    # increment the vote\n",
    "                    all_clusters[best_object_id][best_cluster_index]['votes'] += 1\n",
    "                else:\n",
    "                    # did not find a good cluster create a new one\n",
    "                    new_obj_idx = new_obj_idx -1\n",
    "                    all_clusters[new_obj_idx] = [{'cluster': obj_pcl, 'votes': 1}]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"clusters keys\", all_clusters.keys())\n",
    "# print(\"all clusters\", all_clusters)\n",
    "#now that we have the lists of clusters we need to iterate over them and choose the biggest cluster, downsample it & take the average to predict the center\n",
    "#initialize final object\n",
    "all_centers = {}\n",
    "#iterte through the objects\n",
    "for obj_id, clusters in all_clusters.items():\n",
    "    #print(\"in the for loop with cluster id\", obj_id)\n",
    "    #get the cluster with the most points aka largest \n",
    "    #print(\"clusters\", clusters , \"for object id \" ,obj_id)\n",
    "    #decide the most likely correct cluster based on votes first and then size\n",
    "    largest_cluster_data = max(all_clusters[obj_id], key=lambda c: (c['votes'], len(c['cluster'])))\n",
    "    largest_cluster = largest_cluster_data['cluster']\n",
    "    largest_cluster_votes = largest_cluster_data[\"votes\"]\n",
    "    #print(\"shape of largest cluster\", largest_cluster.shape)\n",
    "    # Compute the mean (centroid) of the inliers\n",
    "    obj_center = np.mean(largest_cluster, axis=0)\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #return the object for the evaluation\n",
    "    all_centers[obj_id] = {\n",
    "        'center': obj_center,\n",
    "        \"size\": len(largest_cluster),\n",
    "        \"votes\" : largest_cluster_votes,\n",
    "        \"points\": largest_cluster\n",
    "\n",
    "    }\n",
    "    #print(all_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in the cluster: 326030\n"
     ]
    }
   ],
   "source": [
    "obj_id = 11\n",
    "if obj_id in all_centers:\n",
    "    points = all_centers[obj_id]['points']  # largest_cluster points for obj_id == 11\n",
    "    print(f\"Number of points in the cluster: {len(points)}\")\n",
    "\n",
    "    # Step 2: Create an Open3D PointCloud object\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "\n",
    "    # Convert the points to Open3D-compatible format (numpy array)\n",
    "    points_np = np.array(points)\n",
    "\n",
    "    # Assign points to the PointCloud object\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(points_np)\n",
    "\n",
    "    # Optionally, you can color the point cloud (e.g., assign random colors or a solid color)\n",
    "    # Here, we're assigning a fixed color to all points (e.g., blue)\n",
    "    colors = np.tile([0.5803922 , 0.6117647 , 0.76862746], (points_np.shape[0], 1))  # Blue color for all points\n",
    "    point_cloud.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    # Step 3: Visualize the point cloud\n",
    "    o3d.visualization.draw_geometries([point_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acces_predicted_objects(scan_id):\n",
    "    all_centers = {}\n",
    "    filename = osp.join(\"/media/ekoller/T7/Predicted_Centers\", scan_id + \".h5\")\n",
    "    with h5py.File(filename, 'r') as h5file:\n",
    "        for obj_id in h5file.keys():\n",
    "            obj_group = h5file[obj_id]\n",
    "            center = np.array(obj_group['center'])\n",
    "            points = np.array(obj_group['points'])\n",
    "            votes = int(obj_group['votes'][()])\n",
    "            \n",
    "            # Add it back to the dictionary\n",
    "            all_centers[int(obj_id)] = {\n",
    "                'center': center,\n",
    "                'points': points,\n",
    "                'votes': votes\n",
    "            }\n",
    "    return all_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all centers 7\n",
      "predicted centers <built-in method keys of dict object at 0x7f00d4122200>\n",
      "number of points (626, 3)\n",
      "center [ 0.69273879  0.389142   -1.29052939]\n",
      "min coordinates [-1.1716208  -0.54547703 -1.3355314 ]\n",
      "max coords [ 2.3581793  1.3605591 -0.8642052]\n",
      "is inside True\n",
      "number of points (436, 3)\n",
      "center [ 0.29162647  0.87564118 -0.86820971]\n",
      "min coordinates [-1.2612445   0.32114795 -1.3115963 ]\n",
      "max coords [ 1.9931948   1.4241216  -0.35267553]\n",
      "is inside True\n",
      "number of points (284, 3)\n",
      "center [ 0.85983828  0.98590419 -0.91150189]\n",
      "min coordinates [ 0.6486233   0.45354584 -1.0384165 ]\n",
      "max coords [ 1.1712588   1.091316   -0.50186944]\n",
      "is inside True\n",
      "number of points (368, 3)\n",
      "center [-0.25172658  0.59049419 -0.93772047]\n",
      "min coordinates [-0.4721623   0.01908122 -1.0174179 ]\n",
      "max coords [ 0.12026805  0.6570657  -0.51299965]\n",
      "is inside True\n",
      "number of points (172, 3)\n",
      "center [ 0.3179398  0.7688555 -1.0895531]\n",
      "min coordinates [ 0.21653768  0.68498415 -1.3009408 ]\n",
      "max coords [ 0.6187489  0.9530291 -0.8449862]\n",
      "is inside True\n",
      "number of points (83, 3)\n",
      "center [-1.04748813  0.04340473 -1.18621239]\n",
      "min coordinates [-1.3877262  -0.26969874 -1.2920151 ]\n",
      "max coords [-0.93481463  0.3591101  -0.87083954]\n",
      "is inside True\n",
      "number of points (40, 3)\n",
      "center [ 1.03388691  1.06391043 -0.40878363]\n",
      "min coordinates [-0.6885713   0.49524286 -0.5211122 ]\n",
      "max coords [ 1.4508706   1.2108164  -0.27118745]\n",
      "is inside True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "\"to doooooooo!!!!!!\"\n",
    "so basically we saw that the votes are not the most imoortant thing but rather the point sizes so adjust the computation for the following things: votes\n",
    "threshold for the size of downsampling; 0.05,0.1\n",
    "number of points[10,20,30,40,50,60]\n",
    "threshold for the overlap of the region [0.1,0.3,0.5,0.7,0.9]\n",
    "threshold for k nearest neigbour: [1,2,3,4,5,,6,7,8,9]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "all_centers = acces_predicted_objects(new_scan_id)\n",
    "print(\"all centers\", len(all_centers))\n",
    "\"to doooooooo!!!!!!\"\n",
    " #access gt pointcenters\n",
    "pklfile = osp.join(data_dir, 'files', 'orig', 'data', '{}.pkl'.format(new_scan_id))\n",
    "\n",
    "with open(pklfile, \"rb\") as f:\n",
    "    # Load the data from the pickle file\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "# extract object points and IDs from the pickle data\n",
    "gt_ids = data['objects_id']\n",
    "gt_centers = data[\"object_centers\"]\n",
    "gt_boxes = data['bounding_boxes']\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "New_scan_id mesh display in original coordinate system (rescan coord)\n",
    "\"\"\"\n",
    "pathToMesh = osp.join(data_dir,\"scenes\", new_scan_id, \"labels.instances.align.annotated.v2.ply\")\n",
    "new_mesh = o3d.io.read_triangle_mesh(pathToMesh)\n",
    "# Visualize the mesh\n",
    "\n",
    "#check if it has colours\n",
    "if not new_mesh.has_vertex_colors():\n",
    "    print(\"Mesh does not have vertex colors\")\n",
    "    exit()\n",
    "\n",
    "#make sure the colours are a oke\n",
    "colors = np.asarray(new_mesh.vertex_colors)\n",
    "if np.max(colors) > 1.0:  # Assuming colors are in the range [0, 255]\n",
    "        colors /= 255.0\n",
    "\n",
    "#bruh swap the channels bc the colours from gt didn't match like zurias hint -> open: wtf does that mean for the rest\n",
    "colors = np.asarray(new_mesh.vertex_colors)\n",
    "#colors = colors[:, [2, 1, 0]]  # Swap red and blue channels\n",
    "new_mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(width=800, height=600)\n",
    "\n",
    "#alsoiterate through the newly predicted centers\n",
    "color_dict = get_id_colours(data_dir, curr_scan_id)\n",
    "print(\"predicted centers\", all_centers.keys)\n",
    "for obj in all_centers.keys():\n",
    "    if obj in gt_centers:\n",
    "        # vis = o3d.visualization.Visualizer()\n",
    "        # vis.create_window(width=800, height=600)\n",
    "        # vis.add_geometry(new_mesh)\n",
    "        #print(\"votes\",all_centers[obj][\"votes\"])\n",
    "    \n",
    "        pcl = all_centers[obj][\"points\"]\n",
    "        #print(\"number of points\", len(pcl))\n",
    "        color = color_dict[obj]\n",
    "        color = color/255.0\n",
    "        center_colors = np.array([color for _ in range(pcl.shape[0])]) \n",
    "        center_point_cloud = o3d.geometry.PointCloud()\n",
    "        center_point_cloud.points = o3d.utility.Vector3dVector(pcl)\n",
    "        center_point_cloud.colors = o3d.utility.Vector3dVector(center_colors)  # Set the color for center points\n",
    "        voxel_size = 0.075  # Adjust this value based on your needs\n",
    "        downsampled_point_cloud = center_point_cloud.voxel_down_sample(voxel_size=voxel_size)\n",
    "        cl, ind = downsampled_point_cloud.remove_statistical_outlier(nb_neighbors=20, std_ratio=1.75)\n",
    "        pcl = np.asarray(cl.points)\n",
    "\n",
    "        print(\"number of points\", pcl.shape)\n",
    "        vis.add_geometry(cl)\n",
    "\n",
    "        #vis.add_geometry(center_point_cloud)\n",
    "\n",
    "        #center = all_centers[obj][\"center\"]\n",
    "        center= np.median(pcl, axis= 0)\n",
    "        #print(\"center shape\", center.shape)\n",
    "        color = color_dict[obj]\n",
    "        color = color/255.0\n",
    "\n",
    "        sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.1)  # Adjust radius for size\n",
    "        sphere.paint_uniform_color(color)  # Set color for the sphere\n",
    "        sphere.translate(center.T)  # Move sphere to the point location\n",
    "        vis.add_geometry(sphere)  # Add sphere to the visualizer\n",
    "        #vis.run()\n",
    "        #vis.destroy_window()\n",
    "        # Apply voxel downsampling\n",
    "        #downsampled_point_cloud = center_point_cloud.voxel_down_sample(voxel_size=voxel_size)\n",
    "        #vis.add_geometry(downsampled_point_cloud)\n",
    "\n",
    "        #also add the boundingbox\n",
    "        corners = gt_boxes[obj]\n",
    "        lines = [\n",
    "            [0, 1], [1, 2], [2, 3], [3, 0],  # Bottom face\n",
    "            [4, 5], [5, 6], [6, 7], [7, 4],  # Top face\n",
    "            [0, 4], [1, 5], [2, 6], [3, 7]   # Vertical lines\n",
    "        ]\n",
    "        \n",
    "        # Create LineSet for the bounding box\n",
    "        line_set = o3d.geometry.LineSet()\n",
    "        line_set.points = o3d.utility.Vector3dVector(corners)\n",
    "        line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "        line_set.paint_uniform_color([0, 0, 0])  # Color the bounding box red\n",
    "        vis.add_geometry(line_set)\n",
    "\n",
    "        min_coords = np.min(corners, axis=0)\n",
    "        max_coords = np.max(corners, axis=0)\n",
    "\n",
    "        print(\"center\", center)\n",
    "        print(\"min coordinates\", min_coords)\n",
    "        print(\"max coords\", max_coords)\n",
    "        is_inside = (np.all(min_coords <= center) and np.all(center <= max_coords))\n",
    "\n",
    "        # if is_inside:\n",
    "        #     print(\"the object was inside\")\n",
    "        print(\"is inside\" ,is_inside)\n",
    "        # vis.run()\n",
    "        # vis.destroy_window()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Display everything\n",
    "\"\"\"\n",
    "\n",
    "# render_option = vis.get_render_option()\n",
    "# render_option.point_size = 10.0  # Increase this value to enlarge the points\n",
    "\n",
    "#vis.add_geometry(new_mesh)\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_matching_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#access the matched ids\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mread_matching_data\u001b[49m(new_scan_id)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#access the dino segmentation\u001b[39;00m\n\u001b[1;32m      6\u001b[0m segmentation_info_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/ekoller/T7/Segmentation/DinoV2/objects\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_scan_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_matching_data' is not defined"
     ]
    }
   ],
   "source": [
    "#access the matched ids\n",
    "matches = read_matching_data(new_scan_id)\n",
    "\n",
    "\n",
    "#access the dino segmentation\n",
    "segmentation_info_path = osp.join(\"/media/ekoller/T7/Segmentation/DinoV2/objects\", new_scan_id + \".h5\")\n",
    "segmentation_data = read_segmentation_data(segmentation_info_path)\n",
    "\n",
    "frame_boxes = segmentation_data[frame_number][2]\n",
    "print(frame_boxes)\n",
    "mask = frame_boxes[\"mask\"]\n",
    "\n",
    "\n",
    "\n",
    "#access the rgb of the image\n",
    "rgb_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.color.jpg\".format(frame_number))\n",
    "#access the file\n",
    "rgb_file = Image.open(rgb_path)\n",
    "\n",
    "rgb_mat = np.array(rgb_file)\n",
    "print(\"rgb shape\", rgb_mat.shape)\n",
    "\n",
    "\n",
    "#access the deph map of a frame\n",
    "depth_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.depth.pgm\".format(frame_number))\n",
    "#access the file\n",
    "pgm_file = Image.open(depth_path)\n",
    "\n",
    "#since its distances so discrete things take the nearest value not a different interpolation\n",
    "depth_mat_resized = pgm_file.resize((img_width, img_height), Image.NEAREST) \n",
    "\n",
    "#depth is given in mm so put it into m\n",
    "depth_mat = np.array(depth_mat_resized)\n",
    "depth_mat = depth_mat * 0.001\n",
    "\n",
    "\n",
    "#create the 3d projection\n",
    "world_coord, rgb_coord = transform_to_3d(data_dir, scenes_dir, new_scan_id, depth_mat, rgb_mat, frame_number)\n",
    "\n",
    "\n",
    "og_point_cloud = o3d.geometry.PointCloud()\n",
    "og_point_cloud.points = o3d.utility.Vector3dVector(np.array(world_coord))\n",
    "og_point_cloud.colors = o3d.utility.Vector3dVector(np.array(rgb_coord))\n",
    " \n",
    "\n",
    "\"\"\"Do the same thing for the second frame\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#access the segmentation of the scan_id\n",
    "\n",
    "frame_boxes = segmentation_data[frame_number_2][2]\n",
    "print(frame_boxes)\n",
    "mask_2 = frame_boxes[\"mask\"]\n",
    "\n",
    "\n",
    "#access the rgb of the image\n",
    "rgb_path_2 = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.color.jpg\".format(frame_number_2))\n",
    "#access the file\n",
    "rgb_file_2 = Image.open(rgb_path_2)\n",
    "\n",
    "rgb_mat_2 = np.array(rgb_file_2)\n",
    "\n",
    "\n",
    "#access the deph map of a frame\n",
    "depth_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.depth.pgm\".format(frame_number_2))\n",
    "#access the file\n",
    "pgm_file = Image.open(depth_path)\n",
    "\n",
    "#since its distances so discrete things take the nearest value not a different interpolation\n",
    "depth_mat_resized_2 = pgm_file.resize((img_width, img_height), Image.NEAREST) \n",
    "\n",
    "#depth is given in mm so put it into m\n",
    "depth_mat_2 = np.array(depth_mat_resized_2)\n",
    "depth_mat_2 = depth_mat * 0.001\n",
    "\n",
    "\n",
    "\n",
    "world_coord_2, rgb_coord_2 = transform_to_3d(data_dir, scenes_dir, new_scan_id, depth_mat_2, rgb_mat_2, frame_number_2)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" based on the points filter the ones out which correspond to our points, turn this into point clouds\n",
    "\"\"\"\n",
    "mask = np.array(mask)  # Replace with your mask array\n",
    "\n",
    "# Convert mask to an image\n",
    "mask_image = Image.fromarray(mask.astype(np.uint8) * 255)  # Convert mask to 8-bit grayscale\n",
    "\n",
    "mask_image.show()\n",
    "mask = mask.flatten()\n",
    "mask =  mask.astype(bool)\n",
    "print(mask)\n",
    "obj_points = world_coord[mask]\n",
    "print(\"shape obje point\", obj_points.shape)\n",
    "obj_rgb = rgb_coord[mask]\n",
    "#turn into a pointcloud\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "point_cloud.points = o3d.utility.Vector3dVector(np.array(obj_points))\n",
    "point_cloud.colors = o3d.utility.Vector3dVector(np.array(obj_rgb))\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mask_2 = np.array(mask_2)  # Replace with your mask array\n",
    "\n",
    "# Convert mask to an image\n",
    "mask_image_2 = Image.fromarray(mask_2.astype(np.uint8) * 255)  # Convert mask to 8-bit grayscale\n",
    "\n",
    "mask_image_2.show()\n",
    "\n",
    "mask_2 = mask_2.flatten()\n",
    "mask_2 = mask_2.astype(bool)\n",
    "obj_points_2 = world_coord_2[mask_2]\n",
    "print(\"obje points shape\", obj_points.shape)\n",
    "obj_rgb_2 = rgb_coord_2[mask_2]\n",
    "#turn into a pointcloud\n",
    "point_cloud_2 = o3d.geometry.PointCloud()\n",
    "point_cloud_2.points = o3d.utility.Vector3dVector(np.array(obj_points_2))\n",
    "point_cloud_2.colors = o3d.utility.Vector3dVector(np.array(obj_rgb_2))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"look at voxelgridoverlap\n",
    "\"\"\"\n",
    "voxel_size = 0.05 # Adjust voxel size as needed\n",
    "voxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(point_cloud, voxel_size)\n",
    "voxel_grid2 = o3d.geometry.VoxelGrid.create_from_point_cloud(point_cloud_2, voxel_size)\n",
    "\n",
    "similarity = compare_voxel_grids(voxel_grid, voxel_grid2)\n",
    "\n",
    "if similarity:\n",
    "    print(\"The new points likely represent the same object. Similarity is\", similarity)\n",
    "\n",
    "    # Combine the points if they represent the same object\n",
    "    all_pointcloud = o3d.geometry.PointCloud()\n",
    "    all_points = np.vstack((np.asarray(point_cloud.points), np.asarray(point_cloud_2.points)))\n",
    "    all_colors = np.vstack((np.asarray(point_cloud.colors), np.asarray(point_cloud_2.colors)))\n",
    "\n",
    "    # Update the point cloud with the combined points and colors\n",
    "    all_pointcloud.points = o3d.utility.Vector3dVector(all_points)\n",
    "    all_pointcloud.colors = o3d.utility.Vector3dVector(all_colors)\n",
    "\n",
    "    # Visualize both point clouds and their bounding boxes\n",
    "    o3d.visualization.draw_geometries([all_pointcloud])\n",
    "else:\n",
    "    print(\"The new points likely represent a different object. Similarity is \", similarity)\n",
    "\n",
    "    # Visualize both point clouds and their bounding boxes\n",
    "    o3d.visualization.draw_geometries([point_cloud, point_cloud_2, og_point_cloud])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
