{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "/local/home/ekoller/BT\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import os.path as osp\n",
    "import numpy as np \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "from collections import Counter\n",
    "import open3d as o3d\n",
    "import open3d.core as o3c\n",
    "import json\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "import pickle\n",
    "from  plyfile import PlyData\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "#the other imports from the local stuff\n",
    "import sys\n",
    "\n",
    "ws_dir = '/local/home/ekoller/BT'\n",
    "print(ws_dir)\n",
    "sys.path.append(ws_dir)\n",
    "from utils import scan3r,visualisation\n",
    "\n",
    "\n",
    "#reading in the necessary data\n",
    "data_dir ='/local/home/ekoller/R3Scan'\n",
    "scenes_dir = '/local/home/ekoller/R3Scan/scenes'\n",
    "#scan_id= \"38770c95-86d7-27b8-8717-3485b411ddc7\" #is reference scan  since it is a reference scan everything shouls be correctly hit\n",
    "frame_number = \"000015\"\n",
    "frame_number_2 =  \"000016\"\n",
    "img_width = 960\n",
    "img_height = 540\n",
    "\n",
    "curr_scan_id = \"38770c9d-86d7-27b8-869e-4f713b04f290\" #is ref 1d2f8510-d757-207c-8c48-3684433860e1\n",
    "new_scan_id =  \"38770c95-86d7-27b8-8717-3485b411ddc7\" #is rescan 9c27de56-6184-2cda-8196-591957b6387d\n",
    "\n",
    "#the original meshes are given in the file  'labels.instances.annotated.v2.ply'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_patches \n",
      "(540, 960)\n"
     ]
    }
   ],
   "source": [
    "def reconstruct_to_image( patchwise_id):\n",
    "        patch_width = 30\n",
    "        patch_height = 30\n",
    "        \n",
    "        # Initialize an empty image with zeros (assuming same type as original)\n",
    "        reconstructed_img = np.zeros((540, 960), dtype=np.int32)\n",
    "        \n",
    "        # Loop over patches and place the patchwise_id values into the reconstructed image\n",
    "        for i in range(18):\n",
    "            for j in range(32):\n",
    "                # Define the coordinates of the current patch\n",
    "                h_start = i * patch_height\n",
    "                w_start = j * patch_width\n",
    "                h_end = h_start + patch_height\n",
    "                w_end = w_start + patch_width\n",
    "                \n",
    "                # Assign the patchwise_id value to the corresponding patch area\n",
    "                reconstructed_img[h_start:h_end, w_start:w_end] = patchwise_id[i, j]\n",
    "        \n",
    "        return reconstructed_img\n",
    "\n",
    "reference_info_path = osp.join(\"/local/home/ekoller/R3Scan/files/patch_anno\", \"patch_anno_{}_{}\".format(32,18),\"{}.pkl\".format(curr_scan_id))\n",
    "gt_patches = scan3r.load_pkl_data(reference_info_path)\n",
    "\n",
    "gt_patches =gt_patches[frame_number]\n",
    "print(\"gt_patches \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "img = reconstruct_to_image(gt_patches)\n",
    "print(img.shape)\n",
    "\n",
    "img_display = cv2.convertScaleAbs(img, alpha=(255.0/np.max(img)))\n",
    "cv2.imshow(\"Reconstructed Image\", img_display)\n",
    "cv2.waitKey(0)  # Wait indefinitely until a key is pressed\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the object centers based on gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: array([-0.25508112, -0.16950876, -1.3687799 ], dtype=float32), 2.0: array([-1.420566 ,  1.3605125, -0.9313757], dtype=float32), 3.0: array([-0.22190462,  1.1124792 , -1.2763855 ], dtype=float32), 4.0: array([-0.5984486 ,  1.9152938 , -0.06844319], dtype=float32), 5.0: array([-2.5459037 ,  1.1099617 ,  0.11228383], dtype=float32), 6.0: array([-2.6811304 ,  0.24980709, -0.16374134], dtype=float32), 7.0: array([-2.9887486 ,  0.51278657, -0.01879567], dtype=float32), 8.0: array([-0.9748137, -1.3483708, -0.1229235], dtype=float32), 9.0: array([-2.2311766 , -1.007209  , -0.01415187], dtype=float32), 10.0: array([-1.6004734 ,  2.2108457 , -0.09211905], dtype=float32), 11.0: array([ 1.4010975, -1.4889357, -0.4900224], dtype=float32), 12.0: array([ 1.6343089 , -0.66845375,  0.22472727], dtype=float32), 13.0: array([ 1.363485  ,  1.030476  , -0.28760946], dtype=float32), 14.0: array([-0.8460716 ,  0.32025608,  1.04693   ], dtype=float32), 15.0: array([-2.281271  , -0.69923085, -0.86690307], dtype=float32), 16.0: array([-0.5488519, -1.2298291, -0.8905526], dtype=float32), 17.0: array([ 1.4728471 , -0.19783333, -1.0864966 ], dtype=float32), 18.0: array([ 1.2885396 , -1.4701233 , -0.10348786], dtype=float32), 19.0: array([-0.6372443, -1.2980866,  0.2815323], dtype=float32), 20.0: array([ 1.8802993 ,  0.46165413, -0.61710083], dtype=float32), 21.0: array([ 1.2348342 , -0.64484745, -1.2645383 ], dtype=float32), 22.0: array([ 0.8726083,  0.5222907, -0.6456075], dtype=float32), 23.0: array([ 0.26352677,  1.4270854 , -1.0400553 ], dtype=float32), 24.0: array([-2.4149208,  0.5430367, -0.9352922], dtype=float32), 25.0: array([-1.6494056, -0.8038466, -1.1311854], dtype=float32), 26.0: array([-2.673624  , -0.95782155, -0.4636425 ], dtype=float32), 27.0: array([-2.6820397 , -0.28151056, -1.1212704 ], dtype=float32), 28.0: array([-1.6822416, -0.8474369, -0.5630053], dtype=float32), 29.0: array([-0.9749577, -1.1828363, -0.3521589], dtype=float32), 30.0: array([-0.32306027, -1.1628003 , -1.1538026 ], dtype=float32), 31.0: array([ 0.00231561, -1.3672814 , -0.5142565 ], dtype=float32), 32.0: array([-0.08302657, -1.5627589 ,  0.4105882 ], dtype=float32), 33.0: array([ 1.3839915 , -0.01657056, -0.658946  ], dtype=float32), 34.0: array([ 1.3823924 , -0.61045057, -0.55204093], dtype=float32), 35.0: array([ 1.4513354 , -0.31604806, -0.30722967], dtype=float32), 38.0: array([-2.0076342 ,  1.6592131 , -0.69052094], dtype=float32), 39.0: array([-2.2811627 ,  1.4637591 , -0.66142666], dtype=float32), 40.0: array([-0.86958045,  1.1219909 , -0.84974927], dtype=float32), 41.0: array([ 0.991789 ,  1.3192525, -1.2227384], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "#access the data of the object centers\n",
    "curr_scene_path = osp.join(data_dir, \"files\", \"orig\", \"data\", curr_scan_id + \".pkl\")\n",
    "with open(curr_scene_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "obj_ids_pkl = data[\"objects_id\"]\n",
    "\n",
    "\n",
    "#access the things for the mesh\n",
    "pathToMesh = osp.join(data_dir,\"scenes\", curr_scan_id, \"labels.instances.align.annotated.v2.ply\")\n",
    "ply_data = PlyData.read(pathToMesh)\n",
    "vertices = ply_data['vertex'].data\n",
    "vertex_array = np.array([list(vertex) for vertex in vertices])\n",
    "\n",
    "# Extract x, y, z coordinates and objectId\n",
    "x = vertex_array[:, 0]\n",
    "y = vertex_array[:, 1]\n",
    "z = vertex_array[:, 2]\n",
    "object_ids_mesh = vertex_array[:, 6]  # Assuming 'objectId' is the 7th property\n",
    "\n",
    "unique_object_ids = np.unique(object_ids_mesh)\n",
    "\n",
    "bounding_boxes_tmp = {}\n",
    "centroids = {}\n",
    "#go over every id and compute the box\n",
    "for obj_id in unique_object_ids:\n",
    "                #we want the same ids for the boxes\n",
    "                if obj_id in obj_ids_pkl:\n",
    "                        # Filter vertices by object ID\n",
    "                        obj_mask = object_ids_mesh == obj_id\n",
    "                        obj_coords = np.vstack((x[obj_mask], y[obj_mask], z[obj_mask])).T\n",
    "                        \n",
    "                        #also compute the centroid\n",
    "                        centroid = np.mean(obj_coords, axis=0)\n",
    "                        centroids[obj_id] = centroid\n",
    "\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this segment gets for an object id the colour the object is assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a given scene get the colours of the differnt object_ids as a dictionary\n",
    "def get_id_colours(data_dir,scan_id):\n",
    "    #access the mesh file to get the colour of the ids\n",
    "    mesh_file = osp.join(data_dir,\"scenes\", scan_id, \"labels.instances.annotated.v2.ply\")\n",
    "    ply_data = PlyData.read(mesh_file)\n",
    "    # Extract vertex data\n",
    "    vertices = ply_data['vertex']\n",
    "    vertex_count = len(vertices)\n",
    "    \n",
    "    # Initialize dictionary to store object_id -> color mappings\n",
    "    object_colors = {}\n",
    "    \n",
    "   # Iterate through vertices\n",
    "    for i in range(vertex_count):\n",
    "        vertex = vertices[i]\n",
    "        object_id = vertex['objectId']\n",
    "        color = (vertex['red'], vertex['green'], vertex['blue'])\n",
    "        \n",
    "        # Check if object_id already in dictionary, otherwise initialize a Counter\n",
    "        if object_id in object_colors:\n",
    "            object_colors[object_id][color] += 1\n",
    "        else:\n",
    "            object_colors[object_id] = Counter({color: 1})\n",
    "    \n",
    "    # Convert Counter to dictionary with most frequent color\n",
    "    for object_id, color_counter in object_colors.items():\n",
    "        most_common_color = color_counter.most_common(1)[0][0]\n",
    "        object_colors[object_id] = np.array(most_common_color)\n",
    "    \n",
    "    return object_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_in_reference(data_dir, scan_id , pose_rescan):\n",
    "    #same coordinate system\n",
    "    ref_id = scan3r.get_reference_id(data_dir,scan_id)\n",
    "    #if we want the coords in the reference coordinate system return the boxes (based on pkl file)\n",
    "    if scan_id==ref_id:\n",
    "        return pose_rescan\n",
    "    \n",
    "\n",
    "    #transform the centers of rescan to ref coord\n",
    "    path = osp.join(data_dir,\"files\", \"3RScan.json\")\n",
    "    map_id_to_trans = scan3r.read_transform_mat(path)\n",
    "    transform = map_id_to_trans[scan_id]\n",
    "    transform= transform.reshape(4,4)\n",
    "\n",
    "    #transform the pose\n",
    "\n",
    "    return  transform.transpose() * pose_rescan\n",
    "\n",
    "#puts the pose into the reference scan\n",
    "# def pose_in_reference(data_dir,scan_id, pose_rescan):\n",
    "#     #for rescan no need to change that\n",
    "#     if scan3r.is_reference(data_dir,scan_id):\n",
    "#         return pose_rescan\n",
    "\n",
    "#     ref_scan_id = scan3r.get_reference_id(data_dir, scan_id)\n",
    "#     #get the path to the matricies of each scan_id for transformation of rescan to reference\n",
    "#     path = osp.join(data_dir,\"files\", \"3RScan.json\")\n",
    "#     #access the particular rescan_scan_id\n",
    "#     ref2_rescan_all_id = scan3r.read_transform_mat(path)\n",
    "#     ref2rescan = ref2_rescan_all_id[ref_scan_id] \n",
    "#     rescan2ref = np.linalg.inv(ref2rescan)\n",
    "\n",
    "#     #based on rayintersection where we transformed the extrinsic matrix we now just transforme the pose\n",
    "#     pose_reference = pose_rescan * rescan2ref.transpose()\n",
    "\n",
    "#     return pose_reference\n",
    "    \n",
    "\n",
    "# pose_in_reference(data_dir,new_scan_id, frame_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "access the depthmap and based on the pose compute the 3d points of this depthmap we do that for the new scan id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose in ref shape [[ 0.3844095   0.40171845 -0.83117437 -0.93633917]\n",
      " [ 0.03752294 -0.90641139 -0.42072672  0.2091498 ]\n",
      " [-0.92239962  0.13054288 -0.36350624  0.10713241]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "og map <PIL.PpmImagePlugin.PpmImageFile image mode=I size=224x172 at 0x7FE1AC43FDF0>\n",
      "resized map <PIL.Image.Image image mode=I size=960x540 at 0x7FE1AC43F580>\n",
      "final map (540, 960)\n",
      "objects shape (540, 960, 3)\n",
      "computed world coords (3, 518400)\n",
      "computed world coords T (518400, 3)\n",
      "computed colours (518400, 3)\n",
      "Original point cloud size: 518400\n",
      "Downsampled point cloud size: 940\n"
     ]
    }
   ],
   "source": [
    "\"\"\" access the needed files and stuff like that\n",
    "\"\"\"\n",
    "#to do: look at the way the pose in reference is done!!!!!\n",
    "\n",
    "#access the gt projection object ids\n",
    "gt_obj_ids_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.color.jpg\".format(frame_number))\n",
    "#access the file\n",
    "obj_ids = Image.open(gt_obj_ids_path)\n",
    "#convert to np array\n",
    "obj_mat = np.array(obj_ids)\n",
    "\n",
    "#access the extrinsic/pose of the camera\n",
    "pose_rescan = scan3r.load_pose(osp.join(data_dir, \"scenes\"), new_scan_id, frame_number)\n",
    "pose_in_ref = pose_in_reference(data_dir, new_scan_id, pose_rescan)\n",
    "print(\"pose in ref shape\", pose_in_ref)\n",
    "\n",
    "#get the intrinsic of the camera\n",
    "# get img info and camera intrinsics \n",
    "\n",
    "#the intrinsics are saved the following way\n",
    "# intrinsic_mat = np.array([[intrinsic_fx, 0, intrinsic_cx],\n",
    "#                                     [0, intrinsic_fy, intrinsic_cy],\n",
    "#                                     [0, 0, 1]])\n",
    "camera_info = scan3r.load_intrinsics(scenes_dir, new_scan_id)\n",
    "intrinsics = camera_info['intrinsic_mat']\n",
    "img_width = int(camera_info['width'])\n",
    "img_height = int(camera_info['height'])\n",
    "\n",
    "\n",
    "#access the depht image\n",
    "depth_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.depth.pgm\".format(frame_number))\n",
    "#access the file\n",
    "pgm_file = Image.open(depth_path)\n",
    "#convert to np array\n",
    "#depth_mat_og = np.array(pgm_file)\n",
    "print(\"og map\", pgm_file)\n",
    "#since its distances so discrete things take the nearest value not a different interpolation\n",
    "obj_mat_size = (obj_mat.shape[1], obj_mat.shape[0]) \n",
    "depth_mat_resized = pgm_file.resize(obj_mat_size, Image.NEAREST) \n",
    "print(\"resized map\", depth_mat_resized)\n",
    "#depth is given in mm so put it into m\n",
    "depth_mat = np.array(depth_mat_resized)\n",
    "depth_mat = depth_mat * 0.001\n",
    "print(\"final map\", depth_mat.shape)\n",
    "print(\"objects shape\", obj_mat.shape)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "do the computations based on following formula \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#from 2d to camera coordinates xc = (u-cx)*z / fx,   yc = (v-cy)*z/ fy    , zc= z \n",
    "\n",
    "\n",
    "#create a mesh grid since apparently that is how it is done lol\n",
    "u, v = np.meshgrid(np.arange(img_width), np.arange(img_height))\n",
    "\n",
    "#also access the intrinsic values\n",
    "# intrinsic_mat = np.array([[intrinsic_fx, 0, intrinsic_cx],\n",
    "#                                     [0, intrinsic_fy, intrinsic_cy],\n",
    "#                                     [0, 0, 1]])\n",
    "\n",
    "fx = intrinsics[0, 0]  # Focal length in x direction\n",
    "fy = intrinsics[1, 1]  # Focal length in y direction\n",
    "cx = intrinsics[0, 2]  # Principal point x\n",
    "cy = intrinsics[1, 2]  # Principal point y\n",
    "#flatten everything for computations\n",
    "u_flat = u.flatten()\n",
    "v_flat = v.flatten()\n",
    "depth_flat = depth_mat.flatten()\n",
    "\n",
    "#apply the formula from above\n",
    "x_c = (u_flat - cx) * depth_flat / fx\n",
    "y_c = (v_flat - cy) * depth_flat / fy\n",
    "z_c = depth_flat\n",
    "\n",
    "#turn the camera coordinates into homogeneous coordinates\n",
    "camera_coords_homog  = np.vstack((x_c, y_c, z_c, np.ones_like(x_c)))  \n",
    "\n",
    "#apply the extrinsic matrix\n",
    "world_coords_homog = pose_in_ref @ camera_coords_homog\n",
    "#normalize\n",
    "world_coords_homog /= world_coords_homog[3, :]  \n",
    "\n",
    "world_coords = world_coords_homog[:3,:]\n",
    "world_coords_T = world_coords.T\n",
    "print(\"computed world coords\" , world_coords.shape)\n",
    "print(\"computed world coords T\" , world_coords_T.shape)\n",
    "#normalize the colour of the gt\n",
    "rgb_array = np.array(obj_mat) / 255.0\n",
    "#access the colours \n",
    "rgb_colors = rgb_array[v_flat, u_flat]\n",
    "print(\"computed colours\" , rgb_colors.shape)\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "point_cloud.points = o3d.utility.Vector3dVector(np.array(world_coords_T))\n",
    "point_cloud.colors = o3d.utility.Vector3dVector(np.array(rgb_colors))\n",
    "#o3d.visualization.draw_geometries([point_cloud])\n",
    "\n",
    "# Set the voxel size (you can adjust this depending on the desired resolution)\n",
    "voxel_size = 0.08  # Adjust this value based on your needs\n",
    "\n",
    "# Apply voxel downsampling\n",
    "downsampled_point_cloud = point_cloud.voxel_down_sample(voxel_size=voxel_size)\n",
    "\n",
    "# Print the number of points before and after downsampling\n",
    "print(\"Original point cloud size:\", len(point_cloud.points))\n",
    "print(\"Downsampled point cloud size:\", len(downsampled_point_cloud.points))\n",
    "\n",
    "\n",
    "\n",
    "# Load the mesh\n",
    "pathToMesh = osp.join(data_dir, \"scenes\", new_scan_id, \"labels.instances.align.annotated.v2.ply\")\n",
    "new_mesh = o3d.io.read_triangle_mesh(pathToMesh)\n",
    "\n",
    "# Check if the mesh has colors\n",
    "if not new_mesh.has_vertex_colors():\n",
    "    print(\"Mesh does not have vertex colors\")\n",
    "    exit()\n",
    "\n",
    "# Normalize the mesh colors if necessary\n",
    "colors = np.asarray(new_mesh.vertex_colors)\n",
    "if np.max(colors) > 1.0:  # Assuming colors are in the range [0, 255]\n",
    "    colors /= 255.0\n",
    "\n",
    "# Swap color channels if necessary\n",
    "colors = colors[:, [2, 1, 0]]  # Swap red and blue channels\n",
    "new_mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "\n",
    "# Display both the mesh and point cloud together\n",
    "o3d.visualization.draw_geometries([point_cloud, new_mesh], window_name=\"Mesh and Point Cloud Display\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this part now tries to get the depthmaps of the same object in 2 frames and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dino generated mask\n",
    "#returns featuer in the form of features: frame: list of {objext_id, bbox, mask} objects\n",
    "def read_segmentation_data(segmentation_path):\n",
    "    features = {}\n",
    "    with h5py.File(segmentation_path, 'r') as hdf_file:\n",
    "            for frame_idx in hdf_file.keys():\n",
    "                #init boxlist for curr frame\n",
    "                bounding_boxes = []\n",
    "                \n",
    "                # get info \n",
    "                frame_group = hdf_file[frame_idx]\n",
    "                \n",
    "                #iterate over each boundingbox\n",
    "                for bbox_key in frame_group.keys():\n",
    "                    bbox_group = frame_group[bbox_key]\n",
    "                    \n",
    "                    #get te obj id\n",
    "                    object_id = bbox_group.attrs['object_id']\n",
    "                    \n",
    "                    #get the boundingbox\n",
    "                    bbox = bbox_group['bbox'][:]\n",
    "                    \n",
    "                    # get the maskt\n",
    "                    mask = bbox_group['mask'][:]\n",
    "                    \n",
    "                    # append to list\n",
    "                    bounding_boxes.append({\n",
    "                        'object_id': object_id,\n",
    "                        'bbox': bbox,\n",
    "                        'mask': mask\n",
    "                    })\n",
    "                \n",
    "                # stor it to the corresponding frame\n",
    "                \n",
    "                features[frame_idx] = bounding_boxes\n",
    "    return features\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform to reference coordinate system\n",
    "def transform_to_3d(data_dir, scenes_dir, scan_id, depth_map, colour_map, frame_number):\n",
    "    \"\"\" access the needed files and stuff like that\n",
    "    \"\"\"\n",
    "   \n",
    "    #access the extrinsic/pose of the camera\n",
    "    pose_rescan = scan3r.load_pose(osp.join(data_dir, \"scenes\"), scan_id, frame_number)\n",
    "    pose_in_ref = pose_in_reference(data_dir, scan_id, pose_rescan)\n",
    "    \n",
    "    camera_info = scan3r.load_intrinsics(scenes_dir, scan_id)\n",
    "    intrinsics = camera_info['intrinsic_mat']\n",
    "    img_width = int(camera_info['width'])\n",
    "    img_height = int(camera_info['height'])\n",
    "\n",
    "    \"\"\"\n",
    "    do the computations based on following formula \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #from 2d to camera coordinates xc = (u-cx)*z / fx,   yc = (v-cy)*z/ fy    , zc= z \n",
    "\n",
    "\n",
    "    #create a mesh grid since apparently that is how it is done lol\n",
    "    u, v = np.meshgrid(np.arange(img_width), np.arange(img_height))\n",
    "\n",
    "    #also access the intrinsic values\n",
    "    # intrinsic_mat = np.array([[intrinsic_fx, 0, intrinsic_cx],\n",
    "    #                                     [0, intrinsic_fy, intrinsic_cy],\n",
    "    #                                     [0, 0, 1]])\n",
    "\n",
    "    fx = intrinsics[0, 0]  # Focal length in x direction\n",
    "    fy = intrinsics[1, 1]  # Focal length in y direction\n",
    "    cx = intrinsics[0, 2]  # Principal point x\n",
    "    cy = intrinsics[1, 2]  # Principal point y\n",
    "    #flatten everything for computations\n",
    "    u_flat = u.flatten()\n",
    "    v_flat = v.flatten()\n",
    "    depth_flat = depth_map.flatten()\n",
    "\n",
    "    #apply the formula from above\n",
    "    x_c = (u_flat - cx) * depth_flat / fx\n",
    "    y_c = (v_flat - cy) * depth_flat / fy\n",
    "    z_c = depth_flat\n",
    "\n",
    "    #turn the camera coordinates into homogeneous coordinates\n",
    "    camera_coords_homog  = np.vstack((x_c, y_c, z_c, np.ones_like(x_c)))  \n",
    "\n",
    "    #apply the extrinsic matrix\n",
    "    world_coords_homog = pose_in_ref @ camera_coords_homog\n",
    "    #normalize\n",
    "    world_coords_homog /= world_coords_homog[3, :]  \n",
    "\n",
    "    world_coords = world_coords_homog[:3,:]\n",
    "    world_coords_T = world_coords.T\n",
    "    print(\"computed world coords\" , world_coords.shape)\n",
    "    print(\"computed world coords T\" , world_coords_T.shape)\n",
    "    #normalize the colour of the gt\n",
    "    rgb_array = np.array(colour_map) / 255.0\n",
    "    #access the colours \n",
    "    rgb_colors = rgb_array[v_flat, u_flat]\n",
    "   \n",
    "\n",
    "    return world_coords_T, rgb_colors\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxel_grid_to_coordinates(voxel_grid):\n",
    "    \"\"\"Extract voxel coordinates from a VoxelGrid object.\"\"\"\n",
    "    voxels = voxel_grid.get_voxels()\n",
    "    voxel_coords = np.array([voxel.grid_index for voxel in voxels])\n",
    "    return voxel_coords\n",
    "\n",
    "\n",
    "def compare_voxel_grids(voxel_grid1, voxel_grid2):\n",
    "    \"\"\"Compare two voxel grids to see how much they overlap.\"\"\"\n",
    "    coords1 = voxel_grid_to_coordinates(voxel_grid1)\n",
    "    coords2 = voxel_grid_to_coordinates(voxel_grid2)\n",
    "    \n",
    "    # Convert to sets of tuples for intersection\n",
    "    voxels1_set = set(map(tuple, coords1))\n",
    "    voxels2_set = set(map(tuple, coords2))\n",
    "    \n",
    "    # Compute intersection\n",
    "    intersection = voxels1_set.intersection(voxels2_set)\n",
    "    union = voxels1_set.union(voxels2_set)\n",
    "    \n",
    "    similarity = len(intersection) / len(union) if len(union) > 0 else 0\n",
    "    return similarity\n",
    "\n",
    "# def create_bounding_box_lines(bbox):\n",
    "#     # Create a LineSet for the bounding box\n",
    "#     lines = [\n",
    "#         [0, 1], [1, 2], [2, 3], [3, 0], # bottom face\n",
    "#         [4, 5], [5, 6], [6, 7], [7, 4], # top face\n",
    "#         [0, 4], [1, 5], [2, 6], [3, 7]  # vertical lines\n",
    "#     ]\n",
    "#     # Convert the bounding box min and max bounds to corner points\n",
    "#     min_bound = bbox.get_min_bound()\n",
    "#     max_bound = bbox.get_max_bound()\n",
    "\n",
    "#     corners = [\n",
    "#         [min_bound[0], min_bound[1], min_bound[2]],\n",
    "#         [max_bound[0], min_bound[1], min_bound[2]],\n",
    "#         [max_bound[0], max_bound[1], min_bound[2]],\n",
    "#         [min_bound[0], max_bound[1], min_bound[2]],\n",
    "#         [min_bound[0], min_bound[1], max_bound[2]],\n",
    "#         [max_bound[0], min_bound[1], max_bound[2]],\n",
    "#         [max_bound[0], max_bound[1], max_bound[2]],\n",
    "#         [min_bound[0], max_bound[1], max_bound[2]]\n",
    "#     ]\n",
    "\n",
    "#     lines_set = o3d.geometry.LineSet()\n",
    "#     lines_set.points = o3d.utility.Vector3dVector(corners)\n",
    "#     lines_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "#     return \n",
    "\n",
    "\n",
    "# def check_mesh_intersection(mesh1, mesh2):\n",
    "#     # Check if two meshes intersect\n",
    "#     mesh1.compute_triangle_normals()\n",
    "#     mesh2.compute_triangle_normals()\n",
    "#     mesh1_tree = o3d.geometry.KDTreeFlann(mesh1)\n",
    "#     mesh2_tree = o3d.geometry.KDTreeFlann(mesh2)\n",
    "    \n",
    "#     # Check for intersection by sampling points from the first mesh and searching for them in the second mesh\n",
    "#     for point in mesh1.vertices:\n",
    "#         [_, idx, _] = mesh2_tree.search_knn_vector_3d(point, 1)\n",
    "#         if len(idx) > 0:\n",
    "#             return True\n",
    "#     return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object_id': 11, 'bbox': array([450, 270,  30,  30], dtype=int32), 'mask': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}\n",
      "rgb shape (540, 960, 3)\n",
      "computed world coords (3, 518400)\n",
      "computed world coords T (518400, 3)\n",
      "{'object_id': 11, 'bbox': array([360, 240,  30,  30], dtype=int32), 'mask': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}\n",
      "computed world coords (3, 518400)\n",
      "computed world coords T (518400, 3)\n",
      "[False False False ... False False False]\n",
      "shape obje point (900, 3)\n",
      "obje points shape (900, 3)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#turn into a pointcloud\u001b[39;00m\n\u001b[1;32m    116\u001b[0m point_cloud_2 \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mPointCloud()\n\u001b[0;32m--> 117\u001b[0m point_cloud_2\u001b[38;5;241m.\u001b[39mpoints \u001b[38;5;241m=\u001b[39m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutility\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVector3dVector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m point_cloud_2\u001b[38;5;241m.\u001b[39mcolors \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mutility\u001b[38;5;241m.\u001b[39mVector3dVector(np\u001b[38;5;241m.\u001b[39marray([]))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"look at voxelgridoverlap\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#access the dino segmentation\n",
    "     #access the segmentation of the scan_id\n",
    "segmentation_info_path = osp.join(\"/media/ekoller/T7/Segmentation/DinoV2/objects\", new_scan_id + \".h5\")\n",
    "segmentation_data = read_segmentation_data(segmentation_info_path)\n",
    "\n",
    "frame_boxes = segmentation_data[frame_number][2]\n",
    "print(frame_boxes)\n",
    "mask = frame_boxes[\"mask\"]\n",
    "\n",
    "\n",
    "\n",
    "#access the rgb of the image\n",
    "rgb_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.color.jpg\".format(frame_number))\n",
    "#access the file\n",
    "rgb_file = Image.open(rgb_path)\n",
    "\n",
    "rgb_mat = np.array(rgb_file)\n",
    "print(\"rgb shape\", rgb_mat.shape)\n",
    "\n",
    "\n",
    "#access the deph map of a frame\n",
    "depth_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.depth.pgm\".format(frame_number))\n",
    "#access the file\n",
    "pgm_file = Image.open(depth_path)\n",
    "\n",
    "#since its distances so discrete things take the nearest value not a different interpolation\n",
    "depth_mat_resized = pgm_file.resize((img_width, img_height), Image.NEAREST) \n",
    "\n",
    "#depth is given in mm so put it into m\n",
    "depth_mat = np.array(depth_mat_resized)\n",
    "depth_mat = depth_mat * 0.001\n",
    "\n",
    "\n",
    "#create the 3d projection\n",
    "world_coord, rgb_coord = transform_to_3d(data_dir, scenes_dir, new_scan_id, depth_mat, rgb_mat, frame_number)\n",
    "\n",
    "\n",
    "og_point_cloud = o3d.geometry.PointCloud()\n",
    "og_point_cloud.points = o3d.utility.Vector3dVector(np.array(world_coord))\n",
    "og_point_cloud.colors = o3d.utility.Vector3dVector(np.array(rgb_coord))\n",
    " \n",
    "\n",
    "\"\"\"Do the same thing for the second frame\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#access the segmentation of the scan_id\n",
    "\n",
    "frame_boxes = segmentation_data[frame_number_2][2]\n",
    "print(frame_boxes)\n",
    "mask_2 = frame_boxes[\"mask\"]\n",
    "\n",
    "\n",
    "#access the rgb of the image\n",
    "rgb_path_2 = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.color.jpg\".format(frame_number_2))\n",
    "#access the file\n",
    "rgb_file_2 = Image.open(rgb_path_2)\n",
    "\n",
    "rgb_mat_2 = np.array(rgb_file_2)\n",
    "\n",
    "\n",
    "#access the deph map of a frame\n",
    "depth_path = osp.join(scenes_dir, new_scan_id, \"sequence\", \"frame-{}.depth.pgm\".format(frame_number_2))\n",
    "#access the file\n",
    "pgm_file = Image.open(depth_path)\n",
    "\n",
    "#since its distances so discrete things take the nearest value not a different interpolation\n",
    "depth_mat_resized_2 = pgm_file.resize((img_width, img_height), Image.NEAREST) \n",
    "\n",
    "#depth is given in mm so put it into m\n",
    "depth_mat_2 = np.array(depth_mat_resized_2)\n",
    "depth_mat_2 = depth_mat * 0.001\n",
    "\n",
    "\n",
    "\n",
    "world_coord_2, rgb_coord_2 = transform_to_3d(data_dir, scenes_dir, new_scan_id, depth_mat_2, rgb_mat_2, frame_number_2)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" based on the points filter the ones out which correspond to our points, turn this into point clouds\n",
    "\"\"\"\n",
    "mask = np.array(mask)  # Replace with your mask array\n",
    "\n",
    "# Convert mask to an image\n",
    "mask_image = Image.fromarray(mask.astype(np.uint8) * 255)  # Convert mask to 8-bit grayscale\n",
    "\n",
    "mask_image.show()\n",
    "mask = mask.flatten()\n",
    "mask =  mask.astype(bool)\n",
    "print(mask)\n",
    "obj_points = world_coord[mask]\n",
    "print(\"shape obje point\", obj_points.shape)\n",
    "obj_rgb = rgb_coord[mask]\n",
    "#turn into a pointcloud\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "point_cloud.points = o3d.utility.Vector3dVector(np.array(obj_points))\n",
    "point_cloud.colors = o3d.utility.Vector3dVector(np.array(obj_rgb))\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mask_2 = np.array(mask_2)  # Replace with your mask array\n",
    "\n",
    "# Convert mask to an image\n",
    "mask_image_2 = Image.fromarray(mask_2.astype(np.uint8) * 255)  # Convert mask to 8-bit grayscale\n",
    "\n",
    "mask_image_2.show()\n",
    "\n",
    "mask_2 = mask_2.flatten()\n",
    "mask_2 = mask_2.astype(bool)\n",
    "obj_points_2 = world_coord_2[mask_2]\n",
    "print(\"obje points shape\", obj_points.shape)\n",
    "obj_rgb_2 = rgb_coord_2[mask_2]\n",
    "#turn into a pointcloud\n",
    "point_cloud_2 = o3d.geometry.PointCloud()\n",
    "point_cloud_2.points = o3d.utility.Vector3dVector(np.array(obj_points_2))\n",
    "point_cloud_2.colors = o3d.utility.Vector3dVector(np.array(obj_rgb_2))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"look at voxelgridoverlap\n",
    "\"\"\"\n",
    "voxel_size = 0.05 # Adjust voxel size as needed\n",
    "voxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(point_cloud, voxel_size)\n",
    "voxel_grid2 = o3d.geometry.VoxelGrid.create_from_point_cloud(point_cloud_2, voxel_size)\n",
    "\n",
    "similarity = compare_voxel_grids(voxel_grid, voxel_grid2)\n",
    "\n",
    "if similarity:\n",
    "    print(\"The new points likely represent the same object. Similarity is\", similarity)\n",
    "\n",
    "    # Combine the points if they represent the same object\n",
    "    all_pointcloud = o3d.geometry.PointCloud()\n",
    "    all_points = np.vstack((np.asarray(point_cloud.points), np.asarray(point_cloud_2.points)))\n",
    "    all_colors = np.vstack((np.asarray(point_cloud.colors), np.asarray(point_cloud_2.colors)))\n",
    "\n",
    "    # Update the point cloud with the combined points and colors\n",
    "    all_pointcloud.points = o3d.utility.Vector3dVector(all_points)\n",
    "    all_pointcloud.colors = o3d.utility.Vector3dVector(all_colors)\n",
    "\n",
    "    # Visualize both point clouds and their bounding boxes\n",
    "    o3d.visualization.draw_geometries([all_pointcloud])\n",
    "else:\n",
    "    print(\"The new points likely represent a different object. Similarity is \", similarity)\n",
    "\n",
    "    # Visualize both point clouds and their bounding boxes\n",
    "    o3d.visualization.draw_geometries([point_cloud, point_cloud_2, og_point_cloud])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
