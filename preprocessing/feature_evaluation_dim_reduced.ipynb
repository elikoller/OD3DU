{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "/local/home/ekoller/BT\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import os \n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import plyfile\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import os.path as osp\n",
    "import sys\n",
    "ws_dir = '/local/home/ekoller/BT'\n",
    "print(ws_dir)\n",
    "sys.path.append(ws_dir)\n",
    "from utils import evaluation, scan3r\n",
    "\n",
    "\n",
    "data_dir ='/local/home/ekoller/R3Scan'\n",
    "scenes_dir = '/local/home/ekoller/R3Scan/scenes'\n",
    "#scan_id= \"38770c95-86d7-27b8-8717-3485b411ddc7\" #is reference scan  since it is a reference scan everything shouls be correctly hit\n",
    "curr_scan_id =  \"02b33e01-be2b-2d54-93fb-4145a709cec5\"#\"02b33e01-be2b-2d54-93fb-4145a709cec5\" \n",
    "new_scan_id =  \"fcf66d88-622d-291c-871f-699b2d063630\" #\"fcf66d8a-622d-291c-8429-0e1109c6bb26\"\n",
    "frame_number = \"000007\"\n",
    "curr_frame_number = \"000007\"\n",
    "new_frame_number = \"000008\"\n",
    "patch_h= 18\n",
    "image_height = 540\n",
    "image_width = 960\n",
    "patch_w = 32\n",
    "patch_height = 30\n",
    "patch_width = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(mask, new_ids, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Display the mask\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title('Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display the new_ids array\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(new_ids, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#visualitation of the following: with the new ids fill in an image with the new ids\n",
    "#compute the resulting image when comparing stuff\n",
    "\n",
    "def generate_pairs_pixel_level( dino_data,id_matches):\n",
    "    \n",
    "    new_ids = np.zeros((image_height, image_width))\n",
    "\n",
    "\n",
    "    #go over every mask and fill in the id into the new_ids which it got mapped to\n",
    "    for seg_region in dino_data:\n",
    "        mask_id = seg_region[\"object_id\"]\n",
    "        #print(\"mask id \", mask_id)\n",
    "        #get to what the region mapped in the embeddings\n",
    "        matched_id = id_matches[mask_id]\n",
    "        #print(\"matched id \", matched_id)\n",
    "        mask = seg_region[\"mask\"]\n",
    "        boolean_mask = mask == 225\n",
    "        new_ids[boolean_mask] = matched_id[0] #[0] is the id the second one is the error\n",
    "        #visualize(boolean_mask, new_ids, f'Updated new_ids with mask id {mask_id}')\n",
    "\n",
    "    #returns the new ids on a pixel wise level\n",
    "    return new_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this codesegment takes an image on a pixel wise level and quantizes it such that every patch has only the id of the most often occuring id\n",
    "def quantize_to_patch_level(pixelwise_img):\n",
    "    #get the shape of the pixelwise img\n",
    "    input_h, input_w = pixelwise_img.shape\n",
    "    patch_width = int(input_w/patch_w)\n",
    "    patch_height= int(input_h/patch_h)\n",
    "\n",
    "    patchwise_w = patch_w #number of patches\n",
    "    patchwise_h = patch_h\n",
    "\n",
    "    patchwise_id = np.zeros((patchwise_h,patchwise_w))\n",
    "\n",
    "    for i in range(patchwise_h):\n",
    "            for j in range(patchwise_w):\n",
    "                # Define the coordinates of the current patch\n",
    "                h_start = i * patch_height\n",
    "                w_start = j * patch_width\n",
    "                h_end = h_start + patch_height\n",
    "                w_end = w_start + patch_width\n",
    "                \n",
    "                # Get the current patch from the input matrix\n",
    "                patch = pixelwise_img[h_start:h_end, w_start:w_end]\n",
    "                \n",
    "                # get the most reoccuring id of the patch\n",
    "                flattened_patch = patch.flatten()\n",
    "                # Find the most common value in the patch\n",
    "                value_counts = Counter(flattened_patch)\n",
    "                most_common_id = value_counts.most_common(1)[0][0]\n",
    "                \n",
    "                # Assign the most common ID to the new matrix\n",
    "                patchwise_id[i, j] = most_common_id\n",
    "\n",
    "\n",
    "    return patchwise_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur scan id colors dict_keys([4, 18, 2, 0, 3, 1, 100, 16, 8, 9, 24, 25, 29, 6, 26, 10, 11, 23, 27, 22, 5, 20, 12, 15, 13, 14, 103, 21])\n",
      "new scan id colors dict_keys([4, 9, 24, 1, 8, 22, 23, 12, 10, 0, 27])\n"
     ]
    }
   ],
   "source": [
    "#for a given scene get the colours of the differnt object_ids\n",
    "def get_id_colours(data_dir,scan_id):\n",
    "    #access the mesh file to get the colour of the ids\n",
    "    mesh_file = osp.join(data_dir,\"scenes\", scan_id, \"labels.instances.annotated.v2.ply\")\n",
    "    ply_data = plyfile.PlyData.read(mesh_file)\n",
    "    # Extract vertex data\n",
    "    vertices = ply_data['vertex']\n",
    "    vertex_count = len(vertices)\n",
    "    \n",
    "    # Initialize dictionary to store object_id -> color mappings\n",
    "    object_colors = {}\n",
    "    \n",
    "   # Iterate through vertices\n",
    "    for i in range(vertex_count):\n",
    "        vertex = vertices[i]\n",
    "        object_id = vertex['objectId']\n",
    "        color = (vertex['red'], vertex['green'], vertex['blue'])\n",
    "        \n",
    "        # Check if object_id already in dictionary, otherwise initialize a Counter\n",
    "        if object_id in object_colors:\n",
    "            object_colors[object_id][color] += 1\n",
    "        else:\n",
    "            object_colors[object_id] = Counter({color: 1})\n",
    "    \n",
    "    # Convert Counter to dictionary with most frequent color\n",
    "    for object_id, color_counter in object_colors.items():\n",
    "        most_common_color = color_counter.most_common(1)[0][0]\n",
    "        object_colors[object_id] = np.array(most_common_color[::-1])\n",
    "    \n",
    "    return object_colors\n",
    "\n",
    "colors = get_id_colours(data_dir,curr_scan_id)\n",
    "print(\"cur scan id colors\", colors.keys())\n",
    "colors = get_id_colours(data_dir,new_scan_id)\n",
    "print(\"new scan id colors\", colors.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function creates a color image of the size 960x540 from the patches\n",
    "def create_color_img_from_obj_id(data_dir,scan_id,obj_id_mat):\n",
    "    #access the mesh file to get the colour of the ids\n",
    "    colour_dict = get_id_colours(data_dir, scan_id)\n",
    "    #initialize the new matrix \n",
    "    og_height, og_width = obj_id_mat.shape\n",
    "    new_height = og_height * patch_height\n",
    "    new_width = og_width * patch_width\n",
    "\n",
    "    colour_mat = np.zeros((new_height,new_width,3))\n",
    "\n",
    "    #go over each element and assign the colour of the dictionary\n",
    "    for h in range(og_height):\n",
    "        for w in range(og_width):\n",
    "            if obj_id_mat[h][w] in colour_dict.keys():\n",
    "                colour = colour_dict[obj_id_mat[h][w]]\n",
    "                colour_mat[h*patch_height:(h+1)*patch_height, w*patch_width:(w+1)*patch_width] = colour\n",
    "\n",
    "\n",
    "    return colour_mat\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a gt patchwise matrix and a newly calculated matrix compute the accuracy\n",
    "#aaparently this is not the same as accuracy\n",
    "#given a gt patchwise matrix and a newly calculated matrix compute the accuracy\n",
    "def get_accuracy(gt_patches, computed_patches):\n",
    "    #make sure we dont do something dumm lol\n",
    "    assert gt_patches.shape == computed_patches.shape, \"Matrices must have the same shape\"\n",
    "\n",
    "    # Flatten matrices to iterate over each element\n",
    "    flat_gt = gt_patches.flatten()\n",
    "    flat_comp = computed_patches.flatten()\n",
    "\n",
    "    percentage = np.zeros_like(flat_gt)\n",
    "    \n",
    "    total_instances = len(flat_gt)\n",
    "    correct_instances = 0\n",
    "\n",
    "    #compute based on the other way\n",
    "    for idx in range(len(percentage)):\n",
    "        if flat_gt[idx] == flat_comp[idx]:\n",
    "            percentage[idx] = 1\n",
    "    \n",
    "    # Count correct instances where IDs match\n",
    "    for id_gt, id_comp in zip(flat_gt, flat_comp):\n",
    "        if id_gt == id_comp:\n",
    "            correct_instances += 1\n",
    "            \n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = correct_instances / total_instances\n",
    "    \n",
    "    return accuracy, np.mean(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def avg_pooling(all_features_curr_scene):\n",
    "    #get every fearure vector and average the patches to one patch\n",
    "    all_features = []\n",
    "    all_colors = []\n",
    "    all_ids = []\n",
    "    for frame in all_features_curr_scene:\n",
    "        for obj_id in all_features_curr_scene[frame]:\n",
    "            if obj_id != 0:\n",
    "                matrix = all_features_curr_scene[frame][obj_id]\n",
    "                #averaging\n",
    "                aggregated_feature = np.mean(matrix, axis=0)  \n",
    "                all_features.append(aggregated_feature)\n",
    "                all_ids.append(obj_id)\n",
    "                #all_colors.append(tuple(c / 255.0 for c in colors[obj_id]))\n",
    "\n",
    "    return all_features, all_ids,all_colors\n",
    "\n",
    "def max_pooling(all_features_curr_scene):\n",
    "    #perform max pooling \n",
    "    all_features = []\n",
    "    all_colors = []\n",
    "    all_ids = []\n",
    "    for frame in all_features_curr_scene:\n",
    "        for obj_id in all_features_curr_scene[frame]:\n",
    "            if obj_id != 0:\n",
    "                matrix = all_features_curr_scene[frame][obj_id]\n",
    "\n",
    "                #max pooling\n",
    "                aggregated_feature = np.max(matrix, axis=0)  # Max pooling across patches\n",
    "                all_features.append(aggregated_feature)\n",
    "                all_ids.append(obj_id)\n",
    "                # append color corresponding to the object id\n",
    "                #all_colors.append(tuple(c / 255.0 for c in colors[obj_id]))\n",
    "\n",
    "    return all_features, all_ids, all_colors\n",
    "\n",
    "\n",
    "def display_images(gt_image, new_image, title1, title2):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(gt_image)\n",
    "    plt.title(title1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(new_image)\n",
    "    plt.title(title2)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given an input image which was segmented using dino, use PCA & either average or maxpooling to compute the accuracy of the predicted object id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcf66d88-622d-291c-871f-699b2d063630\n",
      "id mateches {1: (23, array([0.01570257], dtype=float32)), 2: (12, array([0.01312101], dtype=float32)), 3: (1, array([0.02636981], dtype=float32)), 4: (5, array([0.00739935], dtype=float32)), 5: (9, array([0.01593802], dtype=float32)), 6: (8, array([0.01633052], dtype=float32)), 7: (18, array([0.02055096], dtype=float32)), 8: (9, array([0.00719532], dtype=float32)), 9: (11, array([0.01523954], dtype=float32)), 10: (10, array([0.00754297], dtype=float32)), 11: (5, array([0.01760643], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  5.  8.  9. 10. 11. 12. 18. 23.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.3663194444444444, 0.3663194444444444)\n",
      "id mateches {1: (23, array([0.02890891], dtype=float32)), 2: (27, array([0.01526292], dtype=float32)), 3: (27, array([0.00507543], dtype=float32)), 4: (22, array([0.0243516], dtype=float32)), 5: (8, array([0.01412289], dtype=float32)), 6: (1, array([0.02946538], dtype=float32)), 7: (4, array([0.00834347], dtype=float32)), 8: (20, array([0.00537192], dtype=float32)), 9: (8, array([0.02702368], dtype=float32)), 10: (2, array([0.00913131], dtype=float32)), 11: (1, array([0.00797873], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  2.  4.  8. 20. 22. 23. 27.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5416666666666666, 0.5416666666666666)\n",
      "id mateches {1: (26, array([0.01202489], dtype=float32)), 2: (21, array([0.02526512], dtype=float32)), 3: (24, array([0.00897585], dtype=float32)), 4: (22, array([0.03573767], dtype=float32)), 5: (18, array([0.01583409], dtype=float32)), 6: (1, array([0.02371536], dtype=float32)), 7: (3, array([0.01118859], dtype=float32)), 8: (20, array([0.01512669], dtype=float32)), 9: (20, array([0.00880561], dtype=float32)), 10: (11, array([0.00763051], dtype=float32)), 11: (11, array([0.0095577], dtype=float32)), 12: (8, array([0.0224216], dtype=float32)), 13: (1, array([0.01104942], dtype=float32)), 14: (3, array([0.01908849], dtype=float32)), 15: (103, array([0.00825424], dtype=float32))}\n",
      "unique numbers in new img patchwise  [  1.   3.   8.  11.  18.  20.  21.  22.  24.  26. 103.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.515625, 0.515625)\n",
      "id mateches {1: (26, array([0.01067165], dtype=float32)), 2: (22, array([0.02520757], dtype=float32)), 3: (1, array([0.01333263], dtype=float32)), 4: (1, array([0.02454256], dtype=float32)), 5: (6, array([0.01337301], dtype=float32)), 6: (8, array([0.0214241], dtype=float32)), 7: (5, array([0.01204956], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  5.  6.  8. 22. 26.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5607638888888888, 0.5607638888888888)\n",
      "id mateches {1: (26, array([0.01186473], dtype=float32)), 2: (22, array([0.00481418], dtype=float32)), 3: (26, array([0.01362096], dtype=float32)), 4: (1, array([0.02368042], dtype=float32)), 5: (8, array([0.01454909], dtype=float32)), 6: (6, array([0.01989031], dtype=float32)), 7: (2, array([0.01205758], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  2.  6.  8. 22. 26.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5486111111111112, 0.5486111111111112)\n",
      "id mateches {1: (26, array([0.01401939], dtype=float32)), 2: (12, array([0.01131096], dtype=float32)), 3: (1, array([0.01167667], dtype=float32)), 4: (100, array([0.01327131], dtype=float32))}\n",
      "unique numbers in new img patchwise  [  1.  12.  26. 100.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6006944444444444, 0.6006944444444444)\n",
      "id mateches {1: (26, array([0.01710843], dtype=float32)), 2: (1, array([0.0060561], dtype=float32)), 3: (20, array([0.01725049], dtype=float32)), 4: (1, array([0.00771001], dtype=float32)), 5: (1, array([0.02092633], dtype=float32)), 6: (5, array([0.01264273], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  5. 20. 26.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6006944444444444, 0.6006944444444444)\n",
      "id mateches {1: (22, array([0.02174875], dtype=float32)), 2: (11, array([0.01035352], dtype=float32)), 3: (8, array([0.00425589], dtype=float32)), 4: (1, array([0.01350946], dtype=float32)), 5: (20, array([0.01491873], dtype=float32)), 6: (12, array([0.01384708], dtype=float32)), 7: (11, array([0.00765689], dtype=float32)), 8: (20, array([0.00821186], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  8. 11. 12. 20. 22.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6631944444444444, 0.6631944444444444)\n",
      "id mateches {1: (10, array([0.01616625], dtype=float32)), 2: (26, array([0.00554907], dtype=float32)), 3: (100, array([0.01794502], dtype=float32)), 4: (8, array([0.01908244], dtype=float32)), 5: (8, array([0.00891102], dtype=float32)), 6: (1, array([0.02480602], dtype=float32)), 7: (2, array([0.01240853], dtype=float32)), 8: (12, array([0.02260482], dtype=float32)), 9: (11, array([0.01065981], dtype=float32)), 10: (8, array([0.01770717], dtype=float32)), 11: (20, array([0.00506238], dtype=float32))}\n",
      "unique numbers in new img patchwise  [  1.   2.   8.  10.  11.  12.  20.  26. 100.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5763888888888888, 0.5763888888888888)\n",
      "id mateches {1: (12, array([0.01616373], dtype=float32)), 2: (18, array([0.01358317], dtype=float32)), 3: (8, array([0.01839462], dtype=float32)), 4: (8, array([0.01074163], dtype=float32)), 5: (10, array([0.02290149], dtype=float32)), 6: (27, array([0.01287582], dtype=float32)), 7: (1, array([0.00711869], dtype=float32)), 8: (9, array([0.01533436], dtype=float32)), 9: (8, array([0.01702213], dtype=float32)), 10: (10, array([0.01594253], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  8.  9. 10. 12. 18. 27.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.7517361111111112, 0.7517361111111112)\n",
      "id mateches {1: (8, array([0.0134435], dtype=float32)), 2: (11, array([0.01988258], dtype=float32)), 3: (29, array([0.01037943], dtype=float32)), 4: (1, array([0.00971572], dtype=float32)), 5: (8, array([0.02099512], dtype=float32)), 6: (15, array([0.00808532], dtype=float32)), 7: (10, array([0.01273016], dtype=float32)), 8: (20, array([0.0076775], dtype=float32)), 9: (6, array([0.01147836], dtype=float32)), 10: (29, array([0.00824305], dtype=float32)), 11: (1, array([0.00750424], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  6.  8. 10. 11. 15. 20. 29.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6076388888888888, 0.6076388888888888)\n",
      "id mateches {1: (1, array([0.01387142], dtype=float32)), 2: (8, array([0.00740642], dtype=float32))}\n",
      "unique numbers in new img patchwise  [1. 8.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6145833333333334, 0.6145833333333334)\n",
      "id mateches {1: (5, array([0.00914586], dtype=float32)), 2: (8, array([0.01399323], dtype=float32)), 3: (1, array([0.01699748], dtype=float32)), 4: (11, array([0.01740048], dtype=float32)), 5: (1, array([0.02151862], dtype=float32)), 6: (10, array([0.01093907], dtype=float32)), 7: (20, array([0.0112661], dtype=float32)), 8: (11, array([0.00889755], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  5.  8. 10. 11. 20.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6388888888888888, 0.6388888888888888)\n",
      "mean of total accuracy  0.5836004273504274\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "for the current scene: access all the precomputed dinov2 features and do pca of the space\n",
    "\"\"\"\n",
    "print(new_scan_id)\n",
    "#get all the features of the current scene\n",
    "all_info_path = osp.join(data_dir,\"files/Features2D/projection/DinoV2/patch_32_18\", curr_scan_id + \".pkl\")\n",
    "with open(all_info_path, 'rb') as file:\n",
    "    all_features_curr_scene = pickle.load(file)\n",
    "\n",
    "#using avg pooling\n",
    "all_features, all_ids, all_colors = avg_pooling(all_features_curr_scene)\n",
    "all_features = np.array(all_features)\n",
    "all_ids = np.array(all_ids)\n",
    "all_colors = np.array(all_colors)\n",
    "\n",
    "#perform pca to 3D reduction\n",
    "pca = PCA(n_components=3)\n",
    "reduced_points = pca.fit_transform(all_features)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "access the features of the new scene and the info corresponding to the patches\n",
    "\"\"\"\n",
    "\n",
    "#get the Dinov2 features for the input image ( the segmented one)\n",
    "input_featues_path = osp.join(data_dir,\"files/Features2D/dino_segmentation/DinoV2/patch_32_18\", new_scan_id + \".pkl\")\n",
    "with open(input_featues_path, 'rb') as file:\n",
    "    input_features = pickle.load(file)\n",
    "\n",
    "#get the data information on the segmentation of the input image\n",
    "input_info_path = osp.join(data_dir,\"files/Segmentation/DinoV2/objects\", new_scan_id + \".pkl\")\n",
    "with open(input_info_path, 'rb') as file:\n",
    "    input_info = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "total_accuracy= []\n",
    "\n",
    "\"\"\"\n",
    "decide the id matches based on the projected points and 5 neighbouhood vote\n",
    "\"\"\"\n",
    "\n",
    "#iterate over each frame of the newscan and perform the assignment \n",
    "for frame in input_features:\n",
    "    #initialze the matches\n",
    "    id_matches= {}\n",
    "\n",
    "    input_dict = input_features[frame]\n",
    "\n",
    "    #iterate through every patch of the segmented new scene image, project it into the pca space ang get the closest\n",
    "    for img_id, img_vec in input_dict.items():\n",
    "        #reshape and project into the space\n",
    "\n",
    "        #use avg pooling\n",
    "        aggregated_vec = np.mean(img_vec, axis=0)\n",
    "        #print(\"new vec shape\", aggregated_vec.shape)\n",
    "        reduced_unseen_point = pca.transform(aggregated_vec.reshape(1,-1))\n",
    "\n",
    "        # cosine distances to all reduced points\n",
    "        #distances = cosine_distances(reduced_unseen_point, reduced_points)\n",
    "\n",
    "        #eucledian dist\n",
    "        distances = euclidean_distances(reduced_unseen_point, reduced_points)\n",
    "\n",
    "        #get closest 5 points\n",
    "        num_neighbors = 1\n",
    "        closest_indices = np.argsort(distances[0])[:num_neighbors]\n",
    "        closest_distances = distances[0][closest_indices]\n",
    "\n",
    "        # classs of 5 closest points\n",
    "        closest_classes = all_ids[closest_indices]\n",
    "\n",
    "        # get the majority class and write into id_matches\n",
    "        most_common_class, count = Counter(closest_classes).most_common(1)[0]\n",
    "        id_matches[img_id] = (most_common_class, closest_distances)\n",
    "\n",
    "\n",
    "    print(\"id mateches\", id_matches)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    turn the best match id into a pixelwise level, then patchwise level\n",
    "    \"\"\"\n",
    "\n",
    "    #assign each pixel to the new value, curr_scan_id , frame number is only for the size\n",
    "    new_img_pixelwise = generate_pairs_pixel_level(input_info[frame], id_matches)\n",
    "\n",
    "    #quantize to patches\n",
    "    new_img_patchwise = quantize_to_patch_level(new_img_pixelwise)\n",
    "    #aggregate to patches and colour it, we want the colours which are used in curr_scan id\n",
    "    new_img_colour = create_color_img_from_obj_id(data_dir,curr_scan_id, new_img_patchwise)\n",
    "    print(\"unique numbers in new img patchwise \", np.unique(new_img_patchwise))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    access the gt of the image and get the accuracy of the prediction, print result and display \n",
    "    \"\"\"\n",
    "    #get the gt patches for the segmented scene! so for the dinov2 segmentation: but the colour will be based on the current scene\n",
    "    gt_input_patchwise_path =  osp.join(data_dir,\"files/patch_anno/patch_anno_32_18\", new_scan_id + '.pkl')\n",
    "    with open(gt_input_patchwise_path, 'rb') as file:\n",
    "        gt_input_patchwise = pickle.load(file)\n",
    "    #print(\"unique numbers in gt_input_colour \", np.unique(gt_input_patchwise[frame]))\n",
    "    gt_input_colour = create_color_img_from_obj_id(data_dir,curr_scan_id,gt_input_patchwise[frame])\n",
    "\n",
    "\n",
    "    accuracy = get_accuracy(gt_input_patchwise[frame],new_img_patchwise)\n",
    "    total_accuracy.append(accuracy[0])\n",
    "    print(\"the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is\", accuracy)\n",
    "\n",
    "\n",
    "    # \"\"\"\n",
    "    # display the newly computed images next to each other\n",
    "    # \"\"\"\n",
    "\n",
    "    #display the gt_image and the new patchwise image next to each other\n",
    "    title1 = \"gt_patces\"\n",
    "    title2 = \"dino_seg_patches\"\n",
    "\n",
    "    # Swap color channels from RGB to BGR (if needed)\n",
    "    gt_input_colour_bgr = gt_input_colour[..., ::-1]  # Convert RGB to BGR\n",
    "    new_img_colour_bgr = new_img_colour[..., ::-1]    # Convert RGB to BGR\n",
    "\n",
    "    # Create a blank canvas to combine images horizontally\n",
    "    height = max(gt_input_colour.shape[0], new_img_colour.shape[0])  # Max height of both images\n",
    "    width = gt_input_colour.shape[1] + new_img_colour.shape[1] + 20  # Total width of both images with a small gap\n",
    "    combined_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Place images with titles on the blank canvas\n",
    "    combined_image[:gt_input_colour.shape[0], :gt_input_colour.shape[1]] = gt_input_colour\n",
    "    combined_image[:new_img_colour.shape[0], gt_input_colour.shape[1] + 20:] = new_img_colour\n",
    "\n",
    "    # Add titles to the images\n",
    "    cv2.putText(combined_image, title1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(combined_image, title2, (gt_input_colour.shape[1] + 30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the combined image\n",
    "    cv2.imshow('Two Images Side by Side', combined_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "np_accuracy = np.array(total_accuracy)\n",
    "mean = np.mean(np_accuracy)\n",
    "print(\"mean of total accuracy \", mean)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we use maxpooling insteat of avg pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id mateches {1: (8, 0), 2: (13, 0), 3: (1, 0), 4: (10, 0), 5: (27, 0), 6: (2, 0), 7: (18, 0), 8: (10, 0), 9: (27, 0), 10: (11, 0), 11: (5, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  2.  5.  8. 10. 11. 13. 18. 27.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAErCAYAAABDzICRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAF5UlEQVR4nO3dMU4qawCG4R9CAxVBFzRhQ+7Awh3YuhhX4Apcg7E0FkZOeYqT4yVcuYP3fZ5kOib5MgW8zBSzOBwOhwEAZC3nHgAAzEsMAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIO7oGFgsFj/yuLu7O+f1+9Lt7e3Ju6+vr8fr6+ts2wH47d98n899HMOdAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxq7kH/J/t9/uxWp12iTebzViv19+8CAD+JAbOaJqmMU3T3DMA4EseEwBAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSt5h4AAJduv9+P1eq0n8y3t7dxf38/3t/fv3nV9xEDAPAPpmka0zSddO7Ly8t4eHi46BjwmAAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACDOWwsB4IyWy+XY7XZjubzc/99iAADOaLvdjqenp/H5+Tn3lL8SAwBwRsvlcmy327lnfOly71kAAP8JMQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgbjX3gHN7fHwcHx8fJ5272WzGzc3NWK/X37wKAC7H4nA4HI764GJx7i0X5+rqajw/P4/dbjf3FAA4G48JACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACBODABAnBgAgDgxAABxYgAA4sQAAMStjv3gkW86BgB+GHcGACBODABAnBgAgDgxAABxYgAA4sQAAMSJAQCIEwMAECcGACDuF+vzOagO2RTjAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.2638888888888889, 0.2638888888888889)\n",
      "id mateches {1: (21, 0), 2: (26, 0), 3: (26, 0), 4: (9, 0), 5: (27, 0), 6: (1, 0), 7: (5, 0), 8: (10, 0), 9: (8, 0), 10: (23, 0), 11: (12, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  5.  8.  9. 10. 12. 21. 23. 26. 27.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.4913194444444444, 0.4913194444444444)\n",
      "id mateches {1: (25, 0), 2: (25, 0), 3: (23, 0), 4: (11, 0), 5: (27, 0), 6: (1, 0), 7: (16, 0), 8: (29, 0), 9: (27, 0), 10: (22, 0), 11: (22, 0), 12: (8, 0), 13: (20, 0), 14: (9, 0), 15: (8, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  8.  9. 11. 16. 20. 22. 23. 25. 27. 29.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.515625, 0.515625)\n",
      "id mateches {1: (22, 0), 2: (11, 0), 3: (27, 0), 4: (1, 0), 5: (1, 0), 6: (8, 0), 7: (23, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  8. 11. 22. 23. 27.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6041666666666666, 0.6041666666666666)\n",
      "id mateches {1: (9, 0), 2: (26, 0), 3: (27, 0), 4: (1, 0), 5: (8, 0), 6: (2, 0), 7: (15, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  2.  8.  9. 15. 26. 27.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5486111111111112, 0.5486111111111112)\n",
      "id mateches {1: (26, 0), 2: (27, 0), 3: (1, 0), 4: (22, 0)}\n",
      "unique numbers in new img patchwise  [ 1. 22. 26. 27.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5590277777777778, 0.5590277777777778)\n",
      "id mateches {1: (9, 0), 2: (27, 0), 3: (25, 0), 4: (20, 0), 5: (24, 0), 6: (26, 0)}\n",
      "unique numbers in new img patchwise  [ 9. 20. 24. 25. 26. 27.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.0, 0.0)\n",
      "id mateches {1: (11, 0), 2: (22, 0), 3: (11, 0), 4: (3, 0), 5: (9, 0), 6: (24, 0), 7: (24, 0), 8: (8, 0)}\n",
      "unique numbers in new img patchwise  [ 3.  8.  9. 11. 22. 24.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.003472222222222222, 0.003472222222222222)\n",
      "id mateches {1: (23, 0), 2: (26, 0), 3: (25, 0), 4: (8, 0), 5: (24, 0), 6: (20, 0), 7: (6, 0), 8: (22, 0), 9: (2, 0), 10: (10, 0), 11: (8, 0)}\n",
      "unique numbers in new img patchwise  [ 2.  6.  8. 10. 20. 22. 23. 24. 25. 26.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.001736111111111111, 0.001736111111111111)\n",
      "id mateches {1: (8, 0), 2: (12, 0), 3: (25, 0), 4: (24, 0), 5: (8, 0), 6: (27, 0), 7: (1, 0), 8: (25, 0), 9: (27, 0), 10: (2, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  2.  8. 12. 24. 25. 27.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6232638888888888, 0.6232638888888888)\n",
      "id mateches {1: (12, 0), 2: (25, 0), 3: (25, 0), 4: (20, 0), 5: (27, 0), 6: (5, 0), 7: (23, 0), 8: (29, 0), 9: (1, 0), 10: (26, 0), 11: (2, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  2.  5. 12. 20. 23. 25. 26. 27. 29.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.019097222222222224, 0.019097222222222224)\n",
      "id mateches {1: (1, 0), 2: (8, 0)}\n",
      "unique numbers in new img patchwise  [1. 8.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6145833333333334, 0.6145833333333334)\n",
      "id mateches {1: (5, 0), 2: (22, 0), 3: (20, 0), 4: (100, 0), 5: (9, 0), 6: (23, 0), 7: (3, 0), 8: (25, 0)}\n",
      "unique numbers in new img patchwise  [  3.   5.   9.  20.  22.  23.  25. 100.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.0, 0.0)\n",
      "mean of total accuracy  0.32652243589743585\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "for the current scene: access all the precomputed dinov2 features and do pca of the space\n",
    "\"\"\"\n",
    "#get all the features of the current scene\n",
    "all_info_path = osp.join(data_dir,\"files/Features2D/projection/DinoV2/patch_32_18\", curr_scan_id + \".pkl\")\n",
    "with open(all_info_path, 'rb') as file:\n",
    "    all_features_curr_scene = pickle.load(file)\n",
    "\n",
    "#using avg pooling\n",
    "all_features, all_ids, all_colors = max_pooling(all_features_curr_scene)\n",
    "all_features = np.array(all_features)\n",
    "all_ids = np.array(all_ids)\n",
    "all_colors = np.array(all_colors)\n",
    "\n",
    "#perform pca to 3D reduction\n",
    "pca = PCA(n_components=3)\n",
    "reduced_points = pca.fit_transform(all_features)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "access the features of the new scene and the info corresponding to the patches\n",
    "\"\"\"\n",
    "\n",
    "#get the Dinov2 features for the input image ( the segmented one)\n",
    "input_featues_path = osp.join(data_dir,\"files/Features2D/dino_segmentation/DinoV2/patch_32_18\", new_scan_id + \".pkl\")\n",
    "with open(input_featues_path, 'rb') as file:\n",
    "    input_features = pickle.load(file)\n",
    "\n",
    "#get the data information on the segmentation of the input image\n",
    "input_info_path = osp.join(data_dir,\"files/Segmentation/DinoV2/objects\", new_scan_id + \".pkl\")\n",
    "with open(input_info_path, 'rb') as file:\n",
    "    input_info = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "total_accuracy= []\n",
    "\n",
    "\"\"\"\n",
    "decide the id matches based on the projected points and 5 neighbouhood vote\n",
    "\"\"\"\n",
    "\n",
    "#iterate over each frame of the newscan and perform the assignment \n",
    "for frame in input_features:\n",
    "    #initialze the matches\n",
    "    id_matches= {}\n",
    "\n",
    "    input_dict = input_features[frame]\n",
    "\n",
    "    #iterate through every patch of the segmented new scene image, project it into the pca space ang get the closest\n",
    "    for img_id, img_vec in input_dict.items():\n",
    "        #reshape and project into the space\n",
    "\n",
    "        #use avg pooling\n",
    "        aggregated_vec = np.max(img_vec, axis=0)\n",
    "        #print(\"new vec shape\", aggregated_vec.shape)\n",
    "        reduced_unseen_point = pca.transform(aggregated_vec.reshape(1,-1))\n",
    "\n",
    "        # cosine distances to all reduced points\n",
    "        #distances = cosine_distances(reduced_unseen_point, reduced_points)\n",
    "\n",
    "        #eucledian dist\n",
    "        distances = euclidean_distances(reduced_unseen_point, reduced_points)\n",
    "\n",
    "        #get closest 5 points\n",
    "        num_neighbors = 5\n",
    "        closest_indices = np.argsort(distances[0])[:num_neighbors]\n",
    "\n",
    "        # classs of 5 closest points\n",
    "        closest_classes = all_ids[closest_indices]\n",
    "\n",
    "        # get the majority class and write into id_matches\n",
    "        most_common_class, count = Counter(closest_classes).most_common(1)[0]\n",
    "        id_matches[img_id] = (most_common_class, 0)\n",
    "\n",
    "\n",
    "    print(\"id mateches\", id_matches)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    turn the best match id into a pixelwise level, then patchwise level\n",
    "    \"\"\"\n",
    "\n",
    "    #assign each pixel to the new value, curr_scan_id , frame number is only for the size\n",
    "    new_img_pixelwise = generate_pairs_pixel_level(input_info[frame], id_matches)\n",
    "\n",
    "    #quantize to patches\n",
    "    new_img_patchwise = quantize_to_patch_level(new_img_pixelwise)\n",
    "    #aggregate to patches and colour it, we want the colours which are used in curr_scan id\n",
    "    new_img_colour = create_color_img_from_obj_id(data_dir,curr_scan_id, new_img_patchwise)\n",
    "    print(\"unique numbers in new img patchwise \", np.unique(new_img_patchwise))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    access the gt of the image and get the accuracy of the prediction, print result and display \n",
    "    \"\"\"\n",
    "    #get the gt patches for the segmented scene! so for the dinov2 segmentation: but the colour will be based on the current scene\n",
    "    gt_input_patchwise_path =  osp.join(data_dir,\"files/patch_anno/patch_anno_32_18\", new_scan_id + '.pkl')\n",
    "    with open(gt_input_patchwise_path, 'rb') as file:\n",
    "        gt_input_patchwise = pickle.load(file)\n",
    "    #print(\"unique numbers in gt_input_colour \", np.unique(gt_input_patchwise[frame]))\n",
    "    gt_input_colour = create_color_img_from_obj_id(data_dir,curr_scan_id,gt_input_patchwise[frame])\n",
    "    plt.imshow(gt_input_colour)\n",
    "    plt.axis('off')  # Optional: turn off axis\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    accuracy = get_accuracy(gt_input_patchwise[frame],new_img_patchwise)\n",
    "    total_accuracy.append(accuracy[0])\n",
    "    print(\"the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is\", accuracy)\n",
    "\n",
    "\n",
    "    # \"\"\"\n",
    "    # display the newly computed images next to each other\n",
    "    # \"\"\"\n",
    "\n",
    "    # #display the gt_image and the new patchwise image next to each other\n",
    "    # title1 = \"gt_patces\"\n",
    "    # title2 = \"dino_seg_patches\"\n",
    "\n",
    "    # # Create a blank canvas to combine images horizontally\n",
    "    # height = max(gt_input_colour.shape[0], new_img_colour.shape[0])  # Max height of both images\n",
    "    # width = gt_input_colour.shape[1] + new_img_colour.shape[1] + 20  # Total width of both images with a small gap\n",
    "    # combined_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # # Place images with titles on the blank canvas\n",
    "    # combined_image[:gt_input_colour.shape[0], :gt_input_colour.shape[1]] = gt_input_colour\n",
    "    # combined_image[:new_img_colour.shape[0], gt_input_colour.shape[1] + 20:] = new_img_colour\n",
    "\n",
    "    # # Add titles to the images\n",
    "    # cv2.putText(combined_image, title1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    # cv2.putText(combined_image, title2, (gt_input_colour.shape[1] + 30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # # Display the combined image\n",
    "    # cv2.imshow('Two Images Side by Side', combined_image)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "np_accuracy = np.array(total_accuracy)\n",
    "mean = np.mean(np_accuracy)\n",
    "print(\"mean of total accuracy \", mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
