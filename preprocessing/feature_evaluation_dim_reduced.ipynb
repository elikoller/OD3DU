{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/home/ekoller/BT\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import os \n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import plyfile\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import os.path as osp\n",
    "import sys\n",
    "ws_dir = '/local/home/ekoller/BT'\n",
    "print(ws_dir)\n",
    "sys.path.append(ws_dir)\n",
    "from utils import evaluation, scan3r\n",
    "\n",
    "\n",
    "data_dir ='/local/home/ekoller/R3Scan'\n",
    "scenes_dir = '/local/home/ekoller/R3Scan/scenes'\n",
    "#scan_id= \"38770c95-86d7-27b8-8717-3485b411ddc7\" #is reference scan  since it is a reference scan everything shouls be correctly hit\n",
    "curr_scan_id =  \"02b33e01-be2b-2d54-93fb-4145a709cec5\"#\"02b33e01-be2b-2d54-93fb-4145a709cec5\" \n",
    "new_scan_id =  \"fcf66d88-622d-291c-871f-699b2d063630\" #\"fcf66d8a-622d-291c-8429-0e1109c6bb26\"\n",
    "frame_number = \"000007\"\n",
    "curr_frame_number = \"000007\"\n",
    "new_frame_number = \"000008\"\n",
    "patch_h= 18\n",
    "image_height = 540\n",
    "image_width = 960\n",
    "patch_w = 32\n",
    "patch_height = 30\n",
    "patch_width = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize(mask, new_ids, title):\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "\n",
    "#     # Display the mask\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.imshow(mask, cmap='gray')\n",
    "#     plt.title('Mask')\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     # Display the new_ids array\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.imshow(new_ids, cmap='gray')\n",
    "#     plt.title(title)\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#visualitation of the following: with the new ids fill in an image with the new ids\n",
    "#compute the resulting image when comparing stuff\n",
    "\n",
    "def generate_pairs_pixel_level( dino_data,id_matches):\n",
    "    \n",
    "    new_ids = np.zeros((image_height, image_width))\n",
    "\n",
    "\n",
    "    #go over every mask and fill in the id into the new_ids which it got mapped to\n",
    "    for seg_region in dino_data:\n",
    "        mask_id = seg_region[\"object_id\"]\n",
    "        #print(\"mask id \", mask_id)\n",
    "        #get to what the region mapped in the embeddings\n",
    "        matched_id = id_matches[mask_id]\n",
    "        #print(\"matched id \", matched_id)\n",
    "        mask = seg_region[\"mask\"]\n",
    "        boolean_mask = mask == 225\n",
    "        new_ids[boolean_mask] = matched_id[0] #[0] is the id the second one is the error\n",
    "        #visualize(boolean_mask, new_ids, f'Updated new_ids with mask id {mask_id}')\n",
    "\n",
    "    #returns the new ids on a pixel wise level\n",
    "    return new_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this codesegment takes an image on a pixel wise level and quantizes it such that every patch has only the id of the most often occuring id\n",
    "def quantize_to_patch_level(pixelwise_img):\n",
    "    #get the shape of the pixelwise img\n",
    "    input_h, input_w = pixelwise_img.shape\n",
    "    patch_width = int(input_w/patch_w)\n",
    "    patch_height= int(input_h/patch_h)\n",
    "\n",
    "    patchwise_w = patch_w #number of patches\n",
    "    patchwise_h = patch_h\n",
    "\n",
    "    patchwise_id = np.zeros((patchwise_h,patchwise_w))\n",
    "\n",
    "    for i in range(patchwise_h):\n",
    "            for j in range(patchwise_w):\n",
    "                # Define the coordinates of the current patch\n",
    "                h_start = i * patch_height\n",
    "                w_start = j * patch_width\n",
    "                h_end = h_start + patch_height\n",
    "                w_end = w_start + patch_width\n",
    "                \n",
    "                # Get the current patch from the input matrix\n",
    "                patch = pixelwise_img[h_start:h_end, w_start:w_end]\n",
    "                \n",
    "                # get the most reoccuring id of the patch\n",
    "                flattened_patch = patch.flatten()\n",
    "                # Find the most common value in the patch\n",
    "                value_counts = Counter(flattened_patch)\n",
    "                most_common_id = value_counts.most_common(1)[0][0]\n",
    "                \n",
    "                # Assign the most common ID to the new matrix\n",
    "                patchwise_id[i, j] = most_common_id\n",
    "\n",
    "\n",
    "    return patchwise_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur scan id colors dict_keys([4, 18, 2, 0, 3, 1, 100, 16, 8, 9, 24, 25, 29, 6, 26, 10, 11, 23, 27, 22, 5, 20, 12, 15, 13, 14, 103, 21])\n",
      "new scan id colors dict_keys([4, 9, 24, 1, 8, 22, 23, 12, 10, 0, 27])\n"
     ]
    }
   ],
   "source": [
    "#for a given scene get the colours of the differnt object_ids\n",
    "def get_id_colours(data_dir,scan_id):\n",
    "    #access the mesh file to get the colour of the ids\n",
    "    mesh_file = osp.join(data_dir,\"scenes\", scan_id, \"labels.instances.annotated.v2.ply\")\n",
    "    ply_data = plyfile.PlyData.read(mesh_file)\n",
    "    # Extract vertex data\n",
    "    vertices = ply_data['vertex']\n",
    "    vertex_count = len(vertices)\n",
    "    \n",
    "    # Initialize dictionary to store object_id -> color mappings\n",
    "    object_colors = {}\n",
    "    \n",
    "   # Iterate through vertices\n",
    "    for i in range(vertex_count):\n",
    "        vertex = vertices[i]\n",
    "        object_id = vertex['objectId']\n",
    "        color = (vertex['red'], vertex['green'], vertex['blue'])\n",
    "        \n",
    "        # Check if object_id already in dictionary, otherwise initialize a Counter\n",
    "        if object_id in object_colors:\n",
    "            object_colors[object_id][color] += 1\n",
    "        else:\n",
    "            object_colors[object_id] = Counter({color: 1})\n",
    "    \n",
    "    # Convert Counter to dictionary with most frequent color\n",
    "    for object_id, color_counter in object_colors.items():\n",
    "        most_common_color = color_counter.most_common(1)[0][0]\n",
    "        object_colors[object_id] = np.array(most_common_color[::-1])\n",
    "    \n",
    "    return object_colors\n",
    "\n",
    "colors = get_id_colours(data_dir,curr_scan_id)\n",
    "print(\"cur scan id colors\", colors.keys())\n",
    "colors = get_id_colours(data_dir,new_scan_id)\n",
    "print(\"new scan id colors\", colors.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function creates a color image of the size 960x540 from the patches\n",
    "def create_color_img_from_obj_id(data_dir,scan_id,obj_id_mat):\n",
    "    #access the mesh file to get the colour of the ids\n",
    "    colour_dict = get_id_colours(data_dir, scan_id)\n",
    "    #initialize the new matrix \n",
    "    og_height, og_width = obj_id_mat.shape\n",
    "    new_height = og_height * patch_height\n",
    "    new_width = og_width * patch_width\n",
    "\n",
    "    colour_mat = np.zeros((new_height,new_width,3))\n",
    "\n",
    "    #go over each element and assign the colour of the dictionary\n",
    "    for h in range(og_height):\n",
    "        for w in range(og_width):\n",
    "            if obj_id_mat[h][w] in colour_dict.keys():\n",
    "                colour = colour_dict[obj_id_mat[h][w]]\n",
    "                colour_mat[h*patch_height:(h+1)*patch_height, w*patch_width:(w+1)*patch_width] = colour\n",
    "\n",
    "\n",
    "    return colour_mat\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a gt patchwise matrix and a newly calculated matrix compute the accuracy\n",
    "#aaparently this is not the same as accuracy\n",
    "#given a gt patchwise matrix and a newly calculated matrix compute the accuracy\n",
    "def get_accuracy(gt_patches, computed_patches):\n",
    "    #make sure we dont do something dumm lol\n",
    "    assert gt_patches.shape == computed_patches.shape, \"Matrices must have the same shape\"\n",
    "\n",
    "    # Flatten matrices to iterate over each element\n",
    "    flat_gt = gt_patches.flatten()\n",
    "    flat_comp = computed_patches.flatten()\n",
    "\n",
    "    percentage = np.zeros_like(flat_gt)\n",
    "    \n",
    "    total_instances = len(flat_gt)\n",
    "    correct_instances = 0\n",
    "\n",
    "    #compute based on the other way\n",
    "    for idx in range(len(percentage)):\n",
    "        if flat_gt[idx] == flat_comp[idx]:\n",
    "            percentage[idx] = 1\n",
    "    \n",
    "    # Count correct instances where IDs match\n",
    "    for id_gt, id_comp in zip(flat_gt, flat_comp):\n",
    "        if id_gt == id_comp:\n",
    "            correct_instances += 1\n",
    "            \n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = correct_instances / total_instances\n",
    "    \n",
    "    return accuracy, np.mean(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def avg_pooling(all_features_curr_scene):\n",
    "    #get every fearure vector and average the patches to one patch\n",
    "    all_features = []\n",
    "    all_colors = []\n",
    "    all_ids = []\n",
    "    for frame in all_features_curr_scene:\n",
    "        for obj_id in all_features_curr_scene[frame]:\n",
    "            if obj_id != 0:\n",
    "                matrix = all_features_curr_scene[frame][obj_id]\n",
    "                #averaging\n",
    "                aggregated_feature = np.mean(matrix, axis=0)  \n",
    "                all_features.append(aggregated_feature)\n",
    "                all_ids.append(obj_id)\n",
    "                #all_colors.append(tuple(c / 255.0 for c in colors[obj_id]))\n",
    "\n",
    "    return all_features, all_ids,all_colors\n",
    "\n",
    "def max_pooling(all_features_curr_scene):\n",
    "    #perform max pooling \n",
    "    all_features = []\n",
    "    all_colors = []\n",
    "    all_ids = []\n",
    "    for frame in all_features_curr_scene:\n",
    "        for obj_id in all_features_curr_scene[frame]:\n",
    "            if obj_id != 0:\n",
    "                matrix = all_features_curr_scene[frame][obj_id]\n",
    "\n",
    "                #max pooling\n",
    "                aggregated_feature = np.max(matrix, axis=0)  # Max pooling across patches\n",
    "                all_features.append(aggregated_feature)\n",
    "                all_ids.append(obj_id)\n",
    "                # append color corresponding to the object id\n",
    "                #all_colors.append(tuple(c / 255.0 for c in colors[obj_id]))\n",
    "\n",
    "    return all_features, all_ids, all_colors\n",
    "\n",
    "\n",
    "def display_images(gt_image, new_image, title1, title2):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(gt_image)\n",
    "    plt.title(title1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(new_image)\n",
    "    plt.title(title2)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given an input image which was segmented using dino, use PCA & either average or maxpooling to compute the accuracy of the predicted object id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id mateches {1: (23, array([0.01570327], dtype=float32)), 2: (12, array([0.01312272], dtype=float32)), 3: (1, array([0.02637017], dtype=float32)), 4: (5, array([0.00739837], dtype=float32)), 5: (9, array([0.0159371], dtype=float32)), 6: (8, array([0.01632525], dtype=float32)), 7: (18, array([0.02054708], dtype=float32)), 8: (9, array([0.00719253], dtype=float32)), 9: (11, array([0.01522948], dtype=float32)), 10: (10, array([0.00754274], dtype=float32)), 11: (5, array([0.01760075], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  5.  8.  9. 10. 11. 12. 18. 23.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.3663194444444444, 0.3663194444444444)\n",
      "id mateches {1: (23, array([0.02890418], dtype=float32)), 2: (27, array([0.01526387], dtype=float32)), 3: (27, array([0.00507575], dtype=float32)), 4: (22, array([0.02434961], dtype=float32)), 5: (8, array([0.01412568], dtype=float32)), 6: (1, array([0.02946728], dtype=float32)), 7: (4, array([0.00834342], dtype=float32)), 8: (20, array([0.00536672], dtype=float32)), 9: (8, array([0.02702678], dtype=float32)), 10: (2, array([0.00914445], dtype=float32)), 11: (1, array([0.00797968], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  2.  4.  8. 20. 22. 23. 27.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5416666666666666, 0.5416666666666666)\n",
      "id mateches {1: (26, array([0.01202647], dtype=float32)), 2: (21, array([0.02524503], dtype=float32)), 3: (24, array([0.00897213], dtype=float32)), 4: (22, array([0.03573671], dtype=float32)), 5: (18, array([0.01583523], dtype=float32)), 6: (1, array([0.02371688], dtype=float32)), 7: (3, array([0.01119746], dtype=float32)), 8: (20, array([0.0151446], dtype=float32)), 9: (20, array([0.00879547], dtype=float32)), 10: (11, array([0.00762003], dtype=float32)), 11: (11, array([0.00954886], dtype=float32)), 12: (8, array([0.02242653], dtype=float32)), 13: (1, array([0.01104789], dtype=float32)), 14: (3, array([0.01909309], dtype=float32)), 15: (103, array([0.0082426], dtype=float32))}\n",
      "unique numbers in new img patchwise  [  1.   3.   8.  11.  18.  20.  21.  22.  24.  26. 103.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.515625, 0.515625)\n",
      "id mateches {1: (26, array([0.01065832], dtype=float32)), 2: (22, array([0.0252062], dtype=float32)), 3: (1, array([0.01333159], dtype=float32)), 4: (1, array([0.02454306], dtype=float32)), 5: (6, array([0.0133764], dtype=float32)), 6: (8, array([0.02142951], dtype=float32)), 7: (5, array([0.01204913], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  5.  6.  8. 22. 26.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5607638888888888, 0.5607638888888888)\n",
      "id mateches {1: (26, array([0.01185611], dtype=float32)), 2: (22, array([0.00481515], dtype=float32)), 3: (26, array([0.01362899], dtype=float32)), 4: (1, array([0.0236815], dtype=float32)), 5: (8, array([0.0145503], dtype=float32)), 6: (6, array([0.01989206], dtype=float32)), 7: (2, array([0.01205223], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  2.  6.  8. 22. 26.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5486111111111112, 0.5486111111111112)\n",
      "id mateches {1: (26, array([0.01400753], dtype=float32)), 2: (12, array([0.0113122], dtype=float32)), 3: (1, array([0.01167631], dtype=float32)), 4: (100, array([0.01328255], dtype=float32))}\n",
      "unique numbers in new img patchwise  [  1.  12.  26. 100.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6006944444444444, 0.6006944444444444)\n",
      "id mateches {1: (26, array([0.0171039], dtype=float32)), 2: (1, array([0.00605737], dtype=float32)), 3: (20, array([0.0172584], dtype=float32)), 4: (1, array([0.00771072], dtype=float32)), 5: (1, array([0.0209359], dtype=float32)), 6: (5, array([0.01264728], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  5. 20. 26.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6006944444444444, 0.6006944444444444)\n",
      "id mateches {1: (22, array([0.02175163], dtype=float32)), 2: (11, array([0.0103588], dtype=float32)), 3: (8, array([0.00425652], dtype=float32)), 4: (1, array([0.01351001], dtype=float32)), 5: (20, array([0.0149256], dtype=float32)), 6: (12, array([0.01384641], dtype=float32)), 7: (11, array([0.00765502], dtype=float32)), 8: (20, array([0.00820862], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  8. 11. 12. 20. 22.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6631944444444444, 0.6631944444444444)\n",
      "id mateches {1: (10, array([0.01616197], dtype=float32)), 2: (26, array([0.00555241], dtype=float32)), 3: (100, array([0.01794639], dtype=float32)), 4: (8, array([0.01908192], dtype=float32)), 5: (8, array([0.00891095], dtype=float32)), 6: (1, array([0.02480392], dtype=float32)), 7: (2, array([0.01241749], dtype=float32)), 8: (12, array([0.02260527], dtype=float32)), 9: (11, array([0.0106607], dtype=float32)), 10: (8, array([0.01770554], dtype=float32)), 11: (20, array([0.00505923], dtype=float32))}\n",
      "unique numbers in new img patchwise  [  1.   2.   8.  10.  11.  12.  20.  26. 100.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5763888888888888, 0.5763888888888888)\n",
      "id mateches {1: (12, array([0.01616702], dtype=float32)), 2: (18, array([0.01358708], dtype=float32)), 3: (8, array([0.01839471], dtype=float32)), 4: (8, array([0.01075059], dtype=float32)), 5: (10, array([0.02290128], dtype=float32)), 6: (27, array([0.01288162], dtype=float32)), 7: (1, array([0.00711795], dtype=float32)), 8: (9, array([0.01533557], dtype=float32)), 9: (8, array([0.01702167], dtype=float32)), 10: (10, array([0.01594201], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  8.  9. 10. 12. 18. 27.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.7517361111111112, 0.7517361111111112)\n",
      "id mateches {1: (8, array([0.01344401], dtype=float32)), 2: (11, array([0.01987344], dtype=float32)), 3: (29, array([0.01038167], dtype=float32)), 4: (1, array([0.00971259], dtype=float32)), 5: (8, array([0.02098707], dtype=float32)), 6: (15, array([0.00808654], dtype=float32)), 7: (10, array([0.01273002], dtype=float32)), 8: (20, array([0.00767714], dtype=float32)), 9: (6, array([0.01147991], dtype=float32)), 10: (29, array([0.00824384], dtype=float32)), 11: (1, array([0.00750556], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  6.  8. 10. 11. 15. 20. 29.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6076388888888888, 0.6076388888888888)\n",
      "id mateches {1: (1, array([0.01386196], dtype=float32)), 2: (8, array([0.00739832], dtype=float32))}\n",
      "unique numbers in new img patchwise  [1. 8.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6145833333333334, 0.6145833333333334)\n",
      "id mateches {1: (5, array([0.00914396], dtype=float32)), 2: (8, array([0.01399027], dtype=float32)), 3: (1, array([0.01699795], dtype=float32)), 4: (11, array([0.01739462], dtype=float32)), 5: (1, array([0.02151607], dtype=float32)), 6: (10, array([0.01093858], dtype=float32)), 7: (20, array([0.01126606], dtype=float32)), 8: (11, array([0.00889421], dtype=float32))}\n",
      "unique numbers in new img patchwise  [ 1.  5.  8. 10. 11. 20.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6388888888888888, 0.6388888888888888)\n",
      "mean of total accuracy  0.5836004273504274\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "for the current scene: access all the precomputed dinov2 features and do pca of the space\n",
    "\"\"\"\n",
    "#get all the features of the current scene\n",
    "all_info_path = osp.join(data_dir,\"files/Features2D/projection/DinoV2/patch_32_18\", curr_scan_id + \".pkl\")\n",
    "with open(all_info_path, 'rb') as file:\n",
    "    all_features_curr_scene = pickle.load(file)\n",
    "\n",
    "#using avg pooling\n",
    "all_features, all_ids, all_colors = avg_pooling(all_features_curr_scene)\n",
    "all_features = np.array(all_features)\n",
    "all_ids = np.array(all_ids)\n",
    "all_colors = np.array(all_colors)\n",
    "\n",
    "#perform pca to 3D reduction\n",
    "pca = PCA(n_components=3)\n",
    "reduced_points = pca.fit_transform(all_features)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "access the features of the new scene and the info corresponding to the patches\n",
    "\"\"\"\n",
    "\n",
    "#get the Dinov2 features for the input image ( the segmented one)\n",
    "input_featues_path = osp.join(data_dir,\"files/Features2D/dino_segmentation/DinoV2/patch_32_18\", new_scan_id + \".pkl\")\n",
    "with open(input_featues_path, 'rb') as file:\n",
    "    input_features = pickle.load(file)\n",
    "\n",
    "#get the data information on the segmentation of the input image\n",
    "input_info_path = osp.join(data_dir,\"files/Segmentation/DinoV2/objects\", new_scan_id + \".pkl\")\n",
    "with open(input_info_path, 'rb') as file:\n",
    "    input_info = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "total_accuracy= []\n",
    "\n",
    "\"\"\"\n",
    "decide the id matches based on the projected points and 5 neighbouhood vote\n",
    "\"\"\"\n",
    "\n",
    "#iterate over each frame of the newscan and perform the assignment \n",
    "for frame in input_features:\n",
    "    #initialze the matches\n",
    "    id_matches= {}\n",
    "\n",
    "    input_dict = input_features[frame]\n",
    "\n",
    "    #iterate through every patch of the segmented new scene image, project it into the pca space ang get the closest\n",
    "    for img_id, img_vec in input_dict.items():\n",
    "        #reshape and project into the space\n",
    "\n",
    "        #use avg pooling\n",
    "        aggregated_vec = np.mean(img_vec, axis=0)\n",
    "        #print(\"new vec shape\", aggregated_vec.shape)\n",
    "        reduced_unseen_point = pca.transform(aggregated_vec.reshape(1,-1))\n",
    "\n",
    "        # cosine distances to all reduced points\n",
    "        #distances = cosine_distances(reduced_unseen_point, reduced_points)\n",
    "\n",
    "        #eucledian dist\n",
    "        distances = euclidean_distances(reduced_unseen_point, reduced_points)\n",
    "\n",
    "        #get closest 5 points\n",
    "        num_neighbors = 1\n",
    "        closest_indices = np.argsort(distances[0])[:num_neighbors]\n",
    "        closest_distances = distances[0][closest_indices]\n",
    "\n",
    "        # classs of 5 closest points\n",
    "        closest_classes = all_ids[closest_indices]\n",
    "\n",
    "        # get the majority class and write into id_matches\n",
    "        most_common_class, count = Counter(closest_classes).most_common(1)[0]\n",
    "        id_matches[img_id] = (most_common_class, closest_distances)\n",
    "\n",
    "\n",
    "    print(\"id mateches\", id_matches)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    turn the best match id into a pixelwise level, then patchwise level\n",
    "    \"\"\"\n",
    "\n",
    "    #assign each pixel to the new value, curr_scan_id , frame number is only for the size\n",
    "    new_img_pixelwise = generate_pairs_pixel_level(input_info[frame], id_matches)\n",
    "\n",
    "    #quantize to patches\n",
    "    new_img_patchwise = quantize_to_patch_level(new_img_pixelwise)\n",
    "    #aggregate to patches and colour it, we want the colours which are used in curr_scan id\n",
    "    new_img_colour = create_color_img_from_obj_id(data_dir,curr_scan_id, new_img_patchwise)\n",
    "    print(\"unique numbers in new img patchwise \", np.unique(new_img_patchwise))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    access the gt of the image and get the accuracy of the prediction, print result and display \n",
    "    \"\"\"\n",
    "    #get the gt patches for the segmented scene! so for the dinov2 segmentation: but the colour will be based on the current scene\n",
    "    gt_input_patchwise_path =  osp.join(data_dir,\"files/patch_anno/patch_anno_32_18\", new_scan_id + '.pkl')\n",
    "    with open(gt_input_patchwise_path, 'rb') as file:\n",
    "        gt_input_patchwise = pickle.load(file)\n",
    "    #print(\"unique numbers in gt_input_colour \", np.unique(gt_input_patchwise[frame]))\n",
    "    gt_input_colour = create_color_img_from_obj_id(data_dir,curr_scan_id,gt_input_patchwise[frame])\n",
    "\n",
    "\n",
    "    accuracy = get_accuracy(gt_input_patchwise[frame],new_img_patchwise)\n",
    "    total_accuracy.append(accuracy[0])\n",
    "    print(\"the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is\", accuracy)\n",
    "\n",
    "\n",
    "    # \"\"\"\n",
    "    # display the newly computed images next to each other\n",
    "    # \"\"\"\n",
    "\n",
    "    # #display the gt_image and the new patchwise image next to each other\n",
    "    # title1 = \"gt_patces\"\n",
    "    # title2 = \"dino_seg_patches\"\n",
    "\n",
    "    # # Create a blank canvas to combine images horizontally\n",
    "    # height = max(gt_input_colour.shape[0], new_img_colour.shape[0])  # Max height of both images\n",
    "    # width = gt_input_colour.shape[1] + new_img_colour.shape[1] + 20  # Total width of both images with a small gap\n",
    "    # combined_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # # Place images with titles on the blank canvas\n",
    "    # combined_image[:gt_input_colour.shape[0], :gt_input_colour.shape[1]] = gt_input_colour\n",
    "    # combined_image[:new_img_colour.shape[0], gt_input_colour.shape[1] + 20:] = new_img_colour\n",
    "\n",
    "    # # Add titles to the images\n",
    "    # cv2.putText(combined_image, title1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    # cv2.putText(combined_image, title2, (gt_input_colour.shape[1] + 30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # # Display the combined image\n",
    "    # cv2.imshow('Two Images Side by Side', combined_image)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "np_accuracy = np.array(total_accuracy)\n",
    "mean = np.mean(np_accuracy)\n",
    "print(\"mean of total accuracy \", mean)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we use maxpooling insteat of avg pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id mateches {1: (8, 0), 2: (13, 0), 3: (1, 0), 4: (10, 0), 5: (27, 0), 6: (2, 0), 7: (18, 0), 8: (10, 0), 9: (27, 0), 10: (11, 0), 11: (5, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  2.  5.  8. 10. 11. 13. 18. 27.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.2638888888888889, 0.2638888888888889)\n",
      "id mateches {1: (21, 0), 2: (26, 0), 3: (26, 0), 4: (9, 0), 5: (27, 0), 6: (1, 0), 7: (5, 0), 8: (10, 0), 9: (8, 0), 10: (23, 0), 11: (12, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  5.  8.  9. 10. 12. 21. 23. 26. 27.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.4913194444444444, 0.4913194444444444)\n",
      "id mateches {1: (25, 0), 2: (25, 0), 3: (23, 0), 4: (11, 0), 5: (27, 0), 6: (1, 0), 7: (16, 0), 8: (29, 0), 9: (27, 0), 10: (22, 0), 11: (22, 0), 12: (8, 0), 13: (20, 0), 14: (9, 0), 15: (8, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  8.  9. 11. 16. 20. 22. 23. 25. 27. 29.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.515625, 0.515625)\n",
      "id mateches {1: (22, 0), 2: (11, 0), 3: (27, 0), 4: (1, 0), 5: (1, 0), 6: (8, 0), 7: (23, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  8. 11. 22. 23. 27.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6041666666666666, 0.6041666666666666)\n",
      "id mateches {1: (9, 0), 2: (26, 0), 3: (27, 0), 4: (1, 0), 5: (8, 0), 6: (2, 0), 7: (15, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  2.  8.  9. 15. 26. 27.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5486111111111112, 0.5486111111111112)\n",
      "id mateches {1: (26, 0), 2: (27, 0), 3: (1, 0), 4: (22, 0)}\n",
      "unique numbers in new img patchwise  [ 1. 22. 26. 27.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.5590277777777778, 0.5590277777777778)\n",
      "id mateches {1: (9, 0), 2: (27, 0), 3: (25, 0), 4: (20, 0), 5: (24, 0), 6: (26, 0)}\n",
      "unique numbers in new img patchwise  [ 9. 20. 24. 25. 26. 27.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.0, 0.0)\n",
      "id mateches {1: (11, 0), 2: (22, 0), 3: (11, 0), 4: (3, 0), 5: (9, 0), 6: (24, 0), 7: (24, 0), 8: (8, 0)}\n",
      "unique numbers in new img patchwise  [ 3.  8.  9. 11. 22. 24.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.003472222222222222, 0.003472222222222222)\n",
      "id mateches {1: (23, 0), 2: (26, 0), 3: (25, 0), 4: (8, 0), 5: (24, 0), 6: (20, 0), 7: (6, 0), 8: (22, 0), 9: (2, 0), 10: (10, 0), 11: (8, 0)}\n",
      "unique numbers in new img patchwise  [ 2.  6.  8. 10. 20. 22. 23. 24. 25. 26.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.001736111111111111, 0.001736111111111111)\n",
      "id mateches {1: (8, 0), 2: (12, 0), 3: (25, 0), 4: (24, 0), 5: (8, 0), 6: (27, 0), 7: (1, 0), 8: (25, 0), 9: (27, 0), 10: (2, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  2.  8. 12. 24. 25. 27.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6232638888888888, 0.6232638888888888)\n",
      "id mateches {1: (12, 0), 2: (25, 0), 3: (25, 0), 4: (20, 0), 5: (27, 0), 6: (5, 0), 7: (23, 0), 8: (29, 0), 9: (1, 0), 10: (26, 0), 11: (2, 0)}\n",
      "unique numbers in new img patchwise  [ 1.  2.  5. 12. 20. 23. 25. 26. 27. 29.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.019097222222222224, 0.019097222222222224)\n",
      "id mateches {1: (1, 0), 2: (8, 0)}\n",
      "unique numbers in new img patchwise  [1. 8.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.6145833333333334, 0.6145833333333334)\n",
      "id mateches {1: (5, 0), 2: (22, 0), 3: (20, 0), 4: (100, 0), 5: (9, 0), 6: (23, 0), 7: (3, 0), 8: (25, 0)}\n",
      "unique numbers in new img patchwise  [  3.   5.   9.  20.  22.  23.  25. 100.]\n",
      "the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is (0.0, 0.0)\n",
      "mean of total accuracy  0.32652243589743585\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "for the current scene: access all the precomputed dinov2 features and do pca of the space\n",
    "\"\"\"\n",
    "#get all the features of the current scene\n",
    "all_info_path = osp.join(data_dir,\"files/Features2D/projection/DinoV2/patch_32_18\", curr_scan_id + \".pkl\")\n",
    "with open(all_info_path, 'rb') as file:\n",
    "    all_features_curr_scene = pickle.load(file)\n",
    "\n",
    "#using avg pooling\n",
    "all_features, all_ids, all_colors = max_pooling(all_features_curr_scene)\n",
    "all_features = np.array(all_features)\n",
    "all_ids = np.array(all_ids)\n",
    "all_colors = np.array(all_colors)\n",
    "\n",
    "#perform pca to 3D reduction\n",
    "pca = PCA(n_components=3)\n",
    "reduced_points = pca.fit_transform(all_features)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "access the features of the new scene and the info corresponding to the patches\n",
    "\"\"\"\n",
    "\n",
    "#get the Dinov2 features for the input image ( the segmented one)\n",
    "input_featues_path = osp.join(data_dir,\"files/Features2D/dino_segmentation/DinoV2/patch_32_18\", new_scan_id + \".pkl\")\n",
    "with open(input_featues_path, 'rb') as file:\n",
    "    input_features = pickle.load(file)\n",
    "\n",
    "#get the data information on the segmentation of the input image\n",
    "input_info_path = osp.join(data_dir,\"files/Segmentation/DinoV2/objects\", new_scan_id + \".pkl\")\n",
    "with open(input_info_path, 'rb') as file:\n",
    "    input_info = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "total_accuracy= []\n",
    "\n",
    "\"\"\"\n",
    "decide the id matches based on the projected points and 5 neighbouhood vote\n",
    "\"\"\"\n",
    "\n",
    "#iterate over each frame of the newscan and perform the assignment \n",
    "for frame in input_features:\n",
    "    #initialze the matches\n",
    "    id_matches= {}\n",
    "\n",
    "    input_dict = input_features[frame]\n",
    "\n",
    "    #iterate through every patch of the segmented new scene image, project it into the pca space ang get the closest\n",
    "    for img_id, img_vec in input_dict.items():\n",
    "        #reshape and project into the space\n",
    "\n",
    "        #use avg pooling\n",
    "        aggregated_vec = np.max(img_vec, axis=0)\n",
    "        #print(\"new vec shape\", aggregated_vec.shape)\n",
    "        reduced_unseen_point = pca.transform(aggregated_vec.reshape(1,-1))\n",
    "\n",
    "        # cosine distances to all reduced points\n",
    "        #distances = cosine_distances(reduced_unseen_point, reduced_points)\n",
    "\n",
    "        #eucledian dist\n",
    "        distances = euclidean_distances(reduced_unseen_point, reduced_points)\n",
    "\n",
    "        #get closest 5 points\n",
    "        num_neighbors = 5\n",
    "        closest_indices = np.argsort(distances[0])[:num_neighbors]\n",
    "\n",
    "        # classs of 5 closest points\n",
    "        closest_classes = all_ids[closest_indices]\n",
    "\n",
    "        # get the majority class and write into id_matches\n",
    "        most_common_class, count = Counter(closest_classes).most_common(1)[0]\n",
    "        id_matches[img_id] = (most_common_class, 0)\n",
    "\n",
    "\n",
    "    print(\"id mateches\", id_matches)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    turn the best match id into a pixelwise level, then patchwise level\n",
    "    \"\"\"\n",
    "\n",
    "    #assign each pixel to the new value, curr_scan_id , frame number is only for the size\n",
    "    new_img_pixelwise = generate_pairs_pixel_level(input_info[frame], id_matches)\n",
    "\n",
    "    #quantize to patches\n",
    "    new_img_patchwise = quantize_to_patch_level(new_img_pixelwise)\n",
    "    #aggregate to patches and colour it, we want the colours which are used in curr_scan id\n",
    "    new_img_colour = create_color_img_from_obj_id(data_dir,curr_scan_id, new_img_patchwise)\n",
    "    print(\"unique numbers in new img patchwise \", np.unique(new_img_patchwise))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    access the gt of the image and get the accuracy of the prediction, print result and display \n",
    "    \"\"\"\n",
    "    #get the gt patches for the segmented scene! so for the dinov2 segmentation: but the colour will be based on the current scene\n",
    "    gt_input_patchwise_path =  osp.join(data_dir,\"files/patch_anno/patch_anno_32_18\", new_scan_id + '.pkl')\n",
    "    with open(gt_input_patchwise_path, 'rb') as file:\n",
    "        gt_input_patchwise = pickle.load(file)\n",
    "    #print(\"unique numbers in gt_input_colour \", np.unique(gt_input_patchwise[frame]))\n",
    "    gt_input_colour = create_color_img_from_obj_id(data_dir,curr_scan_id,gt_input_patchwise[frame])\n",
    "\n",
    "\n",
    "    accuracy = get_accuracy(gt_input_patchwise[frame],new_img_patchwise)\n",
    "    total_accuracy.append(accuracy[0])\n",
    "    print(\"the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is\", accuracy)\n",
    "\n",
    "\n",
    "    # \"\"\"\n",
    "    # display the newly computed images next to each other\n",
    "    # \"\"\"\n",
    "\n",
    "    # #display the gt_image and the new patchwise image next to each other\n",
    "    # title1 = \"gt_patces\"\n",
    "    # title2 = \"dino_seg_patches\"\n",
    "\n",
    "    # # Create a blank canvas to combine images horizontally\n",
    "    # height = max(gt_input_colour.shape[0], new_img_colour.shape[0])  # Max height of both images\n",
    "    # width = gt_input_colour.shape[1] + new_img_colour.shape[1] + 20  # Total width of both images with a small gap\n",
    "    # combined_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # # Place images with titles on the blank canvas\n",
    "    # combined_image[:gt_input_colour.shape[0], :gt_input_colour.shape[1]] = gt_input_colour\n",
    "    # combined_image[:new_img_colour.shape[0], gt_input_colour.shape[1] + 20:] = new_img_colour\n",
    "\n",
    "    # # Add titles to the images\n",
    "    # cv2.putText(combined_image, title1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    # cv2.putText(combined_image, title2, (gt_input_colour.shape[1] + 30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # # Display the combined image\n",
    "    # cv2.imshow('Two Images Side by Side', combined_image)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "np_accuracy = np.array(total_accuracy)\n",
    "mean = np.mean(np_accuracy)\n",
    "print(\"mean of total accuracy \", mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
