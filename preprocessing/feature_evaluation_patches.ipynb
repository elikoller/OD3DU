{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/home/ekoller/BT\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import os \n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import plyfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import os.path as osp\n",
    "import sys\n",
    "ws_dir = '/local/home/ekoller/BT'\n",
    "print(ws_dir)\n",
    "sys.path.append(ws_dir)\n",
    "from utils import evaluation\n",
    "\n",
    "data_dir ='/local/home/ekoller/R3Scan'\n",
    "scenes_dir = '/local/home/ekoller/R3Scan/scenes'\n",
    "#scan_id= \"38770c95-86d7-27b8-8717-3485b411ddc7\" #is reference scan  since it is a reference scan everything shouls be correctly hit\n",
    "curr_scan_id = \"02b33e01-be2b-2d54-93fb-4145a709cec5\" \n",
    "new_scan_id =  \"fcf66d8a-622d-291c-8429-0e1109c6bb26\"\n",
    "frame_number = \"000008\"\n",
    "curr_frame_number = \"000008\"\n",
    "new_frame_number = \"000007\"\n",
    "patch_h= 18\n",
    "patch_w = 32\n",
    "patch_height = 30\n",
    "patch_width = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given the following: for the input image we have patchwise features. each patch has the same size. as the current image we have for each object in the scene the patch the following: a feature vector which is the average vector obtained by also dividing the image into the same size patches and then taking the vectors of the patches which belong to the objects based on the gt anno 2d\n",
    "there is also a threshold involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity between 2 vectors\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "\n",
    "#iterate over everything and give back the closest match for every box in sam\n",
    "def of_all_find_closes_pairs(sg_dict, img_dict):\n",
    "    closest_pairs = {}\n",
    "    print(\"sg_data object ids of image\", sg_dict.keys())\n",
    "    print(\"img_data object ids of image\", img_dict.keys())\n",
    "\n",
    "    # print(\"input data sg dict\", sg_dict)\n",
    "    # print(\"input data img dict\", img_dict)\n",
    "    #go over the image dict since we want to get the closest match of all the ids\n",
    "    for img_id, img_vec in img_dict.items():\n",
    "        min_distance = -1\n",
    "        closest_id = None\n",
    "    \n",
    "        \n",
    "        # go over every vector in the scenegraph\n",
    "        for obj_id, obj_vec in sg_dict.items():\n",
    "        \n",
    "            #print(\"obj id vec shape\", obj_vec.shape)\n",
    "            #print(\"img id vec shape\", img_vec.shape)\n",
    "            cosine_similarity_all_patches = [cosine_similarity(obj_vec[i], img_vec[i]) for i in range(img_vec.shape[0])]\n",
    "            average_cosine_similarity = np.mean(cosine_similarity_all_patches)\n",
    "          \n",
    "            \n",
    "            #update\n",
    "            if average_cosine_similarity> min_distance:\n",
    "                min_distance = average_cosine_similarity\n",
    "                closest_id= obj_id\n",
    "        #check if it is close enoug using a threshold\n",
    "        if min_distance > 0:\n",
    "            closest_pairs[img_id] = (closest_id, min_distance)\n",
    "        else:\n",
    "            closest_pairs[img_id] = (0, -1)\n",
    "\n",
    "        #closest_pairs[img_id] = (closest_id, min_distance)\n",
    "\n",
    "    return closest_pairs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualitation of the following: with the new ids fill in an image with the new ids\n",
    "#compute the resulting image when comparing stuff\n",
    "\n",
    "def generate_pairs_pixel_level(data_dir, proj_scan_id, sam_data,frame_number, id_matches):\n",
    "    #get the curren image (gt)\n",
    "    proj_img_path = osp.join(data_dir,\"files/gt_projection/obj_id\",proj_scan_id, 'frame-{}.jpg'.format(frame_number))\n",
    "    proj_img = cv2.imread(proj_img_path, cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    height, width= proj_img.shape\n",
    "\n",
    "    new_ids = np.zeros((height, width))\n",
    "\n",
    "\n",
    "    #go over every mask and fill in the id into the new_ids which it got mapped to\n",
    "    for seg_region in sam_data:\n",
    "        mask_id = seg_region[\"object_id\"]\n",
    "        #get to what the region mapped in the embeddings\n",
    "        matched_id = id_matches[mask_id]\n",
    "        mask = seg_region[\"mask\"]\n",
    "        new_ids[mask] = matched_id[0] #[0] is the id the second one is the error\n",
    "\n",
    "    #returns the new ids on a pixel wise level\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this codesegment takes an image on a pixel wise level and quantizes it such that every patch has only the id of the most often occuring id\n",
    "def quantize_to_patch_level(pixelwise_img):\n",
    "    #get the shape of the pixelwise img\n",
    "    input_h, input_w = pixelwise_img.shape\n",
    "    patch_width = int(input_w/patch_w)\n",
    "    patch_height= int(input_h/patch_h)\n",
    "\n",
    "    patchwise_w = patch_w #number of patches\n",
    "    patchwise_h = patch_h\n",
    "\n",
    "    patchwise_id = np.zeros((patchwise_h,patchwise_w))\n",
    "\n",
    "    for i in range(patchwise_h):\n",
    "            for j in range(patchwise_w):\n",
    "                # Define the coordinates of the current patch\n",
    "                h_start = i * patch_height\n",
    "                w_start = j * patch_width\n",
    "                h_end = h_start + patch_height\n",
    "                w_end = w_start + patch_width\n",
    "                \n",
    "                # Get the current patch from the input matrix\n",
    "                patch = pixelwise_img[h_start:h_end, w_start:w_end]\n",
    "                \n",
    "                # get the most reoccuring id of the patch\n",
    "                flattened_patch = patch.flatten()\n",
    "                # Find the most common value in the patch\n",
    "                value_counts = Counter(flattened_patch)\n",
    "                most_common_id = value_counts.most_common(1)[0][0]\n",
    "                \n",
    "                # Assign the most common ID to the new matrix\n",
    "                patchwise_id[i, j] = most_common_id\n",
    "\n",
    "\n",
    "    return patchwise_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur scan id colors 28\n",
      "new scan id colors 15\n"
     ]
    }
   ],
   "source": [
    "#for a given scene get the colours of the differnt object_ids\n",
    "def get_id_colours(data_dir,scan_id):\n",
    "    #access the mesh file to get the colour of the ids\n",
    "    mesh_file = osp.join(data_dir,\"scenes\", scan_id, \"labels.instances.annotated.v2.ply\")\n",
    "    ply_data = plyfile.PlyData.read(mesh_file)\n",
    "    # Extract vertex data\n",
    "    vertices = ply_data['vertex']\n",
    "    vertex_count = len(vertices)\n",
    "    \n",
    "    # Initialize dictionary to store object_id -> color mappings\n",
    "    object_colors = {}\n",
    "    \n",
    "   # Iterate through vertices\n",
    "    for i in range(vertex_count):\n",
    "        vertex = vertices[i]\n",
    "        object_id = vertex['objectId']\n",
    "        color = (vertex['red'], vertex['green'], vertex['blue'])\n",
    "        \n",
    "        # Check if object_id already in dictionary, otherwise initialize a Counter\n",
    "        if object_id in object_colors:\n",
    "            object_colors[object_id][color] += 1\n",
    "        else:\n",
    "            object_colors[object_id] = Counter({color: 1})\n",
    "    \n",
    "    # Convert Counter to dictionary with most frequent color\n",
    "    for object_id, color_counter in object_colors.items():\n",
    "        most_common_color = color_counter.most_common(1)[0][0]\n",
    "        object_colors[object_id] = np.array(most_common_color[::-1])\n",
    "    \n",
    "    return object_colors\n",
    "\n",
    "colors = get_id_colours(data_dir,curr_scan_id)\n",
    "print(\"cur scan id colors\", len(colors.keys()))\n",
    "colors = get_id_colours(data_dir,new_scan_id)\n",
    "print(\"new scan id colors\", len(colors.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function creates a color image of the size 960x540 from the patches\n",
    "def create_color_img_from_obj_id(data_dir,scan_id,obj_id_mat):\n",
    "    #access the mesh file to get the colour of the ids\n",
    "    colour_dict = get_id_colours(data_dir, scan_id)\n",
    "    #initialize the new matrix \n",
    "    og_height, og_width = obj_id_mat.shape\n",
    "    new_height = og_height * patch_height\n",
    "    new_width = og_width * patch_width\n",
    "\n",
    "    colour_mat = np.zeros((new_height,new_width,3))\n",
    "\n",
    "    #go over each element and assign the colour of the dictionary\n",
    "    for h in range(og_height):\n",
    "        for w in range(og_width):\n",
    "            colour = colour_dict[obj_id_mat[h][w]]\n",
    "            colour_mat[h*patch_height:(h+1)*patch_height, w*patch_width:(w+1)*patch_width] = colour\n",
    "\n",
    "\n",
    "    return colour_mat\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a gt patchwise matrix and a newly calculated matrix compute the accuracy\n",
    "def get_accuracy(gt_patches, computed_patches):\n",
    "    #make sure we dont do something dumm lol\n",
    "    assert gt_patches.shape == computed_patches.shape, \"Matrices must have the same shape\"\n",
    "\n",
    "    # Flatten matrices to iterate over each element\n",
    "    flat_gt = gt_patches.flatten()\n",
    "    flat_comp = computed_patches.flatten()\n",
    "    \n",
    "    total_instances = len(flat_gt)\n",
    "    correct_instances = 0\n",
    "    \n",
    "    # Count correct instances where IDs match\n",
    "    for id_gt, id_comp in zip(flat_gt, flat_comp):\n",
    "        if id_gt == id_comp:\n",
    "            correct_instances += 1\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = correct_instances / total_instances\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg_data object ids of image dict_keys([1, 4, 13, 14, 15, 18])\n",
      "img_data object ids of image dict_keys([1, 4, 13, 14, 15, 105])\n",
      "id mateches {1: (1, 0.23035121), 4: (4, 0.35209072), 13: (13, 0.36667588), 14: (14, 0.3042687), 15: (15, 0.3384763), 105: (18, 0.14608717)}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid mateches\u001b[39m\u001b[38;5;124m\"\u001b[39m, id_matches)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#assign each pixel to the new value, curr_scan_id , frame number is only for the size\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m new_img_pixelwise \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_pairs_pixel_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_scan_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_sam_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mframe_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_matches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#aggregate to patches and colour it, we want the colours which are used in curr_scan id\u001b[39;00m\n\u001b[1;32m     60\u001b[0m new_img_patchwise \u001b[38;5;241m=\u001b[39m quantize_to_patch_level(new_img_pixelwise)\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mgenerate_pairs_pixel_level\u001b[0;34m(data_dir, proj_scan_id, sam_data, frame_number, id_matches)\u001b[0m\n\u001b[1;32m     16\u001b[0m mask_id \u001b[38;5;241m=\u001b[39m seg_region[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#get to what the region mapped in the embeddings\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m matched_id \u001b[38;5;241m=\u001b[39m \u001b[43mid_matches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m mask \u001b[38;5;241m=\u001b[39m seg_region[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m new_ids[mask] \u001b[38;5;241m=\u001b[39m matched_id[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#[0] is the id the second one is the error\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "\n",
    "#given a gt patchwise matrix and a newly calculated matrix compute the accuracy\n",
    "def get_accuracy(gt_patches, computed_patches):\n",
    "    #make sure we dont do something dumm lol\n",
    "    assert gt_patches.shape == computed_patches.shape, \"Matrices must have the same shape\"\n",
    "\n",
    "    # Flatten matrices to iterate over each element\n",
    "    flat_gt = gt_patches.flatten()\n",
    "    flat_comp = computed_patches.flatten()\n",
    "    \n",
    "    total_instances = len(flat_gt)\n",
    "    correct_instances = 0\n",
    "    \n",
    "    # Count correct instances where IDs match\n",
    "    for id_gt, id_comp in zip(flat_gt, flat_comp):\n",
    "        if id_gt == id_comp:\n",
    "            correct_instances += 1\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = correct_instances / total_instances\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "#new_scan_id = curr_scan_id\n",
    "\n",
    "\"\"\"\n",
    "initialize here current = new scan id since we are looking at the gt\n",
    "\"\"\"\n",
    "#read in all the data needed for the evaluation currently this tests the sam input against the ground truth of the input\n",
    "#sam features\n",
    "# input_featues_path = osp.join(data_dir,\"files/Features2D/sam/DinoV2\", new_scan_id + \".pkl\")\n",
    "# with open(input_featues_path, 'rb') as file:\n",
    "#     input_features = pickle.load(file)\n",
    "# #sam data\n",
    "# input_sam_data_path = osp.join(data_dir,\"files/sam_data\", new_scan_id, 'frame-{}.npy'.format(frame_number))\n",
    "# input_sam_data = np.load(input_sam_data_path, allow_pickle=True)\n",
    "#get the dinofeatures for the boundingboxes of the projection\n",
    "input_featues_path = osp.join(data_dir,\"files/Features2D/projection/DinoV2\", new_scan_id + \".pkl\")\n",
    "with open(input_featues_path, 'rb') as file:\n",
    "    input_features = pickle.load(file)\n",
    "#sam data\n",
    "input_sam_data_path = osp.join(data_dir,\"files/sam_data\", new_scan_id, 'frame-{}.npy'.format(new_frame_number))\n",
    "input_sam_data = np.load(input_sam_data_path, allow_pickle=True)\n",
    "\n",
    "#projection features of current situation\n",
    "curr_featues_path = osp.join(data_dir,\"files/Features2D/projection/DinoV2\", curr_scan_id + \".pkl\")\n",
    "with open(curr_featues_path, 'rb') as file:\n",
    "    curr_features = pickle.load(file)\n",
    "\n",
    "\"\"\"\n",
    "compute the best match for each input on a patchwise level\n",
    "\"\"\"\n",
    "#closest pairs  fucking important lol !!!!!!!\n",
    "id_matches = of_all_find_closes_pairs(curr_features[frame_number], input_features[new_frame_number])\n",
    "print(\"id mateches\", id_matches)\n",
    "\n",
    "#assign each pixel to the new value, curr_scan_id , frame number is only for the size\n",
    "new_img_pixelwise = generate_pairs_pixel_level(data_dir, curr_scan_id, input_sam_data,frame_number, id_matches)\n",
    "\n",
    "#aggregate to patches and colour it, we want the colours which are used in curr_scan id\n",
    "new_img_patchwise = quantize_to_patch_level(new_img_pixelwise)\n",
    "new_img_colour = create_color_img_from_obj_id(data_dir,curr_scan_id, new_img_patchwise)\n",
    "\n",
    "#get the gt input patches and also turn them into bigger images\n",
    "gt_input_patchwise_path =  osp.join(data_dir,\"files/patch_anno/patch_anno_32_18\", new_scan_id + '.pkl')\n",
    "with open(gt_input_patchwise_path, 'rb') as file:\n",
    "    gt_input_patchwise = pickle.load(file)\n",
    "gt_input_colour = create_color_img_from_obj_id(data_dir,curr_scan_id,gt_input_patchwise[frame_number])\n",
    "\n",
    "\n",
    "accuracy = get_accuracy(gt_input_patchwise[frame_number],new_img_patchwise)\n",
    "print(\"the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is\", accuracy)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "display the newly computed images next to each other\n",
    "\"\"\"\n",
    "\n",
    "#display the gt_image and the new patchwise image next to each other\n",
    "title1 = \"gt_patces\"\n",
    "title2 = \"sam_patches\"\n",
    "\n",
    "# Create a blank canvas to combine images horizontally\n",
    "height = max(gt_input_colour.shape[0], new_img_colour.shape[0])  # Max height of both images\n",
    "width = gt_input_colour.shape[1] + new_img_colour.shape[1] + 20  # Total width of both images with a small gap\n",
    "combined_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "# Place images with titles on the blank canvas\n",
    "combined_image[:gt_input_colour.shape[0], :gt_input_colour.shape[1]] = gt_input_colour\n",
    "combined_image[:new_img_colour.shape[0], gt_input_colour.shape[1] + 20:] = new_img_colour\n",
    "\n",
    "# Add titles to the images\n",
    "cv2.putText(combined_image, title1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "cv2.putText(combined_image, title2, (gt_input_colour.shape[1] + 30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "# Display the combined image\n",
    "cv2.imshow('Two Images Side by Side', combined_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "find changes beginning: based on projection onto current mesh\n",
    "\"\"\"\n",
    "\n",
    "#compute the projection of the new pose onto the current mesh to get the current situation\n",
    "\n",
    "curr_proj_obj_id, curr_proj_color = evaluation.project_new_pose_in_curr_mesh(data_dir, osp.join(data_dir, \"scenes\"),curr_scan_id, new_scan_id, frame_number)\n",
    "cv2.imshow(\"object_id\", curr_proj_obj_id)\n",
    "cv2.imshow(\"color\", curr_proj_color)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#quantize that projection such that it is on patchwise level\n",
    "#actually with this we can compute the current change\n",
    "curr_proj_obj_patchwise = quantize_to_patch_level(curr_proj_obj_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
