{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/home/ekoller/BT\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import os \n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import plyfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import os.path as osp\n",
    "import sys\n",
    "ws_dir = '/local/home/ekoller/BT'\n",
    "print(ws_dir)\n",
    "sys.path.append(ws_dir)\n",
    "from utils import evaluation\n",
    "\n",
    "data_dir ='/local/home/ekoller/R3Scan'\n",
    "scenes_dir = '/local/home/ekoller/R3Scan/scenes'\n",
    "#scan_id= \"38770c95-86d7-27b8-8717-3485b411ddc7\" #is reference scan  since it is a reference scan everything shouls be correctly hit\n",
    "curr_scan_id = \"02b33e01-be2b-2d54-93fb-4145a709cec5\" \n",
    "new_scan_id =  \"fcf66d8a-622d-291c-8429-0e1109c6bb26\"\n",
    "frame_number = \"000008\"\n",
    "curr_frame_number = \"000008\"\n",
    "new_frame_number = \"000007\"\n",
    "patch_h= 18\n",
    "patch_w = 32\n",
    "patch_height = 30\n",
    "patch_width = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given the following: inout image segmented into masks and boundingboxes based on sam, the scenes of the current scan \n",
    "the features per box get compared and assigned to the most matching box.\n",
    "there is also a threshold involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity between 2 vectors\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "\n",
    "#iterate over everything and give back the closest match for every box in sam\n",
    "def of_all_find_closes_pairs(sg_dict, img_dict):\n",
    "    closest_pairs = {}\n",
    "    print(\"sg_data object ids of image\", sg_dict.keys())\n",
    "    print(\"img_data object ids of image\", img_dict.keys())\n",
    "\n",
    "    # print(\"input data sg dict\", sg_dict)\n",
    "    # print(\"input data img dict\", img_dict)\n",
    "    #go over the image dict since we want to get the closest match of all the ids\n",
    "    for img_id, img_vec in img_dict.items():\n",
    "        min_distance = -1\n",
    "        closest_id = None\n",
    "    \n",
    "        \n",
    "        # go over every vector in the scenegraph\n",
    "        for obj_id, obj_vec in sg_dict.items():\n",
    "        \n",
    "            #print(\"obj id vec shape\", obj_vec.shape)\n",
    "            #print(\"img id vec shape\", img_vec.shape)\n",
    "            cosine_similarity_all_patches = [cosine_similarity(obj_vec[i], img_vec[i]) for i in range(img_vec.shape[0])]\n",
    "            average_cosine_similarity = np.mean(cosine_similarity_all_patches)\n",
    "          \n",
    "            \n",
    "            #update\n",
    "            if average_cosine_similarity> min_distance:\n",
    "                min_distance = average_cosine_similarity\n",
    "                closest_id= obj_id\n",
    "        #check if it is close enoug using a threshold\n",
    "        if min_distance > 0:\n",
    "            closest_pairs[img_id] = (closest_id, min_distance)\n",
    "        else:\n",
    "            closest_pairs[img_id] = (0, -1)\n",
    "\n",
    "        #closest_pairs[img_id] = (closest_id, min_distance)\n",
    "\n",
    "    return closest_pairs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur scan id colors 28\n",
      "new scan id colors 15\n"
     ]
    }
   ],
   "source": [
    "#for a given scene get the colours of the differnt object_ids \n",
    "def get_id_colours(data_dir,scan_id):\n",
    "    #access the mesh file to get the colour of the ids\n",
    "    mesh_file = osp.join(data_dir,\"scenes\", scan_id, \"labels.instances.annotated.v2.ply\")\n",
    "    ply_data = plyfile.PlyData.read(mesh_file)\n",
    "    # Extract vertex data\n",
    "    vertices = ply_data['vertex']\n",
    "    vertex_count = len(vertices)\n",
    "    \n",
    "    # Initialize dictionary to store object_id -> color mappings\n",
    "    object_colors = {}\n",
    "    \n",
    "   # Iterate through vertices\n",
    "    for i in range(vertex_count):\n",
    "        vertex = vertices[i]\n",
    "        object_id = vertex['objectId']\n",
    "        color = (vertex['red'], vertex['green'], vertex['blue'])\n",
    "        \n",
    "        # Check if object_id already in dictionary, otherwise initialize a Counter\n",
    "        if object_id in object_colors:\n",
    "            object_colors[object_id][color] += 1\n",
    "        else:\n",
    "            object_colors[object_id] = Counter({color: 1})\n",
    "    \n",
    "    # Convert Counter to dictionary with most frequent color\n",
    "    for object_id, color_counter in object_colors.items():\n",
    "        most_common_color = color_counter.most_common(1)[0][0]\n",
    "        object_colors[object_id] = np.array(most_common_color[::-1])\n",
    "    \n",
    "    return object_colors\n",
    "\n",
    "colors = get_id_colours(data_dir,curr_scan_id)\n",
    "print(\"cur scan id colors\", len(colors.keys()))\n",
    "colors = get_id_colours(data_dir,new_scan_id)\n",
    "print(\"new scan id colors\", len(colors.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function creates a color image of the size 960x540 from the patches\n",
    "def create_color_img_from_obj_id(data_dir,scan_id,obj_id_mat):\n",
    "    #access the mesh file to get the colour of the ids\n",
    "    colour_dict = get_id_colours(data_dir, scan_id)\n",
    "    #initialize the new matrix \n",
    "    og_height, og_width = obj_id_mat.shape\n",
    "    new_height = og_height * patch_height\n",
    "    new_width = og_width * patch_width\n",
    "\n",
    "    colour_mat = np.zeros((new_height,new_width,3))\n",
    "\n",
    "    #go over each element and assign the colour of the dictionary\n",
    "    for h in range(og_height):\n",
    "        for w in range(og_width):\n",
    "            colour = colour_dict[obj_id_mat[h][w]]\n",
    "            colour_mat[h*patch_height:(h+1)*patch_height, w*patch_width:(w+1)*patch_width] = colour\n",
    "\n",
    "\n",
    "    return colour_mat\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a gt patchwise matrix and a newly calculated matrix compute the accuracy\n",
    "#aaparently this is not the same as accuracy\n",
    "def get_accuracy(gt_patches, computed_patches):\n",
    "    #make sure we dont do something dumm lol\n",
    "    assert gt_patches.shape == computed_patches.shape, \"Matrices must have the same shape\"\n",
    "\n",
    "    # Flatten matrices to iterate over each element\n",
    "    flat_gt = gt_patches.flatten()\n",
    "    flat_comp = computed_patches.flatten()\n",
    "\n",
    "    percentage = np.zeros_like(flat_gt)\n",
    "    \n",
    "    total_instances = len(flat_gt)\n",
    "    correct_instances = 0\n",
    "\n",
    "    #compute based on the other way\n",
    "    for idx in range(len(percentage)):\n",
    "        if flat_gt[idx] == flat_comp[idx]:\n",
    "            percentage[idx] = 1\n",
    "    \n",
    "    # Count correct instances where IDs match\n",
    "    for id_gt, id_comp in zip(flat_gt, flat_comp):\n",
    "        if id_gt == id_comp:\n",
    "            correct_instances += 1\n",
    "            \n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = correct_instances / total_instances\n",
    "    \n",
    "    return accuracy, np.mean(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualitation of the following: with the new ids fill in an image with the new ids\n",
    "#compute the resulting image when comparing stuff\n",
    "\n",
    "def generate_patch_pairs(dino_data, id_matches):\n",
    "    \n",
    "    new_ids = np.zeros((patch_h, patch_w))\n",
    "\n",
    "    patches = dino_data[\"patches\"]\n",
    "    #go over every mask and fill in the id into the new_ids which it got mapped to\n",
    "    for seg_region in dino_data:\n",
    "        mask_id = seg_region[\"object_id\"]\n",
    "        #get to what the region mapped in the embeddings\n",
    "        matched_id = id_matches[mask_id]\n",
    "        mask = seg_region[\"mask\"]\n",
    "        new_ids[mask] = matched_id[0] #[0] is the id the second one is the error\n",
    "\n",
    "    #returns the new ids on a pixel wise level\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#given a gt patchwise matrix and a newly calculated matrix compute the accuracy\n",
    "def get_accuracy(gt_patches, computed_patches):\n",
    "    #make sure we dont do something dumm lol\n",
    "    assert gt_patches.shape == computed_patches.shape, \"Matrices must have the same shape\"\n",
    "\n",
    "    # Flatten matrices to iterate over each element\n",
    "    flat_gt = gt_patches.flatten()\n",
    "    flat_comp = computed_patches.flatten()\n",
    "\n",
    "    percentage = np.zeros_like(flat_gt)\n",
    "    \n",
    "    total_instances = len(flat_gt)\n",
    "    correct_instances = 0\n",
    "\n",
    "    #compute based on the other way\n",
    "    for idx in range(len(percentage)):\n",
    "        if flat_gt[idx] == flat_comp[idx]:\n",
    "            percentage[idx] = 1\n",
    "    \n",
    "    # Count correct instances where IDs match\n",
    "    for id_gt, id_comp in zip(flat_gt, flat_comp):\n",
    "        if id_gt == id_comp:\n",
    "            correct_instances += 1\n",
    "            \n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = correct_instances / total_instances\n",
    "    \n",
    "    return accuracy, np.mean(percentage)\n",
    "\n",
    "new_scan_id = curr_scan_id\n",
    "\n",
    "\"\"\"\n",
    "initialize here current = new scan id since we are looking at the gt to see how good the segmentation is\n",
    "\"\"\"\n",
    "#features of input img\n",
    "input_featues_path = osp.join(data_dir,\"files/Features2D/dino_segmentation/DinoV2\",new_scan_id + \".pkl\")\n",
    "with open(input_featues_path, 'rb') as file:\n",
    "    input_features = pickle.load(file)\n",
    "#sam data\n",
    "input_dino_data_path = osp.join(data_dir,\"files/Segmentation/Dinov2/objects\", new_scan_id, 'frame-{}.npy'.format(new_frame_number))\n",
    "input_dino_data = np.load(input_dino_data_path, allow_pickle=True)\n",
    "\n",
    "#projection features of current situation\n",
    "curr_featues_path = osp.join(data_dir,\"files/Features2D/projection/DinoV2\", curr_scan_id + \".pkl\")\n",
    "with open(curr_featues_path, 'rb') as file:\n",
    "    curr_features = pickle.load(file)\n",
    "\n",
    "\"\"\"\n",
    "compute the best match for each input on a patchwise level\n",
    "\"\"\"\n",
    "#closest pairs  based ont the dino features \n",
    "id_matches = of_all_find_closes_pairs(curr_features[frame_number], input_features[new_frame_number])\n",
    "print(\"id mateches\", id_matches)\n",
    "\n",
    "#assign each patch to the new value \n",
    "new_img_patchwise =  generate_patch_pairs(input_dino_data, id_matches)\n",
    "\n",
    "\n",
    "new_img_colour = create_color_img_from_obj_id(data_dir,curr_scan_id, new_img_patchwise)\n",
    "\n",
    "#get the gt input patches and also turn them into bigger images\n",
    "gt_input_patchwise_path =  osp.join(data_dir,\"files/patch_anno/patch_anno_32_18\", new_scan_id + '.pkl')\n",
    "with open(gt_input_patchwise_path, 'rb') as file:\n",
    "    gt_input_patchwise = pickle.load(file)\n",
    "gt_input_colour = create_color_img_from_obj_id(data_dir,curr_scan_id,gt_input_patchwise[frame_number])\n",
    "\n",
    "\n",
    "accuracy = get_accuracy(gt_input_patchwise[frame_number],new_img_patchwise)\n",
    "print(\"the accuracy for the current features of the sam boundingboxes vs the gt boundingboxes is\", accuracy)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "display the newly computed images next to each other\n",
    "\"\"\"\n",
    "\n",
    "#display the gt_image and the new patchwise image next to each other\n",
    "title1 = \"gt_patces\"\n",
    "title2 = \"dino_patches\"\n",
    "\n",
    "# Create a blank canvas to combine images horizontally\n",
    "height = max(gt_input_colour.shape[0], new_img_colour.shape[0])  # Max height of both images\n",
    "width = gt_input_colour.shape[1] + new_img_colour.shape[1] + 20  # Total width of both images with a small gap\n",
    "combined_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "# Place images with titles on the blank canvas\n",
    "combined_image[:gt_input_colour.shape[0], :gt_input_colour.shape[1]] = gt_input_colour\n",
    "combined_image[:new_img_colour.shape[0], gt_input_colour.shape[1] + 20:] = new_img_colour\n",
    "\n",
    "# Add titles to the images\n",
    "cv2.putText(combined_image, title1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "cv2.putText(combined_image, title2, (gt_input_colour.shape[1] + 30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "# Display the combined image\n",
    "cv2.imshow('Two Images Side by Side', combined_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "find changes beginning: based on projection onto current mesh\n",
    "\"\"\"\n",
    "\n",
    "#compute the projection of the new pose onto the current mesh to get the current situation\n",
    "\n",
    "curr_proj_obj_id, curr_proj_color = evaluation.project_new_pose_in_curr_mesh(data_dir, osp.join(data_dir, \"scenes\"),curr_scan_id, new_scan_id, frame_number)\n",
    "cv2.imshow(\"object_id\", curr_proj_obj_id)\n",
    "cv2.imshow(\"color\", curr_proj_color)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#quantize that projection such that it is on patchwise level\n",
    "#actually with this we can compute the current change\n",
    "curr_proj_obj_patchwise = quantize_to_patch_level(curr_proj_obj_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
