<div align='center'>
<h2 align="center"> OD3DU: Object Detection based 3D Scene Understanding </h2>

<a href="linkedin.com/in/elena-koller-3b94041b4">Elena Koller</a><sup>1</sup>, 
<a href="https://cvg.ethz.ch/team/Dr-Zuria-Bauer"> Dr. Zuria Bauer</a><sup>1</sup> , 
<a href="https://cvg.ethz.ch/team/Dr-Daniel-Bela-Barath"> Dr. Dániel Béla Baráth</a> <sup>1</sup>
<a href="https://cvg.ethz.ch/team/Prof-Dr-Marc-Pollefeys"> Prof. Dr. Marc Pollefeys</a> <sup>1</sup>

<sup>1</sup>ETH Zurich   

OD3DU operates in low-dynamic real-world indoor environments. Given a reference scene graph representing the scene at time t0 and an RGB-D rescan of the scene at time ti, OD3DU predicts the reference 3D object instance centers in the rescan. Since the environment is low-dynamic, scene changes can not be captured directly by the camera but have to be inferred post-hoc.


![teaser](./data/repo_img/pipeline_overview.png)
</div>


## Code Structure:

```
├── OD3DU
│   ├── configs                         <- configuration definition
│   ├── src
│   │   │── preprocessing               <- preprocessing of scene graph
│   │   │── gt_annotations              <- generates 2D ground truths 
│   │   │── rescan_segmentation         <- segmentation of the rescan sequence using Mask2Former x DinoV2
│   │   │── object_2D_features          <- generates object features for both reference scene graph and rescan
│   │   │── segment2object_matching     <- matches predicted segments to reference objects
│   │   │── center_prediction_3D        <- predicts 3D object centers
│   │   
│   ├── scripts                         <- implementation scripts 
│   │── utils                           <- util functions
│   │── environment.yml                 <- conda environment
│   │── README.md                    
```

### Dependencies:

The project has been tested on Ubuntu 20.04.
The main dependencies of the project are the following:

```yaml
python: 3.8.20
cuda: 11.8
```
You can set up an environment as follows :
```bash
git clone https://github.com/elikoller/OD3DU.git
cd OD3DU

conda env create -f environment.yml
```
Other dependences:

For the semantic segmentation a separate docker container must be employed due to incompatible dependencies (<a href="https://github.com/facebookresearch/dinov2/issues/353">source</a>). The docker can be pulled the following way:


```bash
docker pull spped2000/dinov2manyproblem:latest
```


## Dataset Generation:
### Download Dataset - 3RScan + 3DSSG + 
Download [3RScan](https://github.com/WaldJohannaU/3RScan) and [3DSSG](https://3dssg.github.io/). Move all R3Scan files to ``3RScan/scenes/``, all files of 3DSSG to a new ``3RScan/files/`` directory within Scan3R. The additional meta files are available to be downloaded [here](https://drive.google.com/file/d/1abvycfnwZFBBqYuZN5WFJ80JAB1GwWPN/view). Download the additional meta files ([source repo](https://github.com/y9miao/VLSG)) and move them to ``3RScan/files/``. Additionaly, generate ``labels.instances.align.annotated.v2.ply`` (aligns the reference and rescan) using [this](https://github.com/ShunChengWu/3DSSG/blob/main/data_processing/transform_ply.py) repo. Add the newly generated ply file to the corresponding scene folder in ``3RScan/scenes/``.
The structure should be:

```
├── 3RScan
│   ├── files                 <- all 3DSSG files and additional meta files
│   │   meta files
│   ├── scenes                <- scans (3RScan)
```

Please do not forget to unzip the sequence files within the scenes directory.



### Dataset Pre-process:
We pre-process the information provided in the 3RScan dataset and the metafiles in order to get scene graph information along with 2D and 3D ground truths. The code for the preprocessing can be found here ``src/preprocessing``

The first script generates the scene graph datastructures of the scenes for both the train and test set. If you are not intending on performing parameter optimizations based on the train set, you can comment the commands for the train script out. Please also set the env variables of OD3DU_SPACE to the repo path, the San3R_ROOT_DIR  and CONDA_BIN

Don't forget to set the env variables "VLSG_SPACE" as the repository path,  set "Data_ROOT_DIR" as the path to "3RScan" dataset and set "CONDA_BIN" to accordingly in the bash script.

```bash
bash scripts/preprocess/scan3r_data_preprocess.sh
```
The result processed data will be save to "{Data_ROOT_DIR}/files/orig".
<!-- > __Note__ To adhere to our evaluation procedure, please do not change the seed value in the files in ``configs/`` directory.  -->
```

├── 3RScan
│   ├── files                 <- all 3RScan and 3DSSG meta files and annotations
│   │   ├──Segmentation       <- Segmentations of the input rescan
│   │   ├──Features2D         <- Object features generated by DinoV2 (reference and rescan)
│   │   ├──Predicted_Matches  <- input segment to reference object matches
│   │   ├──Predicted_Centers  <- Predicted 3D object centers
│   │   ├──Results            <- if evaluations are run, all results will be saved in this folder
│   │   ├──orig               <- Scene Graph Data
│   │   ├──patch_anno         <- 2D Ground truth patch-object annotation of the scenes
│   │   ├──gt_projection      <- 2D Ground truth projection annotation of the scenes
│   │   meta files
│   ├── scenes                <- scans
```



also some more confusing stuff is the following: the dataset did not provide the graphdata for the evaluation set so in this paper: the original validation set was taken and then split into validation and test. hence for the dataset only the test and validation set get accessed



### Generating Ground Truth Patch-Object Annotastion
To generate ground truth annotation, use : 
```bash
bash scripts/gt_annotations/scan3r_gt_annotations.sh
```
This will create a pixel-wise and patch-level ground truth annotations for each query image. These files will be saved  to "{Data_ROOT_DIR}/files/gt_projection and "{Data_ROOT_DIR}/files/patch_anno only for the eval and train set tho
the current number of the patches is 32x18 but this can be changed, look if there is also th, just take max instead of th == 0.2

while it migh be confusing the ground truth for the reference is actually the data we already have provided: our assumption is that we have the mesh -> so there for the reference scans we will actually compute with them. for the rescans however this is the actual ground truth which we will also use for comparison later on




### Elena's Code
the first approach involved bounduingboxes  intersection for speedup but this was slow & not good enough but it helped a lot to visualize the whole dataset + understand it better



### generating a semantic segmentation for the input images
the bigger goal is to generate dinov features for different objects in the input images and matching them to the objects in the scene. We dont want to train a network but use already pretrained ones. We also tried to divide inpout and gt into same size patches and compare these however the result was not good. Hence we look into a segmentation of the input image to get a better result. We use dinov2 transform2mask, a pretrained model on akd20 something, the segmentation masks are stored here "{Data_ROOT_DIR}/files/Features2D/dino_segmentation. 
since the requirements are a bit tought to manage and use a different version of cuda than the rest, we used the dockercontainer from the issue section so get this part running. we store the info in a h5 file and it contains boundingboxes, obj_ids, masks and so on a patch level and boundingboxes

```bash
bash scripts/dino_segmentation/semantic_segmentation_dino.sh
```


### Feature generation
To compare the inhalt of the input images to the current situation we need some features. in order to do that we do the following:  for the gt projection we go into the projection and for each individual object we compute bounding boxes based on the quantization into the 32x18 patches. For the input images we take the generated dinov2 mask and also quantize it into 32x18 patches in order to eliminate noise in the segmentation. then we compute based on the same pretrained network the features for each object/ segment by resizing it into 224x224 big patches.
      [Dino v2](https://dinov2.metademolab.com/). 
To generate the features, use : 
```bash
bash scripts/features2D/scan3r_dinov2.sh
```
This will create patch-level features for query images and save in "{Data_ROOT_DIR}/Features2D/dino_segmentation/Dinov2/patch_32_18_scan". for the projection and at "{Data_ROOT_DIR}/Features2D/projection/Dinov2/patch_32_18_scan"



### Computation of best parameters "Training
To generate the tables with the accuracies, use : 
```bash
bash scripts/segment_to_object_matching/computation.sh
```

### predict the feature matches
To generates the feature matches for the semantig segments : 
```bash
bash scripts/segment_to_object_matching/obj_matches.sh
```

### Predict objects

```bash
bash scripts/3D_center_prediction/predict_objects.sh
```

## to evaluate and get a statistical perspective ot the predicted object centers

```bash
bash scripts/3D_center_prediction/predict_objects_statistics.sh
```




After running all scripts the final folder structure will look the following way:

```

├── 3RScan
│   ├── files                 <- all 3RScan and 3DSSG meta files and annotations
│   │   ├──Segmentation       <- Segmentations of the input rescan
│   │   ├──Features2D         <- Object features generated by DinoV2 (reference and rescan)
│   │   ├──Predicted_Matches  <- input segment to reference object matches
│   │   ├──Predicted_Centers  <- Predicted 3D object centers
│   │   ├──Results            <- if evaluations are run, all results will be saved in this folder
│   │   ├──orig               <- Scene Graph Data
│   │   ├──patch_anno         <- 2D Ground truth patch-object annotation of the scenes
│   │   ├──gt_projection      <- 2D Ground truth projection annotation of the scenes
│   │   meta files
│   ├── scenes                <- scans
```


## Acknowledgments :recycle:
In this project we use (parts of) the official implementations of the following projects. We want to thank the respective authors for sharing the code for their works!
- [SceneGraphLoc](https://github.com/y9miao/VLSG) 
- [SGAligner](https://github.com/sayands/sgaligner) 
- [SceneGraphFusion](https://github.com/ShunChengWu/3DSSG)

